{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.1\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.0\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** There is a huge chance that the assignment will be impossible to pass if the versions of `lighgbm` and `scikit-learn` are wrong. The versions being tested:\n",
    "\n",
    "    numpy 1.13.1\n",
    "    pandas 0.20.3\n",
    "    scipy 0.19.1\n",
    "    sklearn 0.19.0\n",
    "    ligthgbm 2.0.6\n",
    "    \n",
    "\n",
    "To install an older version of `lighgbm` you may use the following command:\n",
    "```\n",
    "pip uninstall lightgbm\n",
    "pip install lightgbm==2.0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking.\n",
    "\n",
    "We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the hard drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../readonly/final_project_data/shops.csv')\n",
    "items = pd.read_csv('../readonly/final_project_data/items.csv')\n",
    "item_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use only 3 shops for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = sales[sales['shop_id'].isin([26, 27, 28])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the features. This part is all implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f8325a074b42308315155b6f0c0ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in `all_data` variable. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_shop  target_item  \\\n",
       "0       28    10994              12     1.0       6949.0          1.0   \n",
       "1       28    10992              12     3.0       6949.0          4.0   \n",
       "2       28    10991              12     1.0       6949.0          5.0   \n",
       "3       28    10988              12     1.0       6949.0          2.0   \n",
       "4       28    11002              12     1.0       6949.0          1.0   \n",
       "\n",
       "   target_lag_1  target_item_lag_1  target_shop_lag_1  target_lag_2  \\\n",
       "0           0.0                1.0             8499.0           0.0   \n",
       "1           3.0                7.0             8499.0           0.0   \n",
       "2           1.0                3.0             8499.0           0.0   \n",
       "3           2.0                5.0             8499.0           4.0   \n",
       "4           0.0                1.0             8499.0           0.0   \n",
       "\n",
       "   target_item_lag_2  target_shop_lag_2  target_lag_3  target_item_lag_3  \\\n",
       "0                1.0             6454.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                0.0                0.0           0.0                1.0   \n",
       "3                5.0             6454.0           5.0                6.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_3  target_lag_4  target_item_lag_4  target_shop_lag_4  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                0.0                0.0   \n",
       "2             5609.0           0.0                2.0             6753.0   \n",
       "3             5609.0           0.0                2.0             6753.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_5  target_item_lag_5  target_shop_lag_5  target_lag_12  \\\n",
       "0           0.0                0.0                0.0            0.0   \n",
       "1           0.0                1.0             7521.0            0.0   \n",
       "2           2.0                4.0             7521.0            0.0   \n",
       "3           0.0                0.0                0.0            0.0   \n",
       "4           0.0                0.0                0.0            0.0   \n",
       "\n",
       "   target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0                 0.0                 0.0                37  \n",
       "1                 0.0                 0.0                37  \n",
       "2                 0.0                 0.0                40  \n",
       "3                 0.0                 0.0                40  \n",
       "4                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sake of the programming assignment, let's artificially split the data into train and test. We will treat last month data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "print('Test `date_block_num` is %d' % last_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement a basic stacking scheme. We have a time component here, so we will use ***scheme f)*** from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let's see how we get test meta-features first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firts, we will run *linear regression* on numeric columns and get predictions for the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for linreg is 0.743180\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the we run *LightGBM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for LightGBM is 0.738391\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it is your turn to write the code**. You need to implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "That is, you need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "cur_index = 0\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "\n",
    "    print(cur_block_num)\n",
    "    \n",
    "    '''\n",
    "        1. Split `X_train` into parts\n",
    "           Remember, that corresponding dates are stored in `dates_train` \n",
    "        2. Fit linear regression \n",
    "        3. Fit LightGBM and put predictions          \n",
    "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "           You can use `dates_train_level2` for it\n",
    "           Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "    '''      \n",
    "    \n",
    "    #  YOUR CODE GOES HERE\n",
    "    train_chunks = X_train.loc[dates_train < cur_block_num]\n",
    "    test_chunks = X_train.loc[dates_train == cur_block_num]\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train_chunks.values, y_train[dates_train < cur_block_num])\n",
    "    pred_lr = lr.predict(test_chunks.values)\n",
    "    \n",
    "    model = lgb.train(lgb_params, lgb.Dataset(train_chunks, label=y_train[dates_train < cur_block_num]), 100)\n",
    "    pred_lgb = model.predict(test_chunks)\n",
    "    \n",
    "    index = (dates_train_level2 == cur_block_num).sum()\n",
    "    \n",
    "    X_train_level2[cur_index:cur_index+index, 0] = pred_lr\n",
    "    X_train_level2[cur_index:cur_index+index, 1] = pred_lgb\n",
    "    \n",
    "    cur_index += index\n",
    "    \n",
    "    \n",
    "# Sanity check\n",
    "assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988,  1.38811989]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.50148988,  1.38811989])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_level2.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig *scatter plot* between the two metafeatures. Plot the scatter plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f054ff91be0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGkAAAJ7CAYAAABQ2swBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X903eldH/j3c6VrWRlNElWe0MTKMKQOZTM5jltcwtZb\nyoR2Sym4sF4oCQdoC522S7qFUuywu2zIZtsDatOeUmh7QstCWpolRKUTAuU0G+iShMDWA7Y6k6Yw\npIGRE/JDaBJrIl1Lus/+YckjWfIvWffe77Ver3N0JD33e7/3oxzJmfs+n+fzlFprAAAAABis1qAL\nAAAAAEBIAwAAANAIQhoAAACABhDSAAAAADSAkAYAAACgAYQ0AAAAAA0gpAEAAABoACENAAAAQAMI\naQAAAAAaYHTQBdyNI0eO1IceemjQZQAAAADc0OOPP/7pWusDt7puqEOahx56KOfPnx90GQAAAAA3\nVEr5ndu5znYnAAAAgAYQ0gAAAAA0gJAGAAAAoAGENAAAAAANIKQBAAAAaAAhDQAAAEADCGkAAAAA\nGkBIAwAAANAAQhoAAACABhDSAAAAADSAkAYAAACgAXoW0pRSfqyU8slSyhNb1n6qlHJh4+OjpZQL\nG+sPlVKWtzz2z3pVFwAAAEATjfbw3j+e5IeTvG1zodb6Fza/LqW8Jclntlz/27XWEz2sBwAAAKCx\nehbS1Fp/uZTy0G6PlVJKkm9I8ppevT4AAADAMBnUTJo/keQTtdbf2rL2BaWU3yil/L+llD9xoyeW\nUh4tpZwvpZz/1Kc+1ftKAQAAAPpgUCHNa5O8fcv3H0/yYK31jyT5W0n+dSnl+bs9sdb61lrryVrr\nyQceeKAPpQIAAAD0Xt9DmlLKaJL/IclPba7VWju11oWNrx9P8ttJvrDftQEAAAAMyiA6af5Ukg/X\nWuc3F0opD5RSRja+flmSlyf5yABqAwAAABiIXh7B/fYkH0zyh0sp86WUb9t46BuzfatTknxZkrlS\nysUk70zy12qtv9+r2gAAAACappenO732But/cZe12SSzvaoFAAAAoOkGNTgYAAAAgC2ENAAAwFBa\nWOrk4tPPZGGpM+hSAPZFz7Y7AQAA9MpjFy7l3Oxc2q1WVrvdzJw5ntMnjg66LIC7opMGAAAYKgtL\nnZybncvKajeXO2tZWe3m7Oycjhpg6AlpAACAoTK/uJx2a/tbmXarlfnF5QFVBLA/hDQAAMBQmZ4c\nz2q3u21ttdvN9OT4gCoC2B9CGgAAYKhMTYxl5szxHG63cv/YaA63W5k5czxTE2ODLg3grhgcDAAA\nDJ3TJ47m1LEjmV9czvTkuIAGuCcIaQAAgKE0NTEmnAHuKbY7AQAAADSAkAYAAACgAYQ0AAAAAA0g\npAEAAABoACENAAAAQAMIaQAAAAAaQEgDAAAA0ABCGgAAAIAGENIAAAAANICQBgAAAKABhDQAAAAA\nDSCkAQAAAGgAIQ0AAABAAwhpAAAAABpASAMAAADQAEIaAAAAgAYQ0gAAAAA0gJAGAAAAoAGENAAA\nAAANIKQBAAAAaAAhDQAAAEADCGkAAAAAGkBIAwAAANAAQhoAAACABhDSAAAAADSAkAYAAACgAYQ0\nAAAAAA0gpAEAAABoACENAAAAQAMIaQAAAAAaQEgDAAAA0ABCGgAAAIAGENIAAAAANICQBgAAAKAB\nhDQAAAAADSCkAQAAAGgAIQ0AAABAAwhpAAAAABpASAMAAADQAEIaAAAAgAYQ0gAAAAA0gJAGAAAA\noAGENAAAAAANIKQBAAAAaAAhDQAAAEADCGkAAAAAGkBIAwAAANAAQhoAAACABhDSAAAAADSAkAYA\nAACgAYQ0AAAAAA0gpAEAAABoACENAAAAQAMIaQAAAAAaQEgDAAAA0ABCGgAAAIAG6FlIU0r5sVLK\nJ0spT2xZ+/5SyqVSyoWNj6/a8tj3llKeKqX8l1LKn+lVXQAAAABN1MtOmh9P8pW7rP/DWuuJjY+f\nT5JSyiuSfGOShzee809KKSM9rA0AAACgUXoW0tRafznJ79/m5X8+yf9da+3UWv9rkqeSfEmvagMA\nAO49C0udXHz6mSwsdQZdCsCejA7gNV9fSvmWJOeTfHetdTHJ0SS/uuWa+Y21HUopjyZ5NEkefPDB\nHpcKAAAMg8cuXMq52bm0W62sdruZOXM8p0/s+pYCoLH6PTj4nyb5Q0lOJPl4krdsrJddrq273aDW\n+tZa68la68kHHnigN1UCAABDY2Gpk3Ozc1lZ7eZyZy0rq92cnZ3TUQMMnb6GNLXWT9Ra12ut3SQ/\nmue2NM0neemWS6eTfKyftQEAAMNpfnE57db2tzbtVivzi8sDqghgb/oa0pRSXrzl269Lsnny07uS\nfGMpZayU8gVJXp7k/+tnbQAAwHCanhzPare7bW2128305PiAKgLYm14ewf32JB9M8odLKfOllG9L\nMlNK+U+llLkkjyT5riSptT6Z5B1JPpTkF5J8R611vVe1AQAA946pibHMnDmew+1W7h8bzeF2KzNn\njmdqYmzQpQHckVLrrqNfhsLJkyfr+fPnB10GAADQAAtLncwvLmd6clxAAzRKKeXxWuvJW103iNOd\nAAAA9t3UxJhwBhhq/T7dCQAAAIBdCGkAAAAAGkBIAwAAANAAQhoAAACABhDSAAAAADSAkAYAAACg\nAYQ0AAAAAA0gpAEAAABoACENAAAAQAMIaQAAAAAaQEgDAAAA0ABCGgAAAIAGENIAAAAANICQBgAA\nAKABhDQAAAAADSCkAQAAAGgAIQ0AQMMtLHVy8elnsrDUGXQpAEAPjQ66AAAAbuyxC5dybnYu7VYr\nq91uZs4cz+kTRwddFgDQAzppAAAaamGpk3Ozc1lZ7eZyZy0rq92cnZ3TUQMA9yghDQBAQ80vLqfd\n2v6fa+1WK/OLywOqCADoJSENAEBDTU+OZ7Xb3ba22u1menJ8QBUBAL0kpAEAaKipibHMnDmew+1W\n7h8bzeF2KzNnjmdqYmzQpQEAPWBwMABAg50+cTSnjh3J/OJypifHBTTAPWdhqePfONggpAEAaLip\niTFvXGg0b7LZKyfYwXZCGgAAYM+8yWavtp5gt5Kr87fOzs7l1LEjwj4OLDNpAACAPXFMPHfDCXaw\nk5AGAADYE2+yuRtOsIOdhDQAAMCeeJPN3XCCHexkJg0AALAnm2+yz143k8abbG6XE+xgOyENAACw\nZ95kc7ecYAfPEdIAAAB3xZtsgP1hJg0AAABAAwhpAAAAABpASAMAAADQAEIaAAAAgAYQ0gAAQB8t\nLHVy8elnsrDUGcr7A9A7TncCAIA+eezCpZybnUu71cpqt5uZM8dz+sTRobk/AL2lkwYAAPpgYamT\nc7NzWVnt5nJnLSur3Zydndu3jpde3x+A3hPSAABAH8wvLqfd2v6f3+1WK/OLy0NxfwB6T0gDAAB9\nMD05ntVud9vaareb6cnxobg/AL0npAEAgD6YmhjLzJnjOdxu5f6x0RxutzJz5nimJsaG4v4A9F6p\ntQ66hj07efJkPX/+/KDLAACA27aw1Mn84nKmJ8d7EqD0+v4A3LlSyuO11pO3us7pTgAA0EdTE2M9\nDU96fX8Aesd2JwAAAIAGENIAAAAANICQBgAAAKABhDQAAABAIy0sdXLx6WeysNQZdCl9YXAwAAAA\n0DiPXbiUc7NzabdaWe12M3PmeE6fODrosnpKJw0AAADQKAtLnZybncvKajeXO2tZWe3m7OzcPd9R\nI6SBA+qgtQ0CAADDY35xOe3W9sii3WplfnF5QBX1h+1OcAAdxLZBAABgeExPjme12922ttrtZnpy\nfEAV9YdOGjhgDmrbIAAAMDymJsYyc+Z4DrdbuX9sNIfbrcycOZ6pibFBl9ZTOmnggNlsG1zJc6n0\nZtvgvf4PHgAAMDxOnziaU8eOZH5xOdOT4wfi/YqQBg6Yg9o2CAAADJ+pibEDEc5sst0JDpiD2jYI\nAADQdDpp4AA6iG2DAAAATSekgQPqoLUNAgAANJ3tTgAAAAANIKQBAAAAaAAhDQAAAEADCGkAAAAA\nGkBIAwAAANAAQhoAAADueQtLnVx8+pksLHUGXQrckCO4AQAAuKc9duFSzs3Opd1qZbXbzcyZ4zl9\n4uigy4IddNIAAABwz1pY6uTc7FxWVru53FnLymo3Z2fndNTQSD0LaUopP1ZK+WQp5Ykta3+vlPLh\nUspcKeVnSikv3Fh/qJSyXEq5sPHxz3pVFwAAAAfH/OJy2q3tb33brVbmF5cHVBHcWC87aX48yVde\nt/aeJK+stR5P8ptJvnfLY79daz2x8fHXelgXAAAAB8T05HhWu91ta6vdbqYnxwdUEdxYz0KaWusv\nJ/n969b+fa11bePbX00y3avXBwAAgKmJscycOZ7D7VbuHxvN4XYrM2eOZ2pibNClwQ6DHBz8l5P8\n1Jbvv6CU8htJPpvkf6u1vm+3J5VSHk3yaJI8+OCDPS8SAACA4Xb6xNGcOnYk84vLmZ4cF9DQWAMJ\naUop/2uStSQ/ubH08SQP1loXSilfnOTfllIerrV+9vrn1lrfmuStSXLy5Mnar5oBAAAYXlMTY8IZ\nGq/vpzuVUr41yVcn+aZaa02SWmun1rqw8fXjSX47yRf2uzYAAACAQelrSFNK+cok55KcrrV+bsv6\nA6WUkY2vX5bk5Uk+0s/aAAAAAAapZ9udSilvT/LlSY6UUuaTvDFXT3MaS/KeUkqS/OrGSU5fluT/\nKKWsJVlP8tdqrb+/640BAAAA7kE9C2lqra/dZflf3ODa2SSzvaoFAAAAoOn6PpMGAAAAgJ2ENAAA\nAAANIKQBAAAAaAAhDQAAAEADCGkAAAAAGkBIAwAAANAAQhoAAACABhDSAAAAADSAkAYAAACgAYQ0\nAAAAAA0gpAEAAABoACENAAAAQAMIaQAAAAAaQEgDAAAA0ABCGgAAAIAGENIAAAAANICQBgAAAKAB\nhDQAAAAADSCkAQAAAGgAIQ0AAABAAwhpANhXC0udXHz6mSwsdQZdCgAADJXRQRcAwL3jsQuXcm52\nLu1WK6vdbmbOHM/pE0cHXRYAAAwFnTQA7IuFpU7Ozc5lZbWby521rKx2c3Z2TkcNAADcJiENAPti\nfnE57db2/1tpt1qZX1weUEUAADBchDQA7IvpyfGsdrvb1la73UxPjg+oIgAAGC5CGgD2xdTEWGbO\nHM/hdiv3j43mcLuVmTPHMzUxNujSAABgKBgcDMC+OX3iaE4dO5L5xeVMT44LaAAA4A4IaQDYV1MT\nY8IZAADYA9udAAAAABpASAMAAADQAEIaAAAAgAYQ0gAAAAA0gJAGAAAAoAGENAAAAAANIKQBAAAA\naAAhDQAAAEADCGkAAAAAGkBIAwAAANAAQhoAAACABhDSAAAAADSAkAYAAACgAYQ0AAAAAA0gpAEA\nAABoACENAAAAQAMIaQAAAAAaQEgDAAAA0ABCGmighaVOLj79TBaWOoMuBQAAgD4ZHXQBwHaPXbiU\nc7NzabdaWe12M3PmeE6fODrosgAAAOgxnTTQIAtLnZybncvKajeXO2tZWe3m7OycjhoAAIADQEgD\nDTK/uJx2a/ufZbvVyvzi8oAqAgAAoF+ENNAg05PjWe12t62tdruZnhwfUEUAAAD0i5AGGmRqYiwz\nZ47ncLuV+8dGc7jdysyZ45maGBt0aQAAAPSYwcHQMKdPHM2pY0cyv7ic6clxAQ0AAMABIaSBBpqa\nGBPOAAAAHDC2OwEAAAA0gJAGAAAAoAGENAAAAAANIKQBAAAAaAAhDQAAAEADCGkAAAAAGkBIAwDA\nXVlY6uTi089kYakz6FIAYKiNDroAAACG12MXLuXc7FzarVZWu93MnDme0yeODrosABhKOmkAANiT\nhaVOzs3OZWW1m8udtaysdnN2dk5HDQDskZAGAIA9mV9cTru1/T8n261W5heXB1QRAAw3IQ0AAHsy\nPTme1W5329pqt5vpyfEBVQQAw62nIU0p5cdKKZ8spTyxZe0PlFLeU0r5rY3PkxvrpZTyQ6WUp0op\nc6WUP9rL2gAAuDtTE2OZOXM8h9ut3D82msPtVmbOHM/UxNigSwOAodTrwcE/nuSHk7xty9obkry3\n1voDpZQ3bHx/LsmfTfLyjY9XJ/mnG58BAGio0yeO5tSxI5lfXM705LiABgDuQk87aWqtv5zk969b\n/vNJfmLj659I8rVb1t9Wr/rVJC8spby4l/UBAHD3pibG8qqXvlBAAwB3aRAzaT6v1vrxJNn4/KKN\n9aNJnt5y3fzG2jallEdLKedLKec/9alP9bxYAAAAgH5o0uDgssta3bFQ61trrSdrrScfeOCBPpQF\nAAAA0HuDCGk+sbmNaePzJzfW55O8dMt100k+1ufaAAAAAAZiECHNu5J868bX35rksS3r37JxytOX\nJvnM5rYoAAAAgHtdT093KqW8PcmXJzlSSplP8sYkP5DkHaWUb0vyu0m+fuPyn0/yVUmeSvK5JH+p\nl7UBAAAANElPQ5pa62tv8NBX7HJtTfIdvawHAAAAoKmaNDgYAAAA4MAS0gAAAAA0gJAGAAAAoAGE\nNAAAAAANIKQBAAAAaAAhDQAAAEADCGkAAAAAGkBIAwAAANAAQhoAAACABhDSAAAAADSAkAYAAACg\nAYQ0AAAAAA0gpAEAAABoACENAAAAQAMIaQAAAAAaQEgDAAAA0ABCGgAAAIAGENIAAAAANICQBgAA\nAKABhDQAAAAADSCkAQAAAGgAIQ0AAABAAwhpAAAAABpASAMAAADQAEIaAAAAgAYQ0gAAAAA0gJAG\nAAAAoAGENAAAAAANIKQBAAAAaAAhDQAAAEADCGlgiCwsdXLx6WeysNQZdCkAAADss9FBFwCDsrDU\nyfzicqYnxzM1MTbocm7psQuXcm52Lu1WK6vdbmbOHM/pE0cHXRYAAAD7REjDgTRsgcfCUifnZuey\nstrNSrpJkrOzczl17MhQBEwAAADcmu1OHDhbA4/LnbWsrHZzdnZu4FuIbraVaX5xOe3W9j/XdquV\n+cXlfpUHAABAj+mk4cDZDDw2O1KS5wKPQXWl3KqzZ3pyPKvd7rbnrHa7mZ4c73epAAAA9IhOGg6c\npgUet9PZMzUxlpkzx3O43cr9Y6M53G5l5sxxW50AAADuITppOHA2A4+z13WuDCrwuN3OntMnjubU\nsSNDNewYAACA2yek4UBqUuBxJ509UxNjwhkAAIB71C23O5VS2rusHelNOdA/UxNjedVLXzjw0MNW\nJgAAAJKbdNKUUh5J8i+TjJVSfiPJo7XWj248/O+T/NHelwcHQ5M6ewAAABiMm3XSzCT5M7XWB5K8\nNcl7SilfuvFY6XllcMA0pbMHAACAwbjZTJpDtdYnk6TW+s5Syn9O8m9KKW9IUvtSHQAAAMABcbOQ\nZrWU8gdrrb+XJLXWJ0spX5Hk3Un+UF+qAwAAADggbrbd6Q1JPm/rQq11PsmfTPIDvSwKAAAA4KC5\nYUhTa/1/aq0XSyl/87r1zyRZ6nllAAAAAAfILY/gTvKtu6z9xX2uAwAAAOBAu9kR3K9N8rokX1BK\nedeWh+5PstDrwgAAAAAOkpsNDv6VJB9PciTJW7asX04y18uiALg9C0udzC8uZ3py3PHtAAAw5G4Y\n0tRafyfJ7yT5b/tXDgC367ELl3Judi7tViur3W5mzhzP6RNHB10WAACwR7ecSVNK+dJSyn8spSyV\nUq6UUtZLKZ/tR3EA7G5hqZNzs3NZWe3mcmctK6vdnJ2dy8JSZ9ClAQAAe3Q7g4N/OMlrk/xWkvEk\n357kH/eyKABubn5xOe3W9n/C261W5heXB1QRAABwt242k+aaWutTpZSRWut6kv+rlPIrPa4LgJuY\nnhzPare7bW2128305PiAKgIAAO7W7XTSfK6UcijJhVLKTCnlu5Lc1+O6ALiJqYmxzJw5nsPtVu4f\nG83hdiszZ44bHgwAAEPsdjppvjlXw5zXJ/muJC9NcqaXRQFwa6dPHM2pY0ec7gQAAPeIW4Y0tdbf\nKaWMJ3lxrfVNfagJgNs0NTEmnAEAgHvE7Zzu9DVJLiT5hY3vT5RS3tXrwgAAAAAOktuZSfP9Sb4k\nyTNJUmu9kOSh3pUEAAAAcPDcTkizVmv9TM8rAQAAADjAbmdw8BOllNclGSmlvDzJ/5zEEdwAAAAA\n++iGnTSllH+58eVvJ3k4SSfJ25N8Nsl39r40AAAAgIPjZp00X1xK+fwkfyHJI0nesuWx5yVZ6WVh\nAAAAAAfJzUKaf5arJzq9LMn5LeslSd1YBwAAAGAf3HC7U631h2qt/02SH6u1vmzLxxfUWgU0AAAA\nAPvolqc71Vr/ej8KAZ6zsNTJxaefycJSZ9ClAAAA0Ce3c7oT0EePXbiUc7NzabdaWe12M3PmeE6f\nODrosgAAAOixW3bSAP2zsNTJudm5rKx2c7mzlpXVbs7OzumoAQAAOAD6HtKUUv5wKeXClo/PllK+\ns5Ty/aWUS1vWv6rftcGgzS8up93a/mfZbrUyv7g8oIoAAADol75vd6q1/pckJ5KklDKS5FKSn0ny\nl5L8w1rr3+93TdAU05PjWe12t62tdruZnhwfUEUAAAD0y6C3O31Fkt+utf7OgOuARpiaGMvMmeM5\n3G7l/rHRHG63MnPmeKYmxgZdGgAAAD026MHB35jk7Vu+f30p5VuSnE/y3bXWxeufUEp5NMmjSfLg\ngw/2pUjop9MnjubUsSOZX1zO9OS4gAYAAOCAKLXWwbxwKYeSfCzJw7XWT5RSPi/Jp5PUJG9O8uJa\n61++2T1OnjxZz58/3/tiAQAAAPaolPJ4rfXkra4b5HanP5vk12utn0iSWusnaq3rtdZukh9N8iUD\nrA0AAACgrwYZ0rw2W7Y6lVJevOWxr0vyRN8rAgAAABiQgcykKaU8L8mfTvJXtyzPlFJO5Op2p49e\n9xgAAADAPW0gIU2t9XNJpq5b++ZB1AIAAADQBIM+ghsAAACACGkAAAAAGkFIAwAAANAAQhoAAACA\nBhDSAAAAADSAkAYAAACgAYQ0AAAAAA0gpAEAAABoACENAAAAQAMIaQAAAAAaQEgDAAAA0ABCGgAA\nAIAGENIAAAAANICQBgAAAKABhDQAAAAADSCkAQAAAGgAIQ0AAABAAwhpAAAAABpASAMAAADQAEIa\nAAAAgAYQ0gAAAAA0gJAGAAAAoAGENAAAAAANIKQBAAAAaAAhDQAAAEADCGkAAAAAGkBIAwAAANAA\nQhoAAACABhDSAAAAADSAkAYAAACgAYQ0AAAAAA0gpAEAAABoACENAAAAQAMIaQAAAAAaQEgDAAAA\n0ABCGgAAAIAGENLAgCwsdXLx6WeysNQZdCkAAAA0wOigC4CD6LELl3Judi7tViur3W5mzhzP6RNH\nB10WAAAAA6STBvpsYamTc7NzWVnt5nJnLSur3ZydndNRAwAAcMAJaaDP5heX025t/9Nrt1qZX1we\nUEUAAAA0gZAG+mx6cjyr3e62tdVuN9OT4wOqCAAAgCYQ0jDUhnH47tTEWGbOHM/hdiv3j43mcLuV\nmTPHMzUxNujSAAAAGCCDgxlawzx89/SJozl17EjmF5czPTkuoAEAAEBIw3DaOnx3JVe3Dp2dncup\nY0eGJvCYmhgbmloBAADoPdudGEr7PXx3GLdNAQAAcG/RScNQ2s/hu8O8bQoAAIB7h04ahtJ+Dd/d\num3qcmctK6vdnJ2d01EDAABA3+mkYWjtx/DdzW1Tm3Ntkue2TZkXAwAAQD8JaWi0haXOTUOYux2+\nu5/bpgAAAOBuCGlorH7MitncNnX2utfRRQMAAEC/CWlopH4esb0f26YAAADgbglpaKR+z4q5221T\nAAAAcLec7kQjmRUDAADAQSOkoZH264htAAAAGBa2O9FYZsUAAABwkAhpaIQbHbV9/ayYWx3JDQAA\nAMNKSMPA3e5R2/04khsAAAAGxUwaBmrrUduXO2tZWe3m7OxcFpY6e7oOAAAAhpWQhoHaPGp7q82j\ntvdyHQAAAAwrIQ0DdbtHbTuSGwAAgHudkIaBut2jth3JDQAAwL2u1FoHXcOenTx5sp4/f37QZbAP\nbvfUJqc7AQAAMGxKKY/XWk/e6jqnO9EI1x+1fbfXAQAAwLCx3QkAAACgAYQ0AAAAAA0wsO1OpZSP\nJrmcZD3JWq31ZCnlDyT5qSQPJflokm+otS4OqkYAAACAfhl0J80jtdYTW4bnvCHJe2utL0/y3o3v\nAQAAAO55gw5prvfnk/zExtc/keRrB1gLAAAAQN8MMqSpSf59KeXxUsqjG2ufV2v9eJJsfH7R9U8q\npTxaSjlfSjn/qU99qo/lAgAAAPTOII/gPlVr/Vgp5UVJ3lNK+fDtPKnW+tYkb02SkydP1l4WCAAA\nANAvA+ukqbV+bOPzJ5P8TJIvSfKJUsqLk2Tj8ycHVR8AAABAPw0kpCml3FdKuX/z6yT/fZInkrwr\nybduXPatSR4bRH0AAAAA/Tao7U6fl+RnSimbNfzrWusvlFL+Y5J3lFK+LcnvJvn6AdUHAAAA0FcD\nCWlqrR9J8qpd1heSfEX/KwIAAAAYrKYdwQ0AAABwIAlpAAAAABpASAMAAADQAEIaGHILS51cfPqZ\nLCx1BnoPAAAA7s6gTneCRllY6mR+cTnTk+OZmhgbdDm37bELl3Judi7tViur3W5mzhzP6RNH+34P\nAAAA7p6QhgNvWEOKhaVOzs3OZWW1m5V0kyRnZ+dy6tiR2w6a9uMeAAAA7A/bnRg6+7k1Z2tIcbmz\nlpXVbs7Ozg3Ftp/5xeW0W9v/hNutVuYXl/t6DwAAAPaHThqGytaulyvr3bz+kWN53asf3HPXx2ZI\nsdlFkjwXUjS9k2R6cjyr3e62tdVuN9OT4329BwAAAPtDJw1D4/qul85aN295z2/mj//Ae/OuC5f2\ndM9hDimmJsYyc+Z4DrdbuX9sNIfbrcycOX5H4dJ+3AMAAID9oZOGobFb10uSdNbqnueobIYUZ6+b\nSXMn9xnk0OHTJ47m1LEjd/X6+3GPg2pYB04DAADNJKRhaOzW9bLpbrYo7TWkWFjq5Cd/7XfzI7/0\nVA6NDG7aK1gRAAAgAElEQVTo8NTE2F0HBPtxj4NmWAdOAwAAzWW7E0Njs+tlbHTnr+3dblGamhjL\nq176wtsOKh67cCl//Afem3/wnt9MZ234hg5zd4Z54DQAANBcQhqGyukTR/Mrb3hNvvtPf2HGRstA\n5qhsvkHvrNUdj+3XyUj7eYIV+8+pWAAAQC/Y7sTQmZoYy9/4ipfnda9+cCDzQOYXlzNSyq6P7cfQ\nYdtomm+YB04DAADNpZOGoXWnW5T26vqulicufSbPXlnfcd3Y6N139NhGMxx6dSqWDioAADjYdNLA\nTVzf1fJ9f+4VefPPfWjHdX/9T74s3/4nXnbXb9J3O8HqboYi0zv7fSqWDioAAEBIAzewtatlMzR5\n088+mUPXDS6+b2wkX/nKF+9LiGIbzXDZr1Oxdvtd2+ux8gAAwPCy3YkD4063kuw6HHaklSvr2wcG\nr3frvoUovdpGQ7M1ZRCx7VYAADBYOmk4EPaylWS3rpb1WvPGr3lF3vzuD227136GKPu9jYbma0IH\nle1WAAAweKXWnccID4uTJ0/W8+fPD7oMGm5hqZNTP/iLWVl97k3w4XYrHzj3mlsGIO+6cClnd3nj\nurDUEaKwr270u9YPd/M3AgAA3Fop5fFa68lbXaeThnve3QzjvVFXy37NIoFNg+ygMrAaAACaQUjD\nPe9ut5IIZOiXQf2uNWG7FQAAYHAwB4BhvHBz/kYAAKAZzKThwDBHBm7O3wgAAPSGmTRwHduW4Ob8\njQAAwGDZ7gQAAADQAEIaAAAAgAYQ0gAAAAA0gJAGAAAAoAGENAAAAAANIKQBAAAAaAAhDQAAAEAD\nCGk4MBaWOrn49DNZWOoMuhQAAADYYXTQBXDwLCx1Mr+4nOnJ8UxNjPXlNR+7cCnnZufSbrWy2u1m\n5szxnD5xtC+vDQAAALdDSENfDSIsWVjq5NzsXFZWu1lJN0lydnYup44d6VtIBAAAALdiuxN9szUs\nudxZy8pqN2dn53q+/Wh+cTnt1vZf9XarlfnF5Z6+LgAAANwJIQ19M6iwZHpyPKvd7ra11W4305Pj\nPX1dAAAAuBNCGvpmUGHJ1MRYZs4cz+F2K/ePjeZwu5WZM8dtdQIAAKBRzKShbzbDkrPXzaTpR1hy\n+sTRnDp2pO8DiwEAAOB2CWnoiRud4DTIsGRqYkw4AwAAQGMJadh3tzrBSVgCAAAAO5lJw77q5wlO\nC0udXHz6mZ6fDgUAAAD9oJOGfbV5gtNKnhsQvHmC0512z9xoy1Ry624dAAAAGDZCGvbVfp3gdLMQ\nZmu3zmYYdHZ2LqeOHbGNCgAAgKFluxP7autx1/eNjeTQaCvf99WvuGl4cv22pVttmdrs1tlqs1sH\nAAAAhpVOGvbd6RNHc3llLW/62SfTHmnlze/+UO4fG911O9JuHTOfP3Xfji1TI6Xklz78yTzyRS/a\nt24dAAAAaBKdNOy7haVO3vxzH8qV9Zpnr6zfcHjwbh0z3/POi7nv0MiOEObZK+v5/p99Mqd+8Bfz\ngac+fa1b5/6x0Rxut/J9f+4VmV9cNkQYAACAoaWThn232/DgrZ0wm1ufdruus1bz7574vcycOZ6z\ns3MZKSXPXllPkix1rn4+OzuXD5x7TT5w7jWZX1zOE5c+kzf/3IcMEQYAAGCo6aRh3+22HWlrJ8y7\nLly6dt2V9fUdz//hX3oqp44dyQfOvSZvOv1wJsZGtj2+9bSo6cnxvPnnPtSXI78BAACgl4Q07Ltt\nw4MPPRewLHW2b32amhjL6x95+Y7nHxp5LoR55ItelLVu3fb41vkzhggDAABwrxDS0BOnTxy9ZSdM\nkrzu1Q9mbHT7r+HWEGZr4LM5f2bmzPFrW6YMEQYAAOBeIaShZzY7YVbXb9wJMzUxlr/3P944hEme\nC3z+1be/Oh8495pt82ZuFeJsuv6YbwAAAGgag4Ppqfc/9emsb+l0aY+UXUOYU8eOZH5xOdOT4zsC\nluRqGLPb+u08f7djvg0WBgAAoGmENPTM5hHba1t2I7VKcurYkR3X3iyEuR1bn7+w1LkW2CS5dsz3\n5ilSZ2fncurYkbt6PQAAANhvQhp6Zrcjtg+NjFwbCtwL13fNfMeXH9tRw9bToQAAAKAphDTcsa2d\nKjcLOvo91Hezc2dr18wP/9JvJSl3XMPt/owAAACwX4Q03JE7me+yOdT37HXX9yr0uFHnzqNf9rL8\nyH946rZrMMMGAACAQRDScNt261S51XyX2xkKvF9u1Lnzulc/mNe9+sHbqmEvPyMAAADsB0dwc9s2\nO1W22pzvcjNTE2N51Utf2POQ42bHcd9uDXv9GQEAAOBu6aThtk1PjufKev9mzOzF3Xbu9HuODgAA\nAGzSScNte/9Tn876lgCjPVJuOt9lYamTi08/k4WlTr9KTHJ3nTs368YBAACAXtJJw23ZnNWytqXJ\npFWSU8eO7Hr9zYbvNv3kpH7O0QEAAIBNQhpuajNQ+czy6q4nJ80vLu8IMW40fPcVL35+Zn99Pv/8\nfR/JodGRrNfal5OT9hIKbc6xAQAAgH4R0nBDW7thrqx3t211Sm48q+XJj302rZRta7Vb82f+0fuy\n3q1Xn3tlPUnvT05ynDYAAADDwkwadrW1G+ZyZy2dtW5KKRkbLTed1fLYhUv5K287n8+trm9b76zX\nawHNViOl3PDkpLudaXP9z7Cy2s3Z2bm+z8gBAACA29H3TppSykuTvC3JH0zSTfLWWus/KqV8f5K/\nkuRTG5f+L7XWn+93fVy1eRT11u1Nh0dH8iPf9EfygvFDu24d2gxFOmvbO25aSbavPGd1ffdunP3o\ngNntZ9g8TttWJgAAAJpmENud1pJ8d63110sp9yd5vJTyno3H/mGt9e8PoCauc6OjqB9+yQtuGHDs\nFoqMt0ey1u2mu76ziyZJ3vg1D9/2TJs73RblOG0AAACGSd+3O9VaP15r/fWNry8n+c9JDAlpmL0c\nRb1bKLJea1pl57WtJH/n616Zb/rSz9/x2GbYs9VmB0yvfwYAAAAYlIEODi6lPJTkjyT5tSSnkry+\nlPItSc7narfN4i7PeTTJo0ny4IMP9q3Wg2jrUdT3HRrJs1fWs7DUydTE2K4nJm2GIme3bFP6W3/q\nC/N3/92Hd9z7HX/1S3PyC6Z2fd397IBxnDYAAADDYmAhTSllIslsku+stX62lPJPk7w5Sd34/JYk\nf/n659Va35rkrUly8uTJ3ffQsG+mJsby/qc+nbPvnMtIq2S9W/MX/th03nF+ftd5MdeHIj/5a7+7\n455jIyXt0ZFr318f+OwW9uiAAQAA4F43kJCmlNLO1YDmJ2ut/yZJaq2f2PL4jyZ59yBqO4h264rZ\n+tjf/umLWd0yU+ZtH7wavGydF/OKFz8/z15Z33aPJz/22fzj9/7mzhcs5VpXzI0GBJ86diRv/eYv\nTlLy8Euev+eAxhHcAAAADItBnO5UkvyLJP+51voPtqy/uNb68Y1vvy7JE/2u7SC6VYjxz9/3kW0B\nzY181Q+9L2OjI1ntdvMNJ6922qyv16zucqzT6x85dm3L1G4Dgi+vrOXNP/ehuw5W9msAMQAAAPRD\n3wcH5+rsmW9O8ppSyoWNj69KMlNK+U+llLkkjyT5rgHUdqBsDTEud9aystrN2dm5LCx1rj3+z9//\nX295n5XVbq6s12v3eNsHfzcrq92sdneGO4dGkte9+uosod0GBI+0St70s0/esKY7sV8DiAEAAKAf\n+t5JU2t9f5JdzvvJz/e7loNutyOzN0OMqYmxzC8uZ2y0ldX19W3PK0kOjbZyaKSVzno3pdZ0bqPb\nJkm+/oufG/a864Dg9Zpy3W9HrblW051wBDcAAADDZBCdNGxYWOrk4tPP7NolcrPH9sutQozpyfGs\n7dIN839+7SvzK294Tf7Vt786P/83/ruU3c7YvoHHLl7KqR/8xbzrwqVMTYzl+776FTk02sp9YyM5\n3G7lf/qTL0tnbftrdta6+bWPLNzxz+cIbgAAAIZJqXV4D0g6efJkPX/+/KDL2JObzYLZ67Dbmw0A\nvpF3Xbi04xSlra/1rguX8j3vnEurlKx3u3nj6YfzTa/+/B33+J53XsxIaWW9dvMlD/2BvO+p50KV\n0Vaydt1smsPtVr7vz70ib/65D2WklKyud/O1J16Sf3vx47ly/cVJDo2UfPB7v2JPActe/ncBAACA\n/VJKebzWevJW1w3sCO6D7GYDbZPsadjtXoOd64/M3voaC0udfHThc6m1m1JGUkpy/9jOX5nLK2up\nNaklSUo+uEvXy32HRvLslee2TW3OnrmyZZvUOx6/dMM62yOtPW15SnLtWG8AAABoMiHNANxsFszm\n1zeaE7Obuz3FaLcQ47ELl3L2nXPpbHS1XNmYS/O3f/piXvHi52fyvkOZX1zOr31kIX/333346pOu\nm12z6eqpT9s7tlbXa9ojrWv3vZX1Ws2SAQAA4J4mpBmAW82CudNht7caAHw7tm4JSq5283R22XZ0\nZb3mK//R+5LUjI1u7465kfVa88aveUXe/O7njtX+vq+++v2tPK/dSjcxSwYAAIB7npBmADYH2l4/\nC2YzhLjZY7u521OMrt8q9R1ffmxH6LPV5jDhtZsENCMled6h0W1br77y4T+4bVvV/WOj237Obzg5\nnXecn98W5LzyJS8wSwYAAIADweDgAbrZQNs7HXZ7qwHAN6vh1A/+YlZWnwtkxkZbubLWzd38ZoyN\ntvKj33IyD7/k+ZmaGNv151lY6uTJj302Sc3DL3nBDa/bKwODAQAAaAKDg4fAzQba3umw25sNAL6Z\n3bZK7bbN6U4dGmnlBePtTE2M7TrUuCa7Djrey5Df3cKYvQ5SBgAAgEER0txD9hJw7LZV6lZGW1c/\nj5SSzvru/Tab2612G2r8Pe+cS1LTWat7GnS81W5hzKljR+5qkDIAAAAMQmvQBTBYm/NxDrdbed6h\nkVte/7xDI/mxv/jH8gt/88tSS9nx+H1jIzncbl2bo7PZqbPVSKtkpGxf23q61e3aGgBd7qxlZbWb\ns7NzefJjn93xmnu5PwAAAPSTThqubZV68mOfybf9+H/M6k0aa7r16vyY+cXljI1cnV2z6b5DI3nT\n1zycR77oRdc6Vnbr1Flb7ybXTby5k0HHm250qlVS72qQMgAAAAyCThqSXO2o+bIvfFG+//Qrd338\n+g6Z3cKX9VrzyBe9KEnyy7/5qfzyb34ySa516hxuX/11K0lqSkZbyf1jo9vueydudKrVwy95wbXX\nvJv7AwAAQD853YkdfvLXfidvfOzJa0dtj5TkO//UF+Z1r35wW9Cx24lSl1fW8r8/9kQ2R9W0R0re\n8vWvyite/Px81Q+9L1e2zLDZPAHqJS84nGevrO/pFKabnWrldCcAAACa4HZPdxLSsMPCUid//Afe\nm87ac78bh9utfODca256VPgvPPF7+V//7RM77nc1jPnifMdP/kYud9aurd8/Npq/8mUvyz/5D0/d\n1SlMwhgAAACa7HZDGtud2GF+cTlll6HAuw3enZoYy6te+sIkyZve/aFd79cqSVJ2bE26st7Nj/zS\nb+0Y/Luw1LmjejdrENAAAAAwzIQ07HDfoZGsXDc9eGW1m/tucvrT/OJyDo3sDHaSZHm1mw//3md3\nzIl5/SPHcmhk+z1HStkWBi0sdXLx6WeuBTfXfw8AAAD3Cqc7scOzV9YzNlLS2To/ZqTk2SvrN3zO\n9OT4tRk2u/m7P//h/J2ve2U+cO4117YmLT57Jf/4F39rx2s/8bHP5FUvfWEeu3Ap57bMm/mGk9N5\nx/n5u9oaBQAAAE2lk4YdpifHU1rbu2JKq9z0COupibHMnDmesdFyw46aN/3s1e1Qr3rpC/P+pz6d\nr/7h9++6rerN7/5QnvrE5Zybndu2FeptH/zdu94aBQAAAE0lpGGHzcDlTo6wXljq5KMLn0utV4/Y\n3k175OpWpoWlzrUAprPW3Xldq5ULTz+Tduvmv57tVmvXOTkAAAAwjGx3YlenTxzNqWNHMr+4nPsO\njeTZK+tZWOrsGtQ8duFSvuenL247Xns3692a6cnxzC8up91qZSU7A5okubK+nsPtkVxZv/H2qiRZ\n7XZv2t0DAAAAw0RIww1NTYzl/U99ettcmOvnwCwsdfK3f/piVm8Q0Iy2krH2SNa7NTNnjidJPrO8\nmivrOwOa+w6NpLO2nm5Nvvff/Kd069Xnj7dHbziTxolOAAAA3CuENNzQ1m1Jm10v3/POubzweYfy\nkhcczrNX1vP07z97w4Am+f/bu/coOev6juOf7zMzO7vJ5rJsuCUbrkkOJDFZMAoapApKkUuwJxSQ\ni1gp6KmgVSFoKSKNthAU0IJyQCjQUmlIiqFA6y2oBA0QMFmTcDGCkE1oINtNyIbd2bn8+sc8M5nd\nmdlcmN3nmZn365yc3XnmmdnfnueXZ5LP+f2+X+nWc4/R5P1Gqa2lSSs2bNXcG5cr5nlKZzKKRUyN\n0YiSmYyuPWO6JreM0qX3r1IilVEynZIkxaOebr/gWM2YOFatzXF98eRp+cLDBDQAAAAAgFpCSIOy\nSm1LSqQyuuTeZ5TMSI0xT+khAppYxHTUQWO0sz+t7p39RYFPQ0S6/YJjNGPiOLU2x7Vm4zY1RLwB\ndWoinkly+UCmtTlOOAMAAAAAqEmENCirraVJyUzxtqSkf6gvWbqmTM7Hjj5QZ9y2QjEvG7w4NzDQ\n6U87/faVLo1raij7897pT+vS+1fpprNn024bAAAAAFDTbPB/nKvJnDlz3KpVq4IeRk37+rLf6/7f\nvr5Pr414pnRm9/NrdENEaberZs1VSzqKuj41xjw9dfVJrKIBAAAAAFQdM3vOOTdnd+fRgrvGdPUk\ntGbjNnX1JCryXg+s3LeARtIeBTSStLM/rb5kRguWdmjulAm661NzNCoWGXAO7bYBAAAAALWO7U41\nZNnqTVqwZI1MUjLjdPWpR+myE4/c5/e768lXtJuu2hWVC2JmTByrjAb+YNptAwAAAABqHStpakRX\nT0JfWbxaiZRTX8opnZH+8fEXtWDJmn1+v7tXvFrhUe4yqqF46uWCmNbmuBbNn6XGmKcx8agaYx7t\ntgEAAAAANY+VNDVi3ea3lSpRx3fxqk5d9qEjNOXAMXv1fp3dvWqIeEqm0xUaoTQqFlFGTteeMV0z\nJ47T2k3btfCx9Yp5npKZzIAgZl77JM2dMoF22wAAAACAukFIUyOWv7Cl7HOrN27b65BmdENEyQru\ndYpHPd1x0Xs1Y+LYfODS1tKkyfuNkuTybbgL0W4bAAAAAFBPCGlqQFdPQg88U77Ab/vk8Xv1fg+s\nfE3X/9c6Zfaw8O/uxCKmm86epROn7Z8/tmz1Jl29tGPAKhpabAMAAAAA6hkhTQ1Yt3m7huqk/lrX\nzj1eSfPAytd0zY/XvusxRT3Tree2a2xTtGiVTFdPQlcv7VBfMqM+Zfdo5To7sXIGAAAAAFCvKBxc\n5Zat3qRL71+l1BCrXm79+Ut79F5dPQld/+j6iowrlXEa2xTVidMOKApeOrt7FfMGTr3dtdiuZGtx\nAAAAAADCiJU0VSy3IiWRGnpb0mtdvVqzcVu+hXVnd69GN0S0sz+d/9rW0qR1m7crXaEtTllWNN7c\nz05mBlY5HqrFNlujAAAAAAD1gJCmiuVWpOS2DJXzdiKtC3/4tPpSaTnnFI146ktmFDEp7aTGmKd0\nximVdqpkRPPiG2/n69AMDlrOmdOmxas6S3Z2KsTWKAAAAABAvSCkqWJtLU1FK1LK2ZFI5b9P+a/J\nNW/qS+7Ze+ytb//0Jc1/b5skFQUti1d16tHLT8iv4ikXuJQKonJbowhpAAAAAAC1hJo0Vay1Oa5r\nT58e9DDKyqSdOrt7y9ag2dmf1uzJ44cMW0oFUUNtjQIAAAAAoFoR0lS5yfuNCnoIZaUkPf1K17sK\nWlqb47r2jOlqiHoaHY+oMeaV3RoFAAAAAEA1I6Spcpu739mr801SQ2R4xlLKt3/6siRp0fxZaox5\nGhOPFgUtQ3VuWrZ6kxY+ul4xz5RMZXTt6dMpGgwAAAAAqEnUpKlyL2/ZsVfnO2Vr0UQ8q3Anp/I/\nsbO7V3OnTNC3z56trT0JnTBlgqYcOEbSroLCUc/Un3a67szpuuC4QyUNLBqcs/Cx9Tp15kGspAEA\nAAAA1BxCmirX2d27169JZyRVtI9Tef1pp+UvbNH3f/VHJf1KxVFPuvmcds2dMqEohLnm4bWSky44\n/lCKBgMAAAAA6grbnarcuk3bgx7Cbn13+YZ8QCNJqYx05UNrtG7z24qYFZ3/9WVr1dWToGgwAAAA\nAKCuENJUuf99u7iOS6G28XHFo+G7zP1pp/t/86p29qeLnks7ad3mt9XaHB+ylg0AAAAAALWE7U7V\nzjTkzqXObQkt+ezxevh3m/XQcxsV9Ty9kywORoLw8xffGuLZ7C81r32S5k6ZoM7uXrW1NFU0oOnq\nSQzL+wIAAAAAsC8IaapczJPSu8lczr1zpUY1RGUmXfzBQ3XXk68qNSJFg/fdxHG7tjTlApRc/Z1K\nBCq5gsUxz1Myk9Gi+bPoGgUAAAAACBQhTZWLRiy7P2gIaSftSKQkSXeveMUvHBxejTFvwDaoSgcq\nhV2jckWJFyzt0NwpE1hRAwAAAAAITPiKlWCvjGtq2Kvz+9Mj1dfp3WlraVJXT0K/fvktLViSDVR2\nJFLqS2a0YGmHunp21eLp6klozcZtA44NJdc1qlCuaxQAAAAAAEFhJU2V++jRB+q+la8HPYx91hjz\nNP+YNi15fqMaIpH8SpkVG7bq6qUd8mRKpAYu/Slsw70vq2zoGgUAAAAACCNCmir3vsP3q+qQJp1x\n+vIp0/TlU6bli/hK0twbl6svWXpfVi5Q2ddtS7muUQsGhTtsdQIAAAAABImQpop19SS0tac/6GEM\nEI+a+lNuj7dUZTJOT23Yqnntk/IhyZqN2xTzvHzwkjOqIaKMc/lApdR5hatshjKcXaMAAAAAANgX\nhDRVKrfNx1y4KswkUuXHE4+YMpKSBYWO06549Uup7UjxqOmOC4/VjInjhjxvb7YttTbHCWcAAAAA\nAKFB4eAqVLjNp3eIUCRszDPdcs5sjYpFBhwfXLQ3tx2pMeZpTDyqxpinm86erROnHTAgVCl1HtuW\nAAAAAADVipU0VSjXnWjwdqAwinpSUyyar/ty1EFjlRq0+qfU6pc93Y7EtiUAAAAAQK0gpKlCbS1N\n6k+ngx7GHrn13HZN3m+02lqatGLDVp1x24r8Fq3GWHYh16L5syRla9EUBi17uh2JbUsAAAAAgFrA\ndqcq1Noc1+UfmRr0MHYr6kkfOHKCZk8eL0n5LVoJvyZNJuP06OUnyCnbzenCHz6tuTcu1yOrNwU4\nagAAAAAAgkFIU6XOP+4QNUQs6GEUiXrZFTKxiOn6s2bmV7jktmgVikcj2ry9Lx/e7Eik1JfMaMHS\nDnX1JIIYPgAAAAAAgSGkqVKtzXFdcVL4VtOc/p6Dlck4NUQ8LXx0fX5VTLlOTJIrCm8GFxIGAAAA\nAKAeENJUsfOPOyToIRRZtuYN9aeddvanB6yKKdWJ6drTp0tSUX2dvWmjDQAAAABAraBwcBWrhmK5\nJmnd5u06cdoBAzoxrd20XQsfW6+Y5ynjirtAVcPvBgAAAABAJRHSVLGZ1z0e9BB2qzeZ0WfufVY3\nn9Ouee2T8uHLuXf+Vn3JTL6NeDzq6fYLjtWMiWMJaAAAAAAAdYntTlXqx89vVE/CBT2MIjMOHlN0\nLJWRrlqyqxhwqSLCDRFP45pikrKtuCkcDAAAAACoN6ykqVJ3/OqPQQ+hSENE+sObO0s+51w2nGlt\njpctIrzylS5952cvqyFiSmWcFs2fpXntk0Zi6AAAAAAABI6VNFWoqyehF7eUDkOCMLohosaYpytO\nmqaGaOm24P3pjEY3RCSpZBHhebMP1j/994vqT2XUk0jTihsAAAAAUHdYSVNlunoSevCZ14MehiIm\nRSOevn7mdM2cOC7fjen2X24oeX48YtrZv6uLU2ER4dENEZ32vSeLf4Zn+dU3AAAAAADUOkKaKrJs\n9SZd+dAaJdPB16L53J8doc+ccERRgLJo/ixdtaRDidTA7UzmWVFb7dbmuFqb41qzcZtiEa+4FXfa\n0YobAAAAAFA32O5UJbp6EqEJaCSVDGik7AqZ33z1JH3lY9MUj1p+O9NQbbXbWpqUdsW/13VnTmcV\nDQAAAACgboRuJY2ZnSrpu5Iikn7onLsh4CGFwrrN20MT0MQipqc2bC1b1Le1Oa4rTp6q8487RJ3d\nvWpraRoybMnVqFmwtEMRMyXTGV135gxdcNyhw/UrAAAAAAAQOqEKacwsIul2SR+T1CnpWTN7xDm3\nPtiRhUHpgrxBSKadFizt0NwpE3YbvuzpSpjCGjW7C3UAAAAAAKhFoQppJL1f0gbn3CuSZGYPSjpL\nUt2HNDMmjlXUkwaVehl2US/bDnuw4SjquzehDgAAAAAAtSZsNWkmSdpY8LjTP5ZnZpeZ2SozW/XW\nW2+N6OCC1Noc183ntI/oz4ya5FnpLVYU9QUAAAAAoLLCFtKU2tMzICVwzt3pnJvjnJuz//77j9Cw\nwmFe+yQ99/cfHbGNTykn9adLP0dRXwAAAAAAKitsIU2npMkFj9skbQ5oLKHU2hzXqzecrkvnBlNU\nN+pJ3/rETIr6AgAAAABQYWGrSfOspKlmdrikTZLOk3R+sEMKp2vOnKlrzpwpSdqwZYcWPrpev/rD\n1gHnmAYtQ6qAdEYa0xi2aQMAAAAAQPUL1Uoa51xK0uWSfiLpBUmLnXPrgh1V+E05cIxuPrdd8ejA\njVDDsS3KSbpqyRp19SSG4d0BAAAAAKhfoQppJMk597hzbppz7kjn3LeCHk+16OzuVUMkMuBYqUZQ\nsYjpUx84RI0xT6MbIiXOyIoMkfBEzFNnd+8+jhQAAAAAAJQSupAG+6atpUnJzND9uRujnu6++H36\nh/IglKsAAAtjSURBVLPeo0cvP0HvP7yl7LnpIfZJpV2Gzk4AAAAAAFQYIU2NaG2Oa9H8WWqMeRoT\njyoeNcUGL4cxacbEsVq2epNO/+cn9cRLW0u+l0mKR0tPjYhJN509m85OAAAAAABUGBVga8i89kma\nO2WCOrt71dbSpKc2bNWCpR2KeZ6SmYwWzZ+l7p39umpJh/pTpZfKmKSHPnu8LrznmaLnPv2BQ3XF\nyVMJaAAAAAAAGAaENDWmtTmeD1EGhzYrNmzVad97Uv1D7GX65idmas7hrVo0f5YWLO1QxDMl007X\nnTmdttsAAAAAAAwjQpoalwttunoSunppR9mApiHi6bp5u4KYwQEPq2cAAAAAABhehDQ1qqsnMSBg\nKdeNKRYxfeGkqTr/uEOKgpjCVTkAAAAAAGB4EdLUoGWrN+nqglo0154xXeMaY+pLFnd/+tFfH6c5\nh7cGMEoAAAAAAFCIkKbG5LY19SUz6lM2lLnm4bVqihV3a4pHTLFoZKSHCAAAAAAASqAFd43p7O5V\nzCu+rL0lVtGYZ2praRqJYQEAAAAAgN1gJU2NaWtpUjJTHMgUGhWLKCOnRfNnUXMGAAAAAICQIKSp\nMa3N8QHts3cm0gOej0c93XHRezVj4lgCGgAAAAAAQoSQpgYVts9eu2m7Fj62Pl9EeNH8WTpx2v5F\nrxncDQoAAAAAAIwsQpoalWufPXvyeJ0686AhA5jB3aAWzZ+lee2TAhg1AAAAAAD1i5CmDuQCm1JK\ndYNasLRDc6dMYEUNAAAAAAAjiO5Oda5UN6iY56mzu1dSNsRZs3GbunoSQQwPAAAAAIC6wUqaOleq\nG1Qyk1FbSxPboAAAAAAAGEGspKlzuW5QjTFPY+JRNcY8LZo/S5Ly26B2JFLqS2a0YGkHK2oAAAAA\nABgmrKTBgG5QueLCazZuU8zz8nVqpF3boKhVAwAAAABA5RHSQFJxceGhtkEBAAAAAIDKY7sTSiq3\nDYpVNAAAAAAADA9W0qCsUtugAAAAAADA8CCkwZAGb4MCAAAAAADDg+1OAAAAAAAAIUBIAwAAAAAA\nEAKENAAAAAAAACFASAMAAAAAABAChDQAAAAAAAAhQEgDAAAAAAAQAoQ0AAAAAAAAIUBIAwAAAAAA\nEAKENAAAAAAAACFASAMAAAAAABAChDQAAAAAAAAhQEgDAAAAAAAQAoQ0AAAAAAAAIUBIAwAAAAAA\nEAKENAAAAAAAACFASAMAAAAAABAChDQAAAAAAAAhQEgDAAAAAAAQAoQ0AAAAAAAAIUBIAwAAAAAA\nEAKENAAAAAAAACFASAMAAAAAABAC5pwLegz7zMzekvRa0OMIgQmStgY9CASKOQDmAJgD9Y3rD+YA\nmANgDoTboc65/Xd3UlWHNMgys1XOuTlBjwPBYQ6AOQDmQH3j+oM5AOYAmAO1ge1OAAAAAAAAIUBI\nAwAAAAAAEAKENLXhzqAHgMAxB8AcAHOgvnH9wRwAcwDMgRpATRoAAAAAAIAQYCUNAAAAAABACBDS\nAAAAAAAAhAAhTZUzs1PN7CUz22BmXw16PBgZZvYnM/u9ma02s1X+sf3M7Gdm9gf/a0vQ40RlmNk9\nZvamma0tOFbyelvW9/x7QoeZHRvcyFEpZebAN8xsk38fWG1mpxU89zV/DrxkZn8ezKhRSWY22cye\nMLMXzGydmX3RP869oA4Mcf25D9QJM2s0s2fMbI0/B673jx9uZk/794D/MLMG/3jcf7zBf/6wIMeP\nd2+IOXCvmb1acB9o94/zOVClCGmqmJlFJN0u6eOSpkv6pJlND3ZUGEEfcc61O+fm+I+/KukXzrmp\nkn7hP0ZtuFfSqYOOlbveH5c01f9zmaQfjNAYMbzuVfEckKRb/PtAu3PucUnyPwfOkzTDf833/c8L\nVLeUpK84546WdLykz/vXmntBfSh3/SXuA/UiIekk59xsSe2STjWz4yXdqOwcmCqpW9Il/vmXSOp2\nzk2RdIt/HqpbuTkgSVcV3AdW+8f4HKhShDTV7f2SNjjnXnHO9Ut6UNJZAY8JwTlL0n3+9/dJ+kSA\nY0EFOed+Len/Bh0ud73PknS/y1opabyZHTwyI8VwKTMHyjlL0oPOuYRz7lVJG5T9vEAVc8694Zx7\n3v9+h6QXJE0S94K6MMT1L4f7QI3x/y73+A9j/h8n6SRJS/zjg+8BuXvDEkknm5mN0HAxDIaYA+Xw\nOVClCGmq2yRJGwsed2roD2zUDifpp2b2nJld5h870Dn3hpT9x5ykAwIbHUZCuevNfaG+XO4vYb6n\nYIsjc6DG+dsWjpH0tLgX1J1B11/iPlA3zCxiZqslvSnpZ5L+KGmbcy7ln1J4nfNzwH9+u6TWkR0x\nKm3wHHDO5e4D3/LvA7eYWdw/xn2gShHSVLdSaTg91evDXOfcscouY/y8mZ0Y9IAQGtwX6scPJB2p\n7JLnNyR9xz/OHKhhZtYsaamkv3XOvT3UqSWOMQ+qXInrz32gjjjn0s65dkltyq6MOrrUaf5X5kAN\nGjwHzGympK9JOkrS+yTtJ+lq/3TmQJUipKlunZImFzxuk7Q5oLFgBDnnNvtf35T0sLIf1FtySxj9\nr28GN0KMgHLXm/tCnXDObfH/sZaRdJd2bWVgDtQoM4sp+x/0B5xz/+kf5l5QJ0pdf+4D9ck5t03S\nL5WtTzTezKL+U4XXOT8H/OfHac+3zSLkCubAqf52SOecS0j6F3EfqHqENNXtWUlT/aruDcoWiHsk\n4DFhmJnZaDMbk/te0imS1ip77S/2T7tY0rJgRogRUu56PyLpU35F/+Mlbc9thUBtGbSv/C+UvQ9I\n2Tlwnt/Z43BlCwY+M9LjQ2X5tSTulvSCc+7mgqe4F9SBctef+0D9MLP9zWy8/32TpI8qW5voCUln\n+6cNvgfk7g1nS1runGMVRRUrMwdeLAjqTdmaRIX3AT4HqlB096cgrJxzKTO7XNJPJEUk3eOcWxfw\nsDD8DpT0sF/7LSrp351z/2Nmz0pabGaXSHpd0l8GOEZUkJn9SNKHJU0ws05J10m6QaWv9+OSTlO2\nSOQ7kv5qxAeMiiszBz7st9l0kv4k6bOS5JxbZ2aLJa1XtiPM551z6SDGjYqaK+kiSb/36xFI0t+J\ne0G9KHf9P8l9oG4cLOk+v0uXJ2mxc+5RM1sv6UEz+6ak3ykb5sn/+q9mtkHZFTTnBTFoVFS5ObDc\nzPZXdnvTakmf88/nc6BKGYEqAAAAAABA8NjuBAAAAAAAEAKENAAAAAAAACFASAMAAAAAABAChDQA\nAAAAAAAhQEgDAAAAAAAQAoQ0AACgppnZF8zsBTN7YC9fd5iZnV/wuNXMnjCzHjO7rfIjBQAA9Y6Q\nBgAA1Lq/kXSac+6CvXzdYZLOL3jcJ+laSVdWaFwAAAADENIAAICaZWZ3SDpC0iNmdo2Z3WNmz5rZ\n78zsLP+cw8zsSTN73v/zQf/lN0j6kJmtNrMvOed2OudWKBvWAAAAVJw554IeAwAAwLAxsz9JmiPp\ny5LWO+f+zczGS3pG0jGSnKSMc67PzKZK+pFzbo6ZfVjSlc65Mwa936clzXHOXT6CvwYAAKgD0aAH\nAAAAMEJOkTTPzHLblRolHSJps6TbzKxdUlrStIDGBwAA6hwhDQAAqBcmab5z7qUBB82+IWmLpNnK\nbgVnOxMAAAgENWkAAEC9+ImkK8zMJMnMjvGPj5P0hnMuI+kiSRH/+A5JY0Z8lAAAoG4R0gAAgHqx\nUFJMUoeZrfUfS9L3JV1sZiuV3eq00z/eISllZmvM7EtSvr7NzZI+bWadZjZ9JH8BAABQ2ygcDAAA\nAAAAEAKspAEAAAAAAAgBQhoAAAAAAIAQIKQBAAAAAAAIAUIaAAAAAACAECCkAQAAAAAACAFCGgAA\nAAAAgBAgpAEAAAAAAAiB/wfQfjMNdN6RggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f054ff9e630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "pd.DataFrame(X_train_level2, columns=['feat1', 'feat2']).plot(x='feat1', y='feat2', kind='scatter', figsize=(19.20, 10.80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $\\alpha$ out of `alphas_to_try` array. Remember, that you need to use train meta-features (not test) when searching for $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.765000; Corresponding r2 score on train: 0.627255\n"
     ]
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "r2_train_simple_mix = 0\n",
    "# YOUR CODE GOES HERE\n",
    "for alpha in alphas_to_try:\n",
    "    r2 = r2_score(y_train_level2, alpha * X_train_level2[:, 0] + (1 - alpha) * X_train_level2[:, 1])\n",
    "    if r2 > r2_train_simple_mix:\n",
    "        best_alpha = alpha\n",
    "        r2_train_simple_mix = r2\n",
    "\n",
    "print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the $\\alpha$ you've found to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for simple mix is 0.781144\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_alpha * X_test_level2[:, 0] + (1 - best_alpha) * X_test_level2[:, 1]# YOUR CODE GOES HERE\n",
    "r2_test_simple_mix = r2_score(y_test, test_preds)# YOUR CODE GOES HERE\n",
    "\n",
    "print('Test R-squared for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_level2, y_train_level2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute R-squared on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for stacking is 0.632176\n",
      "Test  R-squared for stacking is 0.771297\n"
     ]
    }
   ],
   "source": [
    "train_preds = lr.predict(X_train_level2)# YOUR CODE GOES HERE\n",
    "r2_train_stacking = r2_score(y_train_level2, train_preds)# YOUR CODE GOES HERE\n",
    "\n",
    "test_preds = lr.predict(X_test_level2)# YOUR CODE GOES HERE\n",
    "r2_test_stacking = r2_score(y_test, test_preds)# YOUR CODE GOES HERE\n",
    "\n",
    "print('Train R-squared for stacking is %f' % r2_train_stacking)\n",
    "print('Test  R-squared for stacking is %f' % r2_test_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. **Examine and compare** train and test scores for the two methods. \n",
    "\n",
    "And of course this particular case does not mean simple mix is always better than stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all done! Submit everything we need to the grader now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task best_alpha is: 0.765\n",
      "Current answer for task r2_train_simple_mix is: 0.627255043446\n",
      "Current answer for task r2_test_simple_mix is: 0.781144169579\n",
      "Current answer for task r2_train_stacking is: 0.632175561459\n",
      "Current answer for task r2_test_stacking is: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "from grader import Grader\n",
    "grader = Grader()\n",
    "\n",
    "grader.submit_tag('best_alpha', best_alpha)\n",
    "\n",
    "grader.submit_tag('r2_train_simple_mix', r2_train_simple_mix)\n",
    "grader.submit_tag('r2_test_simple_mix',  r2_test_simple_mix)\n",
    "\n",
    "grader.submit_tag('r2_train_stacking', r2_train_stacking)\n",
    "grader.submit_tag('r2_test_stacking',  r2_test_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these numbers:\n",
      "Task best_alpha: 0.765\n",
      "Task r2_train_simple_mix: 0.627255043446\n",
      "Task r2_test_simple_mix: 0.781144169579\n",
      "Task r2_train_stacking: 0.632175561459\n",
      "Task r2_test_stacking: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = '' # EMAIL HERE\n",
    "STUDENT_TOKEN = '' # TOKEN HERE\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
