## Week â„–1

- [Explanation of Random Forest](https://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)
- [Explanation/Demonstration of Gradient Boosting](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)
- [Example of kNN](https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/)

### Overview of methods
- [Scikit-Learn (or sklearn) library](https://scikit-learn.org/stable/)
- [Overview of k-NN (sklearn's documentation)](https://scikit-learn.org/stable/modules/neighbors.html)
- [Overview of Linear Models (sklearn's documentation)](https://scikit-learn.org/stable/modules/linear_model.html)
- [Overview of Decision Trees (sklearn's documentation)](https://scikit-learn.org/stable/modules/tree.html)
- Overview of algorithms and parameters in [H2O documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)

### Additional Tools
- [Vowpal Wabbit repository](https://github.com/VowpalWabbit/vowpal_wabbit)
- [XGBoost repository](https://github.com/dmlc/xgboost)
- [LightGBM repository](https://github.com/Microsoft/LightGBM)
- [Interactive demo of simple feed-forward Neural Net](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.17601&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
- Frameworks for Neural Nets: [Keras](https://keras.io/), [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), [MXNet](https://mxnet.apache.org/versions/1.8.0/), [Lasagne](https://lasagne.readthedocs.io/en/latest/)
- [Example from sklearn with different decision surfaces](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)
- [Arbitrary order factorization machines](https://github.com/geffy/tffm)

### StandCloud Computing
- [AWS](https://aws.amazon.com/ru/), [Google Cloud](https://cloud.google.com/), [Microsoft Azure](https://azure.microsoft.com/ru-ru/)

### AWS spot option
- [Overview of Spot mechanism](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html)
- [Spot Setup Guide](https://datasciencebowl.com/aws_guide/)

### Stack and packages
- [Basic SciPy stack (ipython, numpy, pandas, matplotlib)](https://www.scipy.org/)
- [Jupyter Notebook](https://jupyter.org/)
- [Stand-alone python tSNE package](https://github.com/danielfrg/tsne)
- Libraries to work with sparse CTR-like data: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)
- Another tree-based method: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))
- Python distribution with all-included packages: [Anaconda](https://anaconda.org/anaconda/python)
- [Blog "datas-frame" (contains posts about effective Pandas usage)](https://tomaugspurger.github.io/)

### Feature preprocessing
- [Preprocessing in Sklearn](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Andrew NG about gradient descent and feature scaling](https://www.coursera.org/learn/machine-learning)
- [Feature Scaling and the effect of standardization for machine learning algorithms](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)

### Feature generation
- [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
- [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)

### Bag of words
- [Feature extraction from text with Sklearn](https://scikit-learn.org/stable/modules/feature_extraction.html)
- [More examples of using Sklearn](https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/)

### Word2vec
- [Tutorial to Word2vec](https://www.tensorflow.org/tutorials/text/word2vec)
- [Tutorial to word2vec usage](https://rare-technologies.com/word2vec-tutorial/)
- [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)
- [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)

### NLP Libraries
- [NLTK](http://www.nltk.org/)
- [TextBlob](https://github.com/sloria/TextBlob)

### Feature extraction from images
#### Pretrained models
- [Using pretrained models in Keras](https://keras.io/api/applications/)
- [Image classification with a pre-trained deep neural network](https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11)

#### Finetuning
- [How to Retrain Inception's Final Layer for New Categories in Tensorflow](https://www.tensorflow.org/tutorials/image_retraining)
- [Fine-tuning Deep Learning Models in Keras](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)