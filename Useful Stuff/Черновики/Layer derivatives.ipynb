{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5091a627-c565-4fc0-8278-be3b486c4bad",
   "metadata": {},
   "source": [
    "# Notebook for visualizing how backward pass is performed step by step through different layers with comparison with torch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a740f52-ea61-4384-bcc4-663cb9934229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Union, Tuple\n",
    "from IPython.display import HTML, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b487cbcd-9344-433a-9fe4-d886f3d4830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens =  136\n"
     ]
    }
   ],
   "source": [
    "BOS, EOS = ' ', '\\n'\n",
    "\n",
    "data = pd.read_json(\"./arxivData.json\")\n",
    "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:128], axis=1) \\\n",
    "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
    "            .tolist()\n",
    "\n",
    "tokens = list(set(''.join(lines)))\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print('num_tokens = ', num_tokens)\n",
    "\n",
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "\n",
    "def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first=True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, data))\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[c] for c in data[i]]\n",
    "        data_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        data_ix = np.transpose(data_ix)\n",
    "\n",
    "    return data_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024104d1-36d2-40ae-88f5-8564481f7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        return self.forward(x, grad)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37479b61-d101-4842-93aa-c74412712114",
   "metadata": {},
   "source": [
    "# emb check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288445f6-69a1-4f84-b836-072acc078598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(BaseLayer):\n",
    "    def __init__(self, n_input, emb_dim, pad_idx=None):\n",
    "        self.n_input = n_input\n",
    "        self.emb_dim = emb_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.weights = np.random.normal(scale=np.sqrt(2/(n_input+emb_dim)), size=(n_input, emb_dim))\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.weights_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.weights_optimizer.set_weight(self.weights)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        self.input = x\n",
    "        return self.weights[x]\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        weights_grad = np.zeros_like(self.weights)\n",
    "        input_shape_len = len(self.input.shape)\n",
    "\n",
    "        if input_shape_len == 2:\n",
    "            for batch_n, s in enumerate(self.input):\n",
    "                for i, emb_i in enumerate(s):\n",
    "                    weights_grad[emb_i] += output_error[batch_n][i]\n",
    "\n",
    "        elif input_shape_len == 1:\n",
    "            for i, emb_i in enumerate(self.input):\n",
    "                weights_grad[emb_i] += output_error[i]\n",
    "\n",
    "        if self.pad_idx is not None:\n",
    "            weights_grad[self.pad_idx] = 0\n",
    "\n",
    "        # self.weights = self.weights_optimizer.step(weights_grad)\n",
    "        return weights_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35730b0d-e52c-4ce9-aa9b-d2a935159e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (136, 16)\n",
      "Forward совпадает: True\n",
      "Градиенты совпадают True\n"
     ]
    }
   ],
   "source": [
    "# проверка на то, что градиент эмбеддингов считается правильно\n",
    "sample = to_matrix(np.random.choice(lines, size=5), token_to_id, max_len=130)\n",
    "emb = Embedding(len(token_to_id), 16)\n",
    "print(\"Embeddings shape:\", emb.weights.shape)\n",
    "\n",
    "torch_emb = torch.nn.Embedding(len(token_to_id), 16)\n",
    "torch_emb.weight.data = torch.as_tensor(emb.weights)\n",
    "\n",
    "torch_out = torch_emb(torch.as_tensor(sample))\n",
    "\n",
    "print(\"Forward совпадает:\", np.allclose(torch_out.detach().numpy(), emb(sample)))\n",
    "\n",
    "check_error = np.random.normal(0, 100, torch_out.shape)\n",
    "check_error_torch = torch.tensor(check_error)\n",
    "\n",
    "torch_out.backward(check_error_torch)\n",
    "\n",
    "print(\"Градиенты совпадают\", np.allclose(torch_emb.weight.grad.detach().numpy(), emb.backward(check_error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9b90e-64ec-4927-b265-659782fade5c",
   "metadata": {},
   "source": [
    "# conv1d check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a41a1e-3f9c-4e3c-a7b0-85a0c178645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dVanilla(BaseLayer):\n",
    "    \"\"\"\n",
    "    Сверточный слой, со страйдом 1 и без паддингов, для 1 предложения, не батча\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        scale = np.sqrt(1/(in_channels*kernel_size))\n",
    "        self.kernel = np.random.uniform(-scale, scale, size=(out_channels, in_channels, kernel_size))\n",
    "        self.bias = np.random.uniform(-scale, scale, size=(out_channels))\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.kernel_optimizer = copy.copy(optimizer)\n",
    "        self.bias_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.kernel_optimizer.set_weight(self.kernel)\n",
    "        self.bias_optimizer.set_weight(self.bias)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        self.input = x\n",
    "\n",
    "        self.output_len = x.shape[0] - self.kernel_size + 1\n",
    "        output = np.zeros(shape=(self.output_len, self.out_channels))\n",
    "\n",
    "        for kernel_i, ker in enumerate(self.kernel):\n",
    "            for i in range(self.output_len):\n",
    "                output[i:self.kernel_size+i, kernel_i] = self.bias[kernel_i] + np.sum(x[i:self.kernel_size+i, :] * ker.T)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        dy_dkernel = np.zeros(shape=self.kernel.shape)\n",
    "        dy_dbias = np.zeros(shape=self.bias.shape)\n",
    "        dy_dx = np.zeros(shape=self.input.shape)\n",
    "\n",
    "        for kernel_i, ker in enumerate(self.kernel):\n",
    "            helper_k = np.zeros(shape=ker.T.shape)\n",
    "\n",
    "            for i in range(self.output_len):\n",
    "                helper_k += self.input[i:self.kernel_size+i, :] * output_error[i, kernel_i]\n",
    "                dy_dx[i:self.kernel_size+i, :] += ker.T * output_error[i, kernel_i]\n",
    "\n",
    "            dy_dkernel[kernel_i] = helper_k.T\n",
    "            dy_dbias[kernel_i] = np.sum(output_error[:, kernel_i])\n",
    "\n",
    "        # self.kernel = self.kernel_optimizer.step(dy_dkernel)\n",
    "        # self.bias = self.bias_optimizer.step(dy_dbias)\n",
    "\n",
    "        # return dy_dx\n",
    "        return dy_dkernel, dy_dbias, dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dbbea5d-0dc8-4c77-9574-349d24d48e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода свертки: (126, 4)\n",
      "Градиент по входу совпадает: True\n",
      "Градиент по смещениям совпадает: True\n",
      "Градиент по ядру совпадает: True\n"
     ]
    }
   ],
   "source": [
    "sample = to_matrix(np.random.choice(lines, size=5), token_to_id, max_len=130)\n",
    "\n",
    "emb = Embedding(len(token_to_id), 16)\n",
    "encoded_data = emb(sample[0])\n",
    "\n",
    "conv = Conv1dVanilla(16, 4, 5)\n",
    "torch_conv = torch.nn.Conv1d(16, 4, 5)\n",
    "torch_conv.weight.data = torch.as_tensor(conv.kernel)\n",
    "torch_conv.bias.data = torch.as_tensor(conv.bias)\n",
    "\n",
    "torch_input = torch.tensor(encoded_data[np.newaxis, :], dtype=torch.float64, requires_grad=True)\n",
    "torch_out = torch_conv(torch_input.permute(0, 2, 1))  # permute потому что torch на вход принимает формат [BATCH_SIZE, EMB_DIM, SENTENCE_LEN]\n",
    "\n",
    "# проверка что forward работает также как у torch, \n",
    "print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.permute(0, 2, 1).detach().numpy(), conv(encoded_data)))\n",
    "\n",
    "print(\"Shape выхода свертки:\", conv(encoded_data).shape)\n",
    "\n",
    "# случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "check_error = np.random.normal(loc=-3, scale=100, size=(conv(encoded_data).shape))\n",
    "check_error_torch = torch.tensor(np.transpose(check_error[np.newaxis, :], (0, 2, 1)))\n",
    "\n",
    "torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "kernel_grad, bias_grad, in_error = conv.backward(check_error)  # тоже самое, только в ручном слое\n",
    "\n",
    "# проверка градиентов весов и входа\n",
    "print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), in_error))\n",
    "print(\"Градиент по смещениям совпадает:\", np.allclose(torch_conv.bias.grad.detach().numpy(), bias_grad))\n",
    "print(\"Градиент по ядру совпадает:\", np.allclose(torch_conv.weight.grad.detach().numpy(), kernel_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c312b6-4da3-40dd-b5d9-d3ce8346f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(BaseLayer):\n",
    "    \"\"\"\n",
    "    Сверточный слой, со страйдом 1 и без паддингов, для батча\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        scale = np.sqrt(1/(in_channels*kernel_size))\n",
    "        self.kernel = np.random.uniform(-scale, scale, size=(out_channels, in_channels, kernel_size))\n",
    "        self.bias = np.random.uniform(-scale, scale, size=(out_channels))\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.kernel_optimizer = copy.copy(optimizer)\n",
    "        self.bias_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.kernel_optimizer.set_weight(self.kernel)\n",
    "        self.bias_optimizer.set_weight(self.bias)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        \"\"\"\n",
    "        Работает с битчами вида [BATCH_SIZE, SENTENCE_LEN, EMB_DIM]\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.input_len = x.shape[1]\n",
    "        self.output_len = self.input_len - self.kernel_size + 1\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for sentence in x:\n",
    "            result.append(self._forward_for_one(sentence))\n",
    "\n",
    "        return np.array(result)\n",
    "\n",
    "    def _forward_for_one(self, x):\n",
    "        \"\"\"\n",
    "        Просто свертка для 1 предложения\n",
    "        \"\"\"\n",
    "        output = np.zeros(shape=(self.output_len, self.out_channels))\n",
    "\n",
    "        # для каждого выходного канала и ядра, отвечающего за этот канал\n",
    "        for kernel_i, ker in enumerate(self.kernel):\n",
    "            # по выходной длине\n",
    "            for i in range(self.output_len):\n",
    "                # умножаем срез по размеру ядра на ядро и суммируем\n",
    "                output[i:self.kernel_size+i, kernel_i] = self.bias[kernel_i] + np.sum(x[i:self.kernel_size+i, :] * ker.T)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        \"\"\"\n",
    "        Градиенты по всем батчу\n",
    "        \"\"\"\n",
    "        dy_dkernels = []\n",
    "        dy_dbiass = []\n",
    "        dy_dxs = []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            dy_dkernel, dy_dbias, dy_dx = self._calc_grad_for_one(output_error[i], self.input[i])\n",
    "            dy_dkernels.append(dy_dkernel)\n",
    "            dy_dbiass.append(dy_dbias)\n",
    "            dy_dxs.append(dy_dx)\n",
    "\n",
    "        dy_dkernels = np.sum(np.array(dy_dkernels), axis=0)  # суммируем градиенты по батчу\n",
    "        dy_dbiass = np.sum(np.array(dy_dbiass), axis=0)\n",
    "        dy_dxs = np.array(dy_dxs)\n",
    "\n",
    "        # self.kernel = self.kernel_optimizer.step(dy_dkernels)  # делаем шаг спуска по сумме градиентов\n",
    "        # self.bias = self.bias_optimizer.step(dy_dbiass)\n",
    "\n",
    "        # return dy_dxs\n",
    "        return dy_dkernels, dy_dbiass, dy_dxs\n",
    "\n",
    "    def _calc_grad_for_one(self, output_error, x):\n",
    "        dy_dkernel = np.zeros(shape=self.kernel.shape)\n",
    "        dy_dbias = np.zeros(shape=self.bias.shape)\n",
    "        dy_dx = np.zeros(shape=x.shape)\n",
    "\n",
    "        for kernel_i, ker in enumerate(self.kernel):\n",
    "            helper_k = np.zeros(shape=ker.T.shape)\n",
    "\n",
    "            for i in range(self.output_len):\n",
    "                helper_k += x[i:self.kernel_size+i, :] * output_error[i, kernel_i]\n",
    "                dy_dx[i:self.kernel_size+i, :] += ker.T * output_error[i, kernel_i]\n",
    "\n",
    "            dy_dkernel[kernel_i] = helper_k.T\n",
    "            dy_dbias[kernel_i] = np.sum(output_error[:, kernel_i])\n",
    "\n",
    "        return dy_dkernel, dy_dbias, dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e00fb2-d638-4a0c-8dc3-c7433a146f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода свертки: (5, 126, 4)\n",
      "Градиент по входу совпадает: True\n",
      "Градиент по смещениям совпадает: True\n",
      "Градиент по ядру совпадает: True\n"
     ]
    }
   ],
   "source": [
    "# тоже самое, только теперь ручная свертка умеет работать с батчами, np.newaxis теперь не нужен, размер входа будет [BATCH_SIZE, SENTENCE_LEN, EMB_DIM]\n",
    "\n",
    "sample = to_matrix(np.random.choice(lines, size=5), token_to_id, max_len=130)\n",
    "\n",
    "emb = Embedding(len(token_to_id), 16)\n",
    "encoded_data = emb(sample) # батч из 5 предложений\n",
    "\n",
    "conv = Conv1d(16, 4, 5)\n",
    "torch_conv = torch.nn.Conv1d(16, 4, 5)\n",
    "torch_conv.weight.data = torch.as_tensor(conv.kernel)\n",
    "torch_conv.bias.data = torch.as_tensor(conv.bias)\n",
    "\n",
    "torch_input = torch.tensor(encoded_data, dtype=torch.float64, requires_grad=True)\n",
    "torch_out = torch_conv(torch_input.permute(0, 2, 1))  # permute потому что torch на вход принимает формат [BATCH_SIZE, EMB_DIM, SENTENCE_LEN]\n",
    "\n",
    "# проверка что forward работает также как у torch, \n",
    "print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.permute(0, 2, 1).detach().numpy(), conv(encoded_data)))\n",
    "\n",
    "print(\"Shape выхода свертки:\", conv(encoded_data).shape)\n",
    "\n",
    "# случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "check_error = np.random.normal(loc=-3, scale=100, size=(conv(encoded_data).shape))\n",
    "check_error_torch = torch.tensor(np.transpose(check_error, (0, 2, 1)))\n",
    "\n",
    "torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "kernel_grad, bias_grad, in_error = conv.backward(check_error)  # тоже самое, только в ручном слое\n",
    "\n",
    "# проверка градиентов весов и входа\n",
    "print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), in_error))\n",
    "print(\"Градиент по смещениям совпадает:\", np.allclose(torch_conv.bias.grad.detach().numpy(), bias_grad))\n",
    "print(\"Градиент по ядру совпадает:\", np.allclose(torch_conv.weight.grad.detach().numpy(), kernel_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c935dff-a20e-40be-89b1-19b839e1c5ce",
   "metadata": {},
   "source": [
    "# linear check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a9cde34-2484-494d-ba65-d1f14c97018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(BaseLayer):\n",
    "    \"\"\"\n",
    "    Linear class permorms ordinary FC layer in neural networks\n",
    "    Parameters:\n",
    "    n_input - size of input neurons\n",
    "    n_output - size of output neurons\n",
    "    Methods:\n",
    "    set_optimizer(optimizer) - is used for setting an optimizer for gradient descent\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input: int, n_output: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.w = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(n_input, n_output))\n",
    "        self.b = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(1, n_output))\n",
    "\n",
    "        self.w_optimizer = None\n",
    "        self.b_optimizer = None\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.w_optimizer = copy.copy(optimizer)\n",
    "        self.b_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.w_optimizer.set_weight(self.w)\n",
    "        self.b_optimizer.set_weight(self.b)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return x.dot(self.w) + self.b\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        # assert self.w_optimizer is not None and self.b_optimizer is not None, 'You should set an optimizer'\n",
    "        w_grad = self.input.T.dot(output_error)\n",
    "        b_grad = np.ones((1, len(output_error))).dot(output_error)\n",
    "        input_error = output_error.dot(self.w.T)\n",
    "\n",
    "        # self.w = self.w_optimizer.step(w_grad)\n",
    "        # self.b = self.b_optimizer.step(b_grad)\n",
    "        return w_grad, b_grad, input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4f48bc-8fed-49f0-b7a1-643be42d4d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода линейного слоя: (100, 25)\n",
      "Градиент по входу совпадает: True\n",
      "Градиент по смещениям совпадает: True\n",
      "Градиент по ядру совпадает: True\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что линейный слой работает правильно, транспонирование весов происходит потому, что домножение на веса в моем слое справа\n",
    "\n",
    "sample = np.random.normal(loc=0, scale=100, size=(100, 16))\n",
    "\n",
    "linear = Linear(16, 25)\n",
    "torch_linear = torch.nn.Linear(16, 25)\n",
    "torch_linear.weight.data = torch.as_tensor(linear.w.T)\n",
    "torch_linear.bias.data = torch.as_tensor(linear.b)\n",
    "\n",
    "torch_input = torch.tensor(sample, dtype=torch.float64, requires_grad=True)\n",
    "torch_out = torch_linear(torch_input)\n",
    "\n",
    "# проверка что forward работает также как у torch, \n",
    "print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), linear(sample)))\n",
    "\n",
    "print(\"Shape выхода линейного слоя:\", linear(sample).shape)\n",
    "\n",
    "# случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "check_error = np.random.normal(loc=-3, scale=100, size=(linear(sample).shape))\n",
    "check_error_torch = torch.tensor(check_error)\n",
    "\n",
    "torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "kernel_grad, bias_grad, in_error = linear.backward(check_error)  # тоже самое, только в ручном слое\n",
    "\n",
    "# проверка градиентов весов и входа\n",
    "print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), in_error))\n",
    "print(\"Градиент по смещениям совпадает:\", np.allclose(torch_linear.bias.grad.detach().numpy(), bias_grad))\n",
    "print(\"Градиент по ядру совпадает:\", np.allclose(torch_linear.weight.grad.detach().numpy(), kernel_grad.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2710d3c-e393-40ef-9ea8-4779a39bb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear3d(BaseLayer):\n",
    "    \"\"\"\n",
    "    Linear class permorms ordinary FC layer in neural networks\n",
    "    Parameters:\n",
    "    n_input - size of input neurons\n",
    "    n_output - size of output neurons\n",
    "    Methods:\n",
    "    set_optimizer(optimizer) - is used for setting an optimizer for gradient descent\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input: int, n_output: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.w = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(n_input, n_output))\n",
    "        self.b = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(1, n_output))\n",
    "\n",
    "        self.w_optimizer = None\n",
    "        self.b_optimizer = None\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.w_optimizer = copy.copy(optimizer)\n",
    "        self.b_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.w_optimizer.set_weight(self.w)\n",
    "        self.b_optimizer.set_weight(self.b)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return np.matmul(x, self.w) + self.b  # the same as @\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        # assert self.w_optimizer is not None and self.b_optimizer is not None, 'You should set an optimizer'\n",
    "        # перемножаем последние 2 измерения друг с другом с помощью matmul и суммируем\n",
    "        w_grad = np.sum(np.transpose(self.input, (0, 2, 1)) @ output_error, axis=0)\n",
    "        b_grad = np.sum(output_error, axis=(0, 1))\n",
    "        input_error = output_error @ self.w.T\n",
    "\n",
    "        # self.w = self.w_optimizer.step(w_grad)\n",
    "        # self.b = self.b_optimizer.step(b_grad)\n",
    "        return w_grad, b_grad, input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e859cd5d-9307-4e26-a1b5-739ef0dbc75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода линейного слоя: (100, 32, 25)\n",
      "Градиент по входу совпадает: True\n",
      "Градиент по смещениям совпадает: True\n",
      "Градиент по ядру совпадает: True\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что линейный слой для 3-х измерений работает правильно, транспонирование весов происходит потому, что домножение на веса в моем слое справа\n",
    "\n",
    "sample = np.random.normal(loc=0, scale=100, size=(100, 32, 16))\n",
    "\n",
    "linear = Linear3d(16, 25)\n",
    "torch_linear = torch.nn.Linear(16, 25)\n",
    "torch_linear.weight.data = torch.as_tensor(linear.w.T)\n",
    "torch_linear.bias.data = torch.as_tensor(linear.b)\n",
    "\n",
    "torch_input = torch.tensor(sample, dtype=torch.float64, requires_grad=True)\n",
    "torch_out = torch_linear(torch_input)\n",
    "\n",
    "# проверка что forward работает также как у torch, \n",
    "print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), linear(sample)))\n",
    "\n",
    "print(\"Shape выхода линейного слоя:\", linear(sample).shape)\n",
    "\n",
    "# случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "check_error = np.random.normal(loc=-3, scale=100, size=(linear(sample).shape))\n",
    "check_error_torch = torch.tensor(check_error)\n",
    "\n",
    "torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "kernel_grad, bias_grad, in_error = linear.backward(check_error)  # тоже самое, только в ручном слое\n",
    "\n",
    "# проверка градиентов весов и входа\n",
    "print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), in_error))\n",
    "print(\"Градиент по смещениям совпадает:\", np.allclose(torch_linear.bias.grad.detach().numpy(), bias_grad))\n",
    "print(\"Градиент по ядру совпадает:\", np.allclose(torch_linear.weight.grad.detach().numpy(), kernel_grad.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1626fa1-bf58-4950-bac1-d65ea7c07b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  67568.71473288,  -54068.69574521,   23596.48727212,\n",
       "        -128521.19845805,    9880.1331249 ,  -36268.42439887,\n",
       "         -41735.56494009,   56111.89244778,   35586.33672801,\n",
       "         -34324.09351658,  -32552.19429502,   57874.73568831,\n",
       "          47993.05864908,   -7400.9907628 ,  -56195.24721095,\n",
       "         -68185.55195928,  -31738.97832506,   49116.0314091 ,\n",
       "          -9851.24133631,  -53612.86705739,  -41753.73630245,\n",
       "         -19076.15984663,   81432.55214513,  -24529.29526251,\n",
       "          77003.18886427],\n",
       "       [  17865.53255925,   -5576.68223925,   63146.60126128,\n",
       "         -22283.36554166,   23351.3610662 ,  -57360.81541882,\n",
       "         -45170.46704203,  -18070.80811844,   41350.80997168,\n",
       "          31992.24958086,   87390.00853691,  -10115.6751183 ,\n",
       "           2225.1500625 ,   22179.83199527,    6592.1886647 ,\n",
       "         176285.46799103,  -76457.40936593,  128275.29064507,\n",
       "         -35240.8041254 ,   38713.31895115,  -14188.958722  ,\n",
       "         138192.19414686,   15355.06285137,  -22563.08108401,\n",
       "          64201.84635307],\n",
       "       [-111426.72143773,   33605.26108152,  -49161.24005264,\n",
       "         -27094.55258352,   13213.76481926,  -12021.74163817,\n",
       "          76812.60464423,  -57154.4282144 ,  -62679.66852166,\n",
       "          28248.12127874,   24482.14034277,   20491.35711224,\n",
       "         -42324.876533  ,   47674.21260871,   45873.58631214,\n",
       "         -55219.37810897,   47064.4403489 ,   14392.02656933,\n",
       "         -15082.38595637,  -13164.98270687,  -84841.35427857,\n",
       "          69923.65244157,  -57029.37007945,   87001.56502281,\n",
       "         -42362.10534101],\n",
       "       [  23336.68049052,   19740.28925125,   -4589.3669493 ,\n",
       "          80919.00060227,   17425.99729933,     628.88189816,\n",
       "          -6213.91110075,   40734.20399646,   46982.6199806 ,\n",
       "          13944.27364791,   37701.42891626,  -81481.67169533,\n",
       "        -105444.51648216,    5103.59235581, -141493.66938859,\n",
       "         -43905.70234265,   79726.11424132,  -40425.14059302,\n",
       "         -53731.52824826,   38082.51348274,  -22873.30774097,\n",
       "         -53734.36701758,   -7790.90870294,   37104.60293373,\n",
       "         -16776.36525441],\n",
       "       [  81151.53213391,  -66862.71137038,   11238.22101858,\n",
       "           3618.17062051,   -1383.80530392, -101836.03455623,\n",
       "         -15004.62981943,  126975.24395699,   14281.74819217,\n",
       "           7770.10811863,   -4799.47723124,  126573.84476679,\n",
       "          32685.3224576 ,   49535.72285163,   14762.03803239,\n",
       "         116539.86915379,    3183.26985321,  -22906.75531863,\n",
       "         -21440.29313087,   10371.17199303,    6824.73783159,\n",
       "         105282.10534945,   35833.4841194 ,  -72586.2457384 ,\n",
       "          18596.17796162],\n",
       "       [-115722.16860345,  -25000.11414692,  -76237.86542205,\n",
       "         -36724.79726758,    9421.08264048,   30329.78494643,\n",
       "          39947.635186  ,   42059.40959962,  -23926.64108299,\n",
       "         -37646.17222253,  -14657.48618017,   17342.92728462,\n",
       "          -2332.48113207,  147578.01224145,    2213.78169404,\n",
       "          29604.09973464, -133076.85833769,   25692.01675389,\n",
       "          11445.84442001,   51317.96574756,  -12414.6316716 ,\n",
       "          15012.03456216,   -9940.60210543,   40645.06766353,\n",
       "        -118829.23830997],\n",
       "       [  80792.03180489,  -18350.47164542,   64818.62862678,\n",
       "          10501.5138706 ,  -38267.86792664,   30490.26474425,\n",
       "         -19206.93082762,   35421.7763229 ,  -29738.43879391,\n",
       "         -11405.44353578,   36938.95526233,   19691.07583679,\n",
       "          57840.84043009, -111076.08283197,   21791.62418736,\n",
       "          58300.78734283,   87891.39084802, -108159.63078534,\n",
       "          19825.56421651, -103212.60869973,   78525.08270146,\n",
       "         -49107.5930062 ,   71106.38223225, -145207.75599424,\n",
       "          87512.40137858],\n",
       "       [  -5122.05081704,   12511.35592601,   56405.47028435,\n",
       "         138090.84841021,   18119.81986905,  -82880.74253638,\n",
       "          38188.36055129,   67808.60213013,  131031.81214849,\n",
       "          67565.31973132,   37146.27897851,   60203.6477241 ,\n",
       "          27057.86617915,   12643.48546239,  -26536.18702047,\n",
       "          77851.42056722,    1558.56999287,   14758.52749467,\n",
       "          -1036.01718686,   -6969.91758888,    -547.31237056,\n",
       "          63982.65168613,  -53020.14147531,  -76317.49049902,\n",
       "            337.04626371],\n",
       "       [  42194.53384257,   35509.53475566,  -22459.52879862,\n",
       "          25692.45006326,   42283.94859223,    9807.42792287,\n",
       "         -25023.78273588,  -20345.84520558,   20351.96625988,\n",
       "         -56663.92081669, -123121.86515073,  -20163.54765128,\n",
       "          11374.62567326,  -91494.52228827,   66867.38350498,\n",
       "          88751.79436368,   95239.47874859,  -60765.01031059,\n",
       "         -81612.281506  ,   43697.35183948,   69105.27075269,\n",
       "           6247.89493218,   86139.28545576,   38877.73647846,\n",
       "          59640.83879237],\n",
       "       [  74873.21957718,   17022.33572061,   28265.60467258,\n",
       "          75747.42133357, -161868.33928901,   34388.50255948,\n",
       "           5972.41891284,  -48785.05101569,  -35193.58635976,\n",
       "          29102.93459023,   58750.6104306 ,  -13335.9206369 ,\n",
       "         -44500.31514312,  -74558.42399438,  -26815.00535406,\n",
       "         -59069.83675038,   46231.99312165,  -22832.45818224,\n",
       "          16011.19689165,   71862.06153719,   42057.21541953,\n",
       "         -87819.39880094,  -56840.83906968,    9504.96879628,\n",
       "          12539.19979368],\n",
       "       [ -57268.53006142,  -24448.61055575,  -41906.26903696,\n",
       "         103503.51835902,  -39793.34489839,   53044.14401829,\n",
       "         -24271.9187587 ,  112920.03881544,   -2838.94295531,\n",
       "         -66606.98880765,   60933.83133124,   40132.0183682 ,\n",
       "          -3214.82854457,    4413.30146006,   -3070.44480039,\n",
       "         -50089.52266047,   23727.80963814, -103738.86178253,\n",
       "           3396.53334322,   -2716.94899088,  -26882.90808468,\n",
       "          -9049.75954918,  -73569.19375241,  -38521.46866931,\n",
       "        -101635.73233539],\n",
       "       [   3031.42791931,   42911.9525281 ,  -54138.77635671,\n",
       "         122266.67597412,   -2399.21231067,   39333.63249229,\n",
       "          82789.67773395,  117620.44087133,   99686.22209178,\n",
       "            552.24967375,  -66949.13936675,  -55076.86653615,\n",
       "         -38040.37799998,   -4595.63195666,  -48960.72121908,\n",
       "         103484.01298956,  -24688.55728262,  -40031.06803549,\n",
       "            278.24096088,  -24679.20688601,    5020.41721488,\n",
       "        -101610.88784493,   34188.70157423,   63463.9505514 ,\n",
       "         -90432.67547206],\n",
       "       [  56550.81869703,  -75919.50700873,    3428.13306932,\n",
       "         -81654.5145304 ,    4717.46585748,   52313.47077263,\n",
       "         -27213.33113091,   17111.33372097,  -35069.81286459,\n",
       "          10354.17791726, -102795.23019252,  -16362.48234034,\n",
       "          19536.20879706,  -14995.84258951,   13410.31818147,\n",
       "          56628.76649129,  -23180.64801983,    3762.44754463,\n",
       "          48381.87364422,  -29466.73404632,  -47849.41674314,\n",
       "         -81892.74401234,   87561.58629155,   46320.34461222,\n",
       "         129026.04634543],\n",
       "       [  83117.6594341 ,  -31445.39512046,   37577.19381832,\n",
       "           4981.30951677,   36488.99723161,  -68979.07200799,\n",
       "         -60656.34774575,  -40324.78381989,   83388.74469398,\n",
       "           4863.81695881,  -12936.11449279,  -38119.39594247,\n",
       "          74582.59262217,  -20675.07566355,   22005.70630935,\n",
       "          39093.10289252,    2832.89054986,  -39429.25874884,\n",
       "          42112.41358835, -106942.87291836,   55195.93070708,\n",
       "          94839.03301881,  -21909.36253069,  -32328.96911055,\n",
       "         139157.3962833 ],\n",
       "       [  88497.2259341 ,  -49450.25086188,   44351.57492573,\n",
       "         -49141.05848273,  -11368.22082899,  -21448.00907657,\n",
       "           7571.7079007 ,   47568.61857876,   44109.5042946 ,\n",
       "          62770.53513114,  -79420.22822771,  -63870.01528333,\n",
       "          37334.72732011,  -38312.51063728,  -82109.1200083 ,\n",
       "         -60988.72578852,  -94455.48835997,  -15999.2433596 ,\n",
       "          89494.31209255, -127670.91611473,   -8750.437894  ,\n",
       "         -67746.92806466,   56896.28671995,  -55118.18043528,\n",
       "          22630.29602769],\n",
       "       [ -61792.78566566,  -36166.18187638,   20108.88142129,\n",
       "         -44819.75215516,  -18935.74162165,   35576.96016652,\n",
       "         -20625.9509416 ,   85077.28630486,   64522.8264526 ,\n",
       "         -22488.32616414,    5482.41034452,   13738.35444946,\n",
       "          -3589.63135721,   27084.60959344,   60134.85158326,\n",
       "         -38845.90004402,  -41267.76207987,   81629.99737191,\n",
       "         -22428.78929093,   61891.46437571,   11817.68795877,\n",
       "          -5809.2904447 ,  -77038.79095512,   -2191.0339376 ,\n",
       "        -131940.58657843]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(sample, (0, 2, 1))[1].dot(check_error[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9afc29a5-9c1d-4354-b212-44cae08b5129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  67568.71473288,  -54068.69574521,   23596.48727212,\n",
       "        -128521.19845805,    9880.1331249 ,  -36268.42439887,\n",
       "         -41735.56494009,   56111.89244778,   35586.33672801,\n",
       "         -34324.09351658,  -32552.19429502,   57874.73568831,\n",
       "          47993.05864908,   -7400.9907628 ,  -56195.24721095,\n",
       "         -68185.55195928,  -31738.97832506,   49116.0314091 ,\n",
       "          -9851.24133631,  -53612.86705739,  -41753.73630245,\n",
       "         -19076.15984663,   81432.55214513,  -24529.29526251,\n",
       "          77003.18886427],\n",
       "       [  17865.53255925,   -5576.68223925,   63146.60126128,\n",
       "         -22283.36554166,   23351.3610662 ,  -57360.81541882,\n",
       "         -45170.46704203,  -18070.80811844,   41350.80997168,\n",
       "          31992.24958086,   87390.00853691,  -10115.6751183 ,\n",
       "           2225.1500625 ,   22179.83199527,    6592.1886647 ,\n",
       "         176285.46799103,  -76457.40936593,  128275.29064507,\n",
       "         -35240.8041254 ,   38713.31895115,  -14188.958722  ,\n",
       "         138192.19414686,   15355.06285137,  -22563.08108401,\n",
       "          64201.84635307],\n",
       "       [-111426.72143773,   33605.26108152,  -49161.24005264,\n",
       "         -27094.55258352,   13213.76481926,  -12021.74163817,\n",
       "          76812.60464423,  -57154.4282144 ,  -62679.66852166,\n",
       "          28248.12127874,   24482.14034277,   20491.35711224,\n",
       "         -42324.876533  ,   47674.21260871,   45873.58631214,\n",
       "         -55219.37810897,   47064.4403489 ,   14392.02656933,\n",
       "         -15082.38595637,  -13164.98270687,  -84841.35427857,\n",
       "          69923.65244157,  -57029.37007945,   87001.56502281,\n",
       "         -42362.10534101],\n",
       "       [  23336.68049052,   19740.28925125,   -4589.3669493 ,\n",
       "          80919.00060227,   17425.99729933,     628.88189816,\n",
       "          -6213.91110075,   40734.20399646,   46982.6199806 ,\n",
       "          13944.27364791,   37701.42891626,  -81481.67169533,\n",
       "        -105444.51648216,    5103.59235581, -141493.66938859,\n",
       "         -43905.70234265,   79726.11424132,  -40425.14059302,\n",
       "         -53731.52824826,   38082.51348274,  -22873.30774097,\n",
       "         -53734.36701758,   -7790.90870294,   37104.60293373,\n",
       "         -16776.36525441],\n",
       "       [  81151.53213391,  -66862.71137038,   11238.22101858,\n",
       "           3618.17062051,   -1383.80530392, -101836.03455623,\n",
       "         -15004.62981943,  126975.24395699,   14281.74819217,\n",
       "           7770.10811863,   -4799.47723124,  126573.84476679,\n",
       "          32685.3224576 ,   49535.72285163,   14762.03803239,\n",
       "         116539.86915379,    3183.26985321,  -22906.75531863,\n",
       "         -21440.29313087,   10371.17199303,    6824.73783159,\n",
       "         105282.10534945,   35833.4841194 ,  -72586.2457384 ,\n",
       "          18596.17796162],\n",
       "       [-115722.16860345,  -25000.11414692,  -76237.86542205,\n",
       "         -36724.79726758,    9421.08264048,   30329.78494643,\n",
       "          39947.635186  ,   42059.40959962,  -23926.64108299,\n",
       "         -37646.17222253,  -14657.48618017,   17342.92728462,\n",
       "          -2332.48113207,  147578.01224145,    2213.78169404,\n",
       "          29604.09973464, -133076.85833769,   25692.01675389,\n",
       "          11445.84442001,   51317.96574756,  -12414.6316716 ,\n",
       "          15012.03456216,   -9940.60210543,   40645.06766353,\n",
       "        -118829.23830997],\n",
       "       [  80792.03180489,  -18350.47164542,   64818.62862678,\n",
       "          10501.5138706 ,  -38267.86792664,   30490.26474425,\n",
       "         -19206.93082762,   35421.7763229 ,  -29738.43879391,\n",
       "         -11405.44353578,   36938.95526233,   19691.07583679,\n",
       "          57840.84043009, -111076.08283197,   21791.62418736,\n",
       "          58300.78734283,   87891.39084802, -108159.63078534,\n",
       "          19825.56421651, -103212.60869973,   78525.08270146,\n",
       "         -49107.5930062 ,   71106.38223225, -145207.75599424,\n",
       "          87512.40137858],\n",
       "       [  -5122.05081704,   12511.35592601,   56405.47028435,\n",
       "         138090.84841021,   18119.81986905,  -82880.74253638,\n",
       "          38188.36055129,   67808.60213013,  131031.81214849,\n",
       "          67565.31973132,   37146.27897851,   60203.6477241 ,\n",
       "          27057.86617915,   12643.48546239,  -26536.18702047,\n",
       "          77851.42056722,    1558.56999287,   14758.52749467,\n",
       "          -1036.01718686,   -6969.91758888,    -547.31237056,\n",
       "          63982.65168613,  -53020.14147531,  -76317.49049902,\n",
       "            337.04626371],\n",
       "       [  42194.53384257,   35509.53475566,  -22459.52879862,\n",
       "          25692.45006326,   42283.94859223,    9807.42792287,\n",
       "         -25023.78273588,  -20345.84520558,   20351.96625988,\n",
       "         -56663.92081669, -123121.86515073,  -20163.54765128,\n",
       "          11374.62567326,  -91494.52228827,   66867.38350498,\n",
       "          88751.79436368,   95239.47874859,  -60765.01031059,\n",
       "         -81612.281506  ,   43697.35183948,   69105.27075269,\n",
       "           6247.89493218,   86139.28545576,   38877.73647846,\n",
       "          59640.83879237],\n",
       "       [  74873.21957718,   17022.33572061,   28265.60467258,\n",
       "          75747.42133357, -161868.33928901,   34388.50255948,\n",
       "           5972.41891284,  -48785.05101569,  -35193.58635976,\n",
       "          29102.93459023,   58750.6104306 ,  -13335.9206369 ,\n",
       "         -44500.31514312,  -74558.42399438,  -26815.00535406,\n",
       "         -59069.83675038,   46231.99312165,  -22832.45818224,\n",
       "          16011.19689165,   71862.06153719,   42057.21541953,\n",
       "         -87819.39880094,  -56840.83906968,    9504.96879628,\n",
       "          12539.19979368],\n",
       "       [ -57268.53006142,  -24448.61055575,  -41906.26903696,\n",
       "         103503.51835902,  -39793.34489839,   53044.14401829,\n",
       "         -24271.9187587 ,  112920.03881544,   -2838.94295531,\n",
       "         -66606.98880765,   60933.83133124,   40132.0183682 ,\n",
       "          -3214.82854457,    4413.30146006,   -3070.44480039,\n",
       "         -50089.52266047,   23727.80963814, -103738.86178253,\n",
       "           3396.53334322,   -2716.94899088,  -26882.90808468,\n",
       "          -9049.75954918,  -73569.19375241,  -38521.46866931,\n",
       "        -101635.73233539],\n",
       "       [   3031.42791931,   42911.9525281 ,  -54138.77635671,\n",
       "         122266.67597412,   -2399.21231067,   39333.63249229,\n",
       "          82789.67773395,  117620.44087133,   99686.22209178,\n",
       "            552.24967375,  -66949.13936675,  -55076.86653615,\n",
       "         -38040.37799998,   -4595.63195666,  -48960.72121908,\n",
       "         103484.01298956,  -24688.55728262,  -40031.06803549,\n",
       "            278.24096088,  -24679.20688601,    5020.41721488,\n",
       "        -101610.88784493,   34188.70157423,   63463.9505514 ,\n",
       "         -90432.67547206],\n",
       "       [  56550.81869703,  -75919.50700873,    3428.13306932,\n",
       "         -81654.5145304 ,    4717.46585748,   52313.47077263,\n",
       "         -27213.33113091,   17111.33372097,  -35069.81286459,\n",
       "          10354.17791726, -102795.23019252,  -16362.48234034,\n",
       "          19536.20879706,  -14995.84258951,   13410.31818147,\n",
       "          56628.76649129,  -23180.64801983,    3762.44754463,\n",
       "          48381.87364422,  -29466.73404632,  -47849.41674314,\n",
       "         -81892.74401234,   87561.58629155,   46320.34461222,\n",
       "         129026.04634543],\n",
       "       [  83117.6594341 ,  -31445.39512046,   37577.19381832,\n",
       "           4981.30951677,   36488.99723161,  -68979.07200799,\n",
       "         -60656.34774575,  -40324.78381989,   83388.74469398,\n",
       "           4863.81695881,  -12936.11449279,  -38119.39594247,\n",
       "          74582.59262217,  -20675.07566355,   22005.70630935,\n",
       "          39093.10289252,    2832.89054986,  -39429.25874884,\n",
       "          42112.41358835, -106942.87291836,   55195.93070708,\n",
       "          94839.03301881,  -21909.36253069,  -32328.96911055,\n",
       "         139157.3962833 ],\n",
       "       [  88497.2259341 ,  -49450.25086188,   44351.57492573,\n",
       "         -49141.05848273,  -11368.22082899,  -21448.00907657,\n",
       "           7571.7079007 ,   47568.61857876,   44109.5042946 ,\n",
       "          62770.53513114,  -79420.22822771,  -63870.01528333,\n",
       "          37334.72732011,  -38312.51063728,  -82109.1200083 ,\n",
       "         -60988.72578852,  -94455.48835997,  -15999.2433596 ,\n",
       "          89494.31209255, -127670.91611473,   -8750.437894  ,\n",
       "         -67746.92806466,   56896.28671995,  -55118.18043528,\n",
       "          22630.29602769],\n",
       "       [ -61792.78566566,  -36166.18187638,   20108.88142129,\n",
       "         -44819.75215516,  -18935.74162165,   35576.96016652,\n",
       "         -20625.9509416 ,   85077.28630486,   64522.8264526 ,\n",
       "         -22488.32616414,    5482.41034452,   13738.35444946,\n",
       "          -3589.63135721,   27084.60959344,   60134.85158326,\n",
       "         -38845.90004402,  -41267.76207987,   81629.99737191,\n",
       "         -22428.78929093,   61891.46437571,   11817.68795877,\n",
       "          -5809.2904447 ,  -77038.79095512,   -2191.0339376 ,\n",
       "        -131940.58657843]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.transpose(sample, (0, 2, 1))@check_error)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3d590-2ad2-4fcb-bda2-9b4ad545148d",
   "metadata": {},
   "source": [
    "# Softmax (dim=-1) check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9f0c4c-3535-4810-9493-ebbf6d73870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxLayer3D(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.forward_result = None\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        self.input = x\n",
    "        exp = np.exp(x)\n",
    "        self.forward_result = exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "        return self.forward_result\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        \"https://binpord.github.io/2021/09/26/softmax_backprop.html\"\n",
    "        return (output_error - (output_error*self.forward_result).sum(axis=-1, keepdims=True)) * self.forward_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3de4af5-0e55-48ed-9049-d98da1b20727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 32, 16)\n",
      "Градиент по входу совпадает: True\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что softmax работает также как и в torch\n",
    "\n",
    "sample = np.random.normal(loc=0, scale=100, size=(100, 32, 16))\n",
    "\n",
    "softmax = SoftMaxLayer3D()\n",
    "\n",
    "torch_input = torch.tensor(sample, dtype=torch.float64, requires_grad=True)\n",
    "torch_out = nn.functional.softmax(torch_input, dim=-1)\n",
    "\n",
    "# проверка что forward работает также как у torch, \n",
    "print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), softmax(sample)))\n",
    "\n",
    "print(\"Shape выхода слоя:\", softmax(sample).shape)\n",
    "\n",
    "# случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "check_error = np.random.normal(loc=-3, scale=100, size=(softmax(sample).shape))\n",
    "check_error_torch = torch.tensor(check_error)\n",
    "\n",
    "torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "in_error = softmax.backward(check_error)  # тоже самое, только в ручном слое\n",
    "\n",
    "# проверка градиентов весов и входа\n",
    "print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), in_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa6f69-ec31-4c49-a3ab-58d84cebf2e2",
   "metadata": {},
   "source": [
    "# attention check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43070d43-77aa-488f-b34a-697d143b2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(BaseLayer):\n",
    "    def __init__(self, hid_dim: int, n_heads: int) -> None:\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.input = None\n",
    "        self.attn_bias = None\n",
    "        self.attentions = []\n",
    "        self.q = None\n",
    "        self.k = None\n",
    "        self.v = None\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = hid_dim // n_heads\n",
    "        \n",
    "        self.c_attn = Linear3d(hid_dim, hid_dim * 3)\n",
    "        self.c_proj = Linear3d(hid_dim, hid_dim)\n",
    "        self.softmax = SoftMaxLayer3D()\n",
    "\n",
    "        self.scale = np.sqrt(self.head_size)\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.c_attn.set_optimizer(optimizer)\n",
    "        self.c_proj.set_optimizer(optimizer)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        \n",
    "        q_k_v = self.c_attn(x)\n",
    "        self.q = q_k_v[:, :, :self.hid_dim]\n",
    "        self.k = q_k_v[:, :, self.hid_dim:self.hid_dim*2]\n",
    "        self.v = q_k_v[:, :, self.hid_dim*2:self.hid_dim*3]\n",
    "        assert self.q.shape == self.k.shape == self.v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
    "\n",
    "        head_outputs = []\n",
    "        for head_index in range(self.n_heads):\n",
    "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
    "\n",
    "            head_queries = self.q[..., head_selector]\n",
    "            head_keys = self.k[..., head_selector]\n",
    "            head_values = self.v[..., head_selector]\n",
    "\n",
    "            single_head_output = self._attention_for_head(\n",
    "                head_queries, head_keys, head_values,\n",
    "                is_causal=True)\n",
    "            head_outputs.append(single_head_output)\n",
    "\n",
    "        combined_head_outputs = np.concatenate(head_outputs, axis=-1)\n",
    "        return self.c_proj(combined_head_outputs)\n",
    "\n",
    "    def _attention_for_head(self, query, key, value, is_causal=False):\n",
    "        L, S = query.shape[-2], key.shape[-2]\n",
    "        self.attn_bias = np.zeros((L, S), dtype=query.dtype)\n",
    "        if is_causal:\n",
    "            temp_mask = np.tril(np.ones((L, S)))\n",
    "            self.attn_bias = np.where(temp_mask==0, float('-inf'), 0)\n",
    "            self.attn_bias = self.attn_bias.astype(query.dtype)\n",
    "    \n",
    "        attn_weight = query @ key.transpose(0, 2, 1) / self.scale\n",
    "        attn_weight += self.attn_bias\n",
    "        \n",
    "        attn_weight = self.softmax(attn_weight)\n",
    "        self.attentions.append(attn_weight)\n",
    "        \n",
    "        result = attn_weight @ value\n",
    "\n",
    "        # result = self.softmax((query @ key.transpose(0, 2, 1) / self.scale) + self.attn_bias) @ value\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _attention_for_head_grad(self, query, key, value, output_error, head_n):\n",
    "        # looking at the result expression in the forward pass\n",
    "        cur_attention = self.attentions[head_n]\n",
    "        \n",
    "        v_grad = cur_attention.transpose((0, 2, 1)) @ output_error\n",
    "        input_error = output_error @ value.transpose((0, 2, 1))\n",
    "\n",
    "        # we use layer, which uses it's states to calculate grad, so we need to set state that was used during forward pass in current head\n",
    "        self.softmax.forward_result = cur_attention\n",
    "        softmax_grad = self.softmax.backward(input_error)\n",
    "\n",
    "        k_grad = softmax_grad.transpose(0, 2, 1) @ query / self.scale # we need to transpose output error due to the fact key was transposed in forward\n",
    "        q_grad = softmax_grad @ key / self.scale\n",
    "\n",
    "        return q_grad, k_grad, v_grad\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        c_proj_w_grad, c_proj_b_grad, projection_error = self.c_proj.backward(output_error)\n",
    "        \n",
    "        q_grads, k_grads, v_grads = [], [], []\n",
    "        for head_index in range(self.n_heads):\n",
    "            # for each head we choose it's error\n",
    "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
    "\n",
    "            attention_error = projection_error[..., head_selector]\n",
    "            head_queries = self.q[..., head_selector]\n",
    "            head_keys = self.k[..., head_selector]\n",
    "            head_values = self.v[..., head_selector]\n",
    "\n",
    "            q_grad, k_grad, v_grad = self._attention_for_head_grad(head_queries, head_keys, head_values, attention_error, head_index)\n",
    "            q_grads.append(q_grad)\n",
    "            k_grads.append(k_grad)\n",
    "            v_grads.append(v_grad)\n",
    "\n",
    "        q_grads = np.concatenate(q_grads, axis=-1)\n",
    "        k_grads = np.concatenate(k_grads, axis=-1)\n",
    "        v_grads = np.concatenate(v_grads, axis=-1)\n",
    "\n",
    "        q_k_v_output = np.concatenate([q_grads, k_grads, v_grads], axis=-1)\n",
    "        \n",
    "        c_attn_w_grad, c_attn_b_grad, input_error = self.c_attn.backward(q_k_v_output)\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "353a550d-b039-46de-997e-5d6413f947f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch attention\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
    "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
    "        self.dim, self.num_heads = dim, num_heads\n",
    "        self.head_size = dim // num_heads\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_size]))\n",
    "\n",
    "    # you can choose less effective method and uncomment the code below, it works the same\n",
    "    # ------------- start of less effective -------------\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
    "    #     assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
    "\n",
    "\n",
    "    #     # Note: this is an inefficient implementation that uses a for-loop.\n",
    "    #     # To get the full grade during homework, please re-implement this code:\n",
    "    #     # 1) do not use for-loops (or other loops). Compute everything in parallel with vectorized operations\n",
    "    #     # 2) do not use F.scaled_dot_product_attention - write your own attention code using basic PyTorch ops\n",
    "    #     head_outputs = []\n",
    "    #     for head_index in range(self.num_heads):\n",
    "    #         head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
    "\n",
    "    #         head_queries = q[..., head_selector]\n",
    "    #         head_keys = k[..., head_selector]\n",
    "    #         head_values = v[..., head_selector]\n",
    "\n",
    "    #         single_head_output = self.scaled_dot_product_attention(\n",
    "    #             head_queries, head_keys, head_values,\n",
    "    #             is_causal=True)\n",
    "    #         # docs: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
    "    #         head_outputs.append(single_head_output)\n",
    "\n",
    "    #     combined_head_outputs = torch.cat(head_outputs, dim=-1)\n",
    "    #     return self.c_proj(combined_head_outputs)\n",
    "\n",
    "    # def scaled_dot_product_attention(self, query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
    "    #     # Efficient implementation equivalent to the following:\n",
    "    #     L, S = query.size(-2), key.size(-2)\n",
    "    #     scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    #     attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    #     if is_causal:\n",
    "    #         assert attn_mask is None\n",
    "    #         temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "    #         attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "    #         attn_bias.to(query.dtype)\n",
    "    \n",
    "    #     if attn_mask is not None:\n",
    "    #         if attn_mask.dtype == torch.bool:\n",
    "    #             attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "    #         else:\n",
    "    #             attn_bias += attn_mask\n",
    "    #     attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    #     attn_weight += attn_bias\n",
    "    #     attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    #     # attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    #     return attn_weight @ value\n",
    "\n",
    "    # ------------- end of less effective -------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # kinda more effective\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
    "        # [BATCH_SIZE, SEQ_LEN, DIM]\n",
    "        assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
    "        \n",
    "        # [BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_SIZE]\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        # [BATCH_SIZE, NUM_HEADS, SEQ_LEN, SEQ_LEN]\n",
    "        energy = torch.matmul(q, k.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # чтобы не смотреть в будущее\n",
    "        mask = torch.tril(torch.ones((seq_len, seq_len)))\n",
    "        energy = energy.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # [BATCH_SIZE, NUM_HEADS, SEQ_LEN, SEQ_LEN]\n",
    "        probs = nn.functional.softmax(energy, dim=-1)\n",
    "\n",
    "        # [BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_SIZE]\n",
    "        output = torch.matmul(probs, v)\n",
    "\n",
    "        # [BATCH_SIZE, SEQ_LEN, NUM_HEADS, HEAD_SIZE]\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # [BATCH_SIZE, SEQ_LEN, DIM]\n",
    "        output = output.view(batch_size, -1, self.dim)\n",
    "\n",
    "        # [BATCH_SIZE, SEQ_LEN, DIM]\n",
    "        output = self.c_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "386b2420-5232-496b-8d84-c4936425f4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число голов: 1\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 32, 36)\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n",
      "Число голов: 3\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 32, 36)\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n",
      "Число голов: 6\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 32, 36)\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что multihead masked attention работает также как и в torch\n",
    "\n",
    "for head_n in [1, 3, 6]:\n",
    "    print('Число голов:', head_n)\n",
    "    sample = np.random.randn(100, 32, 36)\n",
    "    \n",
    "    attention = MultiHeadAttentionLayer(36, head_n)\n",
    "    torch_attention = MaskedSelfAttention(36, head_n)\n",
    "    # ставим одинаковые иходные веса для слоев\n",
    "    attention.c_attn.w = torch_attention.c_attn.weight.data.numpy().T\n",
    "    attention.c_attn.b = torch_attention.c_attn.bias.data.numpy().reshape(1, -1)\n",
    "    attention.c_proj.w = torch_attention.c_proj.weight.data.numpy().T\n",
    "    attention.c_proj.b = torch_attention.c_proj.bias.data.numpy().reshape(1, -1)\n",
    "\n",
    "    attention_out = attention(sample)\n",
    "    torch_input = torch.tensor(sample, dtype=torch.float32, requires_grad=True)\n",
    "    torch_out = torch_attention(torch_input)\n",
    "    \n",
    "    # проверка что forward работает также как у torch, \n",
    "    print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), attention_out, atol=1e-6))\n",
    "    \n",
    "    print(\"Shape выхода слоя:\", attention_out.shape)\n",
    "    \n",
    "    # случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "    check_error = np.random.randn(100, 32, 36)\n",
    "    check_error_torch = torch.tensor(check_error)\n",
    "    \n",
    "    torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "    in_error = attention.backward(check_error)  # тоже самое, только в ручном слое\n",
    "    \n",
    "    # проверка градиентов весов и входа\n",
    "    print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), in_error, atol=1e-6))\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d8650-5da0-40b3-bb1d-d3e7bbee2fbb",
   "metadata": {},
   "source": [
    "# Layernorm check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1e8ceac-89c4-403b-bf47-e0e02d5b5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(BaseLayer):\n",
    "    def __init__(self, dim, eps=1e-5, elementwise_affine=True, bias=True):\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        self.use_bias = bias\n",
    "\n",
    "        self.input = None\n",
    "        self.normalized = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "\n",
    "        self.weight = np.ones(shape=dim)\n",
    "        self.bias = np.zeros(shape=dim)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        \"\"\"Only for -1 axis\"\"\"\n",
    "        self.input = x\n",
    "\n",
    "        self.mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        self.var = np.var(x, axis=-1, keepdims=True, ddof=0)\n",
    "        self.x_centered = (x - self.mean)\n",
    "        self.std = np.sqrt(self.var+self.eps)\n",
    "\n",
    "        self.normalized = self.x_centered / self.std\n",
    "\n",
    "        result = self.normalized * self.weight + self.bias\n",
    "        return result\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        weight_grad = None\n",
    "        bias_grad = None\n",
    "        if self.elementwise_affine:\n",
    "            weight_grad = np.sum(output_error * self.normalized, axis=(0, 1))\n",
    "\n",
    "        if self.use_bias:\n",
    "            bias_grad = np.sum(output_error, axis=(0, 1))\n",
    "\n",
    "        # производная по normalized\n",
    "        dldx = output_error * self.weight\n",
    "        # производная по дисперсии в знаменателе\n",
    "        dldsigma2 = np.sum(dldx * self.x_centered * (-1/2) * (self.std ** -3), axis=-1, keepdims=True)\n",
    "        dldmu = np.sum((dldx * (-1/self.std)), axis=-1, keepdims=True) + dldsigma2\\\n",
    "        * np.sum(-2*self.x_centered, axis=-1, keepdims=True) / self.dim\n",
    "\n",
    "        input_error = dldx / self.std + dldsigma2 * 2 * self.x_centered / self.dim + dldmu / self.dim\n",
    "        return weight_grad, bias_grad, input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee3d6e5c-47e9-43f8-993c-f90501bc5e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 10, 32)\n",
      "Градиент weight совпадает: True\n",
      "Градиент bias совпадает: True\n",
      "Градиент по входу совпадает: True\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что layernorm работает также как и в torch\n",
    "batch_size, seq_len, emb_dim = 100, 10, 32\n",
    "sample = np.random.randn(batch_size, seq_len, emb_dim)\n",
    "\n",
    "weight = np.random.randn(emb_dim)\n",
    "bias = np.random.randn(emb_dim)\n",
    "\n",
    "layernorm = LayerNorm(emb_dim)\n",
    "layernorm.weight = weight\n",
    "layernorm.bias = bias\n",
    "\n",
    "out = layernorm(sample)\n",
    "\n",
    "torch_input = torch.tensor(sample, dtype=torch.float64, requires_grad=True)\n",
    "torch_layernorm = nn.LayerNorm(emb_dim)\n",
    "torch_layernorm.weight.data = torch.tensor(weight, requires_grad=True, dtype=torch.float64)\n",
    "torch_layernorm.bias.data = torch.tensor(bias, requires_grad=True, dtype=torch.float64)\n",
    "torch_out = torch_layernorm(torch_input)\n",
    "\n",
    "# проверка что forward работает также как у torch, \n",
    "print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), out))\n",
    "\n",
    "print(\"Shape выхода слоя:\", out.shape)\n",
    "\n",
    "# случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "check_error = np.random.normal(loc=-3, scale=100, size=out.shape)\n",
    "check_error_torch = torch.tensor(check_error)\n",
    "\n",
    "torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "weight_grad, bias_grad, in_error = layernorm.backward(check_error)  # тоже самое, только в ручном слое\n",
    "\n",
    "# проверка градиентов весов и входа\n",
    "print(\"Градиент weight совпадает:\", np.allclose(torch_layernorm.weight.grad.detach().numpy(), weight_grad))\n",
    "print(\"Градиент bias совпадает:\", np.allclose(torch_layernorm.bias.grad.detach().numpy(), bias_grad))\n",
    "print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), in_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1819af3e-e6fe-4542-9809-7202f92c4d94",
   "metadata": {},
   "source": [
    "# gpt fully connected check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bfe5566-24b2-4507-8d1a-9f9711a1b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(BaseLayer):\n",
    "    \"\"\"\n",
    "    Activation class is used for activation function of the FC layer\n",
    "    Params:\n",
    "    activation_function - activation function (e.g. sigmoid, RElU, tanh)\n",
    "    activation_derivative - derivative of the activation function\n",
    "    Methods:\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_function: callable, activation_derivative: callable) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.activation = activation_function\n",
    "        self.derivative = activation_derivative\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return self.activation(x)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return output_error * self.derivative(self.input)\n",
    "\n",
    "def relu(z: Union[np.array, float, int, list]) -> Union[np.array, float, int]:\n",
    "    \"\"\"\n",
    "    ReLU function\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def relu_derivative(z: Union[np.array, float, int, list]) -> np.array:\n",
    "    \"\"\"\n",
    "    ReLU function derivative\n",
    "    \"\"\"\n",
    "    return (z > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf55aab3-2a0e-473a-bf37-f75d7e3c96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFullyConnected(BaseLayer):\n",
    "    def __init__(self, dim):\n",
    "        self.c_fc = Linear3d(dim, 4*dim)\n",
    "        self.relu = Activation(relu, relu_derivative)\n",
    "        self.c_proj = Linear3d(4*dim, dim)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        outputs = self.c_fc(x)\n",
    "\n",
    "        outputs = self.relu(outputs)\n",
    "\n",
    "        outputs = self.c_proj(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        c_proj_w_grad, c_proj_b_grad, c_proj_error = self.c_proj.backward(output_error)\n",
    "        relu_error = self.relu.backward(c_proj_error)\n",
    "        c_fc_w_grad, c_fc_b_grad, c_fc_error = self.c_fc.backward(relu_error)\n",
    "\n",
    "        return c_proj_w_grad, c_proj_b_grad, c_fc_w_grad, c_fc_b_grad, c_fc_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ae98c4a-1cea-401c-beea-867cd4ffd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(dim, 4  * dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.c_proj = nn.Linear(4 * dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch_size, seq_length, dim]\n",
    "        outputs = self.c_fc(x)\n",
    "\n",
    "        outputs = self.relu(outputs)\n",
    "\n",
    "        outputs = self.c_proj(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7be7c31-dd73-4797-9d7b-9ddcd0e71565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 10, 32)\n",
      "Градиент c_proj_weight совпадает: True\n",
      "Градиент c_proj_bias совпадает: True\n",
      "Градиент c_fc_weight совпадает: True\n",
      "Градиент c_fc_bias совпадает: True\n",
      "Градиент по входу совпадает: True\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что gpt fully connected работает также как и в torch\n",
    "batch_size, seq_len, emb_dim = 100, 10, 32\n",
    "sample = np.random.randn(batch_size, seq_len, emb_dim).astype(np.float32)\n",
    "\n",
    "my_mlp = MyFullyConnected(emb_dim)\n",
    "torch_mlp = FullyConnected(emb_dim)\n",
    "# ставим одинаковые иходные веса для слоев\n",
    "my_mlp.c_fc.w = torch_mlp.c_fc.weight.data.numpy().T\n",
    "my_mlp.c_fc.b = torch_mlp.c_fc.bias.data.numpy().reshape(1, -1)\n",
    "my_mlp.c_proj.w = torch_mlp.c_proj.weight.data.numpy().T\n",
    "my_mlp.c_proj.b = torch_mlp.c_proj.bias.data.numpy().reshape(1, -1)\n",
    "\n",
    "out = my_mlp(sample)\n",
    "\n",
    "torch_input = torch.tensor(sample, dtype=torch.float32, requires_grad=True)\n",
    "torch_out = torch_mlp(torch_input)\n",
    "\n",
    "# проверка что forward работает также как у torch, \n",
    "print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), out, atol=1e-6))\n",
    "\n",
    "print(\"Shape выхода слоя:\", out.shape)\n",
    "\n",
    "# случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "check_error = np.random.randn(*out.shape)\n",
    "check_error_torch = torch.tensor(check_error)\n",
    "\n",
    "torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "c_proj_weight_grad, c_proj_bias_grad, c_fc_weight_grad, c_fc_bias_grad, c_fc_in_error = my_mlp.backward(check_error)  # тоже самое, только в ручном слое\n",
    "\n",
    "# проверка градиентов весов и входа\n",
    "print(\"Градиент c_proj_weight совпадает:\", np.allclose(torch_mlp.c_proj.weight.grad.detach().numpy(), c_proj_weight_grad.T, atol=1e-5))\n",
    "print(\"Градиент c_proj_bias совпадает:\", np.allclose(torch_mlp.c_proj.bias.grad.detach().numpy(), c_proj_bias_grad, atol=1e-5))\n",
    "print(\"Градиент c_fc_weight совпадает:\", np.allclose(torch_mlp.c_fc.weight.grad.detach().numpy(), c_fc_weight_grad.T, atol=1e-5))\n",
    "print(\"Градиент c_fc_bias совпадает:\", np.allclose(torch_mlp.c_fc.bias.grad.detach().numpy(), c_fc_bias_grad, atol=1e-5))\n",
    "\n",
    "print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), c_fc_in_error, atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0bbce-abf6-4549-b536-d9011ac99323",
   "metadata": {},
   "source": [
    "# gpt transformer check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89d25770-1c97-4117-b9f4-8ab35e977346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.attn = MaskedSelfAttention(dim, num_heads)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.mlp = FullyConnected(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.ln_1(x)\n",
    "        output1 = self.attn(output1)\n",
    "        output1 = output1 + x\n",
    "\n",
    "        output2 = self.ln_2(output1)\n",
    "        output2 = self.mlp(output2)\n",
    "        output = output2 + output1\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89c32d42-3938-4f70-bfd6-632f4d4b255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(BaseLayer):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        self.ln_1 = LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttentionLayer(dim, num_heads)\n",
    "        self.ln_2 = LayerNorm(dim)\n",
    "        self.mlp = MyFullyConnected(dim)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        output1 = self.ln_1(x)\n",
    "        output1 = self.attn(output1)\n",
    "        output1 = output1 + x\n",
    "\n",
    "        output2 = self.ln_2(output1)\n",
    "        output2 = self.mlp(output2)\n",
    "        output = output2 + output1\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # output = output1 + output2 + x\n",
    "        # doutput_doutput2 = 1 и надо это умножить на output_error\n",
    "        c_proj_w_grad, c_proj_b_grad, c_fc_w_grad, c_fc_b_grad, mlp_error = self.mlp.backward(output_error)\n",
    "        _, _, ln2_error = self.ln_2.backward(mlp_error)\n",
    "        # doutput_doutput1 = 1 и надо это умножить на output_error + ошибка, которая пришла выше\n",
    "        attn_error = self.attn.backward(ln2_error+output_error) \n",
    "        ln1_weight_grad, _, ln1_error = self.ln_1.backward(attn_error)\n",
    "        # doutput_dx = 1 и надо это умножить на output_error + ошибка, которая пришла выше\n",
    "        return c_proj_w_grad, ln1_weight_grad, ln1_error+ln2_error+output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77fd8a7a-f6bf-4766-ba8b-7adb6e29b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 10, 32)\n",
      "Градиент mlp c_proj_weight совпадает: True\n",
      "Градиент ln1 weight совпадает: True\n",
      "Градиент по входу совпадает: True\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что gpt работает также как и в torch\n",
    "batch_size, seq_len, emb_dim = 100, 10, 32\n",
    "sample = np.random.randn(batch_size, seq_len, emb_dim).astype(np.float32)\n",
    "\n",
    "my_transformer = MyTransformer(emb_dim, 8)\n",
    "torch_transformer = TransformerLayer(emb_dim, 8)\n",
    "\n",
    "# ставим одинаковые иходные веса для слоев, layernorm инциализируются одинаково\n",
    "my_transformer.attn.c_attn.w = torch_transformer.attn.c_attn.weight.data.numpy().T\n",
    "my_transformer.attn.c_attn.b = torch_transformer.attn.c_attn.bias.data.numpy()\n",
    "my_transformer.attn.c_proj.w = torch_transformer.attn.c_proj.weight.data.numpy().T\n",
    "my_transformer.attn.c_proj.b = torch_transformer.attn.c_proj.bias.data.numpy()\n",
    "\n",
    "my_transformer.mlp.c_fc.w = torch_transformer.mlp.c_fc.weight.data.numpy().T\n",
    "my_transformer.mlp.c_fc.b = torch_transformer.mlp.c_fc.bias.data.numpy()\n",
    "my_transformer.mlp.c_proj.w = torch_transformer.mlp.c_proj.weight.data.numpy().T\n",
    "my_transformer.mlp.c_proj.b = torch_transformer.mlp.c_proj.bias.data.numpy()\n",
    "\n",
    "out = my_transformer(sample)\n",
    "\n",
    "torch_input = torch.tensor(sample, dtype=torch.float32, requires_grad=True)\n",
    "torch_out = torch_transformer(torch_input)\n",
    "\n",
    "# проверка что forward работает также как у torch, \n",
    "print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), out, atol=1e-6))\n",
    "\n",
    "print(\"Shape выхода слоя:\", out.shape)\n",
    "\n",
    "# # случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "check_error = np.random.randn(*out.shape)\n",
    "check_error_torch = torch.tensor(check_error)\n",
    "\n",
    "torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "c_proj_w_grad, ln1_weight_grad, in_error = my_transformer.backward(check_error)  # тоже самое, только в ручном слое\n",
    "\n",
    "# проверка градиентов весов и входа\n",
    "print(\"Градиент mlp c_proj_weight совпадает:\", np.allclose(torch_transformer.mlp.c_proj.weight.grad.detach().numpy(), c_proj_w_grad.T, atol=1e-5))\n",
    "print(\"Градиент ln1 weight совпадает:\", np.allclose(torch_transformer.ln_1.weight.grad.detach().numpy(), ln1_weight_grad, atol=1e-5))\n",
    "\n",
    "print(\"Градиент по входу совпадает:\", np.allclose(torch_input.grad.detach().numpy(), in_error, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4798ad40-7a62-44d7-b953-80b708052046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 1.2432584 ,  0.16611727, -1.6155889 , ..., -0.45531756,\n",
       "          -0.79728395, -0.29408318],\n",
       "         [-1.3477178 ,  1.2574115 , -2.1409097 , ..., -1.3946066 ,\n",
       "           0.23250823, -0.51982355],\n",
       "         [-0.52904916, -0.01775089, -0.05373952, ..., -2.4188576 ,\n",
       "          -1.1829457 ,  1.1646185 ],\n",
       "         ...,\n",
       "         [ 0.43370864, -0.22322391, -0.8629938 , ..., -0.52332103,\n",
       "           0.62400216,  0.7159316 ],\n",
       "         [-3.0608191 , -0.4776267 , -0.9876322 , ..., -0.20581292,\n",
       "          -0.8383774 ,  1.4449457 ],\n",
       "         [ 0.2463032 , -0.10206585, -0.07061028, ...,  0.14600801,\n",
       "          -0.8268822 , -1.879227  ]],\n",
       " \n",
       "        [[-1.3731863 ,  1.2201213 ,  1.451661  , ..., -0.4690495 ,\n",
       "          -0.83982235, -1.0552561 ],\n",
       "         [ 1.6166644 ,  0.82144153, -0.4345418 , ...,  0.6809467 ,\n",
       "           0.2236872 ,  0.7280807 ],\n",
       "         [-0.7798389 ,  0.450372  ,  1.6508105 , ..., -0.32078117,\n",
       "           0.4679607 , -0.38489607],\n",
       "         ...,\n",
       "         [ 0.53584814,  1.1203095 , -1.1691325 , ...,  0.4867895 ,\n",
       "          -0.19739687,  0.58962405],\n",
       "         [-0.25216874, -1.122847  ,  0.21574035, ..., -0.37815174,\n",
       "           0.15859833, -0.60561824],\n",
       "         [ 1.4078933 , -1.6381502 , -0.6885657 , ...,  0.15440416,\n",
       "           0.74923563,  0.25678653]],\n",
       " \n",
       "        [[-0.17974898, -1.3133391 , -0.9302149 , ..., -0.3298024 ,\n",
       "          -0.95755655, -1.0625978 ],\n",
       "         [-0.28865072, -0.3684951 , -1.5281583 , ...,  1.303878  ,\n",
       "           0.4816141 , -0.63338375],\n",
       "         [ 0.665718  , -2.3602529 , -0.82274216, ...,  0.54124594,\n",
       "          -0.90260255,  2.2872267 ],\n",
       "         ...,\n",
       "         [ 0.63307554, -0.03447639,  0.9594099 , ..., -0.89320475,\n",
       "          -0.33672222,  0.00536692],\n",
       "         [-1.8602597 , -2.185045  ,  0.48234588, ...,  1.4481856 ,\n",
       "          -1.1280445 ,  0.31316647],\n",
       "         [-0.6776633 ,  0.09269326,  1.4459292 , ...,  0.02691777,\n",
       "           0.14843084, -0.36690006]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.24144368, -0.54673606,  0.2164184 , ...,  0.40089625,\n",
       "          -0.0639479 , -2.4039516 ],\n",
       "         [ 0.99244225, -1.03157   , -0.94467753, ...,  1.5171486 ,\n",
       "           1.1177441 ,  0.6227092 ],\n",
       "         [-1.5022088 , -1.2771645 ,  0.3966135 , ...,  0.07663861,\n",
       "          -1.0028502 ,  0.13506964],\n",
       "         ...,\n",
       "         [ 0.16924082,  1.6852006 ,  0.85274225, ..., -0.22761519,\n",
       "           0.14590815, -0.10094165],\n",
       "         [-0.625815  ,  0.5682376 , -0.3102363 , ..., -0.03902183,\n",
       "          -0.88859165, -0.6429085 ],\n",
       "         [-1.7043515 ,  0.17215863,  0.399484  , ...,  0.36729085,\n",
       "           0.38514644,  1.6090883 ]],\n",
       " \n",
       "        [[ 0.06536114, -1.0886227 ,  0.6782167 , ..., -0.72340935,\n",
       "          -0.2808685 ,  2.5005229 ],\n",
       "         [ 0.4389836 , -1.3043202 , -1.1165931 , ..., -0.6562547 ,\n",
       "          -0.53629285,  0.34238434],\n",
       "         [ 1.4317052 , -2.1546938 , -1.0856305 , ...,  0.04533201,\n",
       "           0.17901129,  0.22035924],\n",
       "         ...,\n",
       "         [ 0.78532374,  1.0760968 , -0.5305116 , ..., -0.4655149 ,\n",
       "          -2.4774716 , -1.8832062 ],\n",
       "         [ 0.27356237,  0.70328635,  1.0568986 , ...,  0.8843836 ,\n",
       "           0.3274474 ,  0.31601793],\n",
       "         [ 0.64075226,  1.014851  ,  0.67525595, ...,  1.4950377 ,\n",
       "          -0.86138904,  0.58581597]],\n",
       " \n",
       "        [[-0.3624089 ,  0.34433046,  0.8627778 , ..., -0.7076325 ,\n",
       "          -1.148633  ,  0.3373323 ],\n",
       "         [-0.45281112, -0.4291902 , -1.1529394 , ...,  1.6589863 ,\n",
       "          -0.63983977, -0.37661913],\n",
       "         [-1.4134926 ,  0.20027947,  0.49875057, ..., -0.12130681,\n",
       "           0.2576141 ,  0.42421246],\n",
       "         ...,\n",
       "         [-0.37547806,  0.9487836 , -0.883279  , ..., -0.22727844,\n",
       "          -1.436971  ,  1.4644558 ],\n",
       "         [ 0.8347961 , -2.0987034 , -0.54193497, ..., -2.2795653 ,\n",
       "           0.4998083 ,  0.5430764 ],\n",
       "         [-1.7940412 , -0.12581646,  1.0710375 , ...,  0.58776957,\n",
       "           0.5826229 , -0.8758072 ]]], dtype=float32),\n",
       " array([[[ 1.24325847,  0.16611734, -1.615589  , ..., -0.45531762,\n",
       "          -0.79728365, -0.29408317],\n",
       "         [-1.34771769,  1.2574115 , -2.14090965, ..., -1.39460662,\n",
       "           0.23250836, -0.51982351],\n",
       "         [-0.52904919, -0.01775082, -0.05373956, ..., -2.41885758,\n",
       "          -1.18294573,  1.16461863],\n",
       "         ...,\n",
       "         [ 0.43370872, -0.22322383, -0.86299384, ..., -0.52332102,\n",
       "           0.62400211,  0.71593149],\n",
       "         [-3.06081918, -0.4776267 , -0.9876322 , ..., -0.20581294,\n",
       "          -0.83837739,  1.44494574],\n",
       "         [ 0.24630315, -0.10206584, -0.07061022, ...,  0.14600797,\n",
       "          -0.82688217, -1.87922696]],\n",
       " \n",
       "        [[-1.37318596,  1.22012118,  1.45166097, ..., -0.46904967,\n",
       "          -0.83982233, -1.05525617],\n",
       "         [ 1.61666434,  0.82144157, -0.4345418 , ...,  0.6809469 ,\n",
       "           0.22368712,  0.72808068],\n",
       "         [-0.77983881,  0.45037201,  1.65081055, ..., -0.32078114,\n",
       "           0.46796068, -0.384896  ],\n",
       "         ...,\n",
       "         [ 0.53584821,  1.12030941, -1.16913244, ...,  0.48678946,\n",
       "          -0.19739695,  0.5896241 ],\n",
       "         [-0.25216884, -1.12284692,  0.21574039, ..., -0.37815178,\n",
       "           0.15859831, -0.60561822],\n",
       "         [ 1.40789327, -1.6381502 , -0.68856569, ...,  0.1544042 ,\n",
       "           0.7492356 ,  0.25678659]],\n",
       " \n",
       "        [[-0.17974896, -1.31333918, -0.93021496, ..., -0.32980246,\n",
       "          -0.95755661, -1.06259762],\n",
       "         [-0.28865071, -0.36849507, -1.52815835, ...,  1.3038779 ,\n",
       "           0.48161413, -0.63338376],\n",
       "         [ 0.66571789, -2.36025306, -0.82274214, ...,  0.54124594,\n",
       "          -0.90260241,  2.28722683],\n",
       "         ...,\n",
       "         [ 0.63307556, -0.03447629,  0.9594098 , ..., -0.89320474,\n",
       "          -0.3367222 ,  0.00536686],\n",
       "         [-1.86025968, -2.18504501,  0.48234586, ...,  1.44818562,\n",
       "          -1.12804439,  0.31316649],\n",
       "         [-0.67766333,  0.09269333,  1.4459292 , ...,  0.02691782,\n",
       "           0.14843082, -0.36690007]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.24144366, -0.54673605,  0.21641836, ...,  0.40089616,\n",
       "          -0.06394787, -2.40395177],\n",
       "         [ 0.99244228, -1.03156974, -0.94467752, ...,  1.51714862,\n",
       "           1.11774426,  0.62270907],\n",
       "         [-1.50220874, -1.27716448,  0.39661339, ...,  0.07663848,\n",
       "          -1.00285012,  0.13506958],\n",
       "         ...,\n",
       "         [ 0.16924081,  1.68520054,  0.85274221, ..., -0.2276152 ,\n",
       "           0.14590818, -0.10094172],\n",
       "         [-0.62581498,  0.56823749, -0.31023626, ..., -0.03902178,\n",
       "          -0.88859168, -0.6429085 ],\n",
       "         [-1.7043515 ,  0.17215864,  0.39948393, ...,  0.36729085,\n",
       "           0.38514645,  1.60908818]],\n",
       " \n",
       "        [[ 0.0653613 , -1.08862258,  0.67821674, ..., -0.72340951,\n",
       "          -0.28086842,  2.50052285],\n",
       "         [ 0.43898369, -1.30432014, -1.11659299, ..., -0.6562546 ,\n",
       "          -0.53629279,  0.34238436],\n",
       "         [ 1.43170526, -2.1546939 , -1.08563049, ...,  0.04533204,\n",
       "           0.1790113 ,  0.22035926],\n",
       "         ...,\n",
       "         [ 0.78532374,  1.07609668, -0.5305116 , ..., -0.46551492,\n",
       "          -2.47747169, -1.88320623],\n",
       "         [ 0.27356238,  0.70328636,  1.0568987 , ...,  0.88438359,\n",
       "           0.32744738,  0.31601779],\n",
       "         [ 0.64075218,  1.01485105,  0.67525593, ...,  1.4950377 ,\n",
       "          -0.86138913,  0.58581601]],\n",
       " \n",
       "        [[-0.36240893,  0.34433034,  0.86277779, ..., -0.7076326 ,\n",
       "          -1.14863309,  0.3373324 ],\n",
       "         [-0.45281102, -0.42919023, -1.15293939, ...,  1.65898617,\n",
       "          -0.63983972, -0.37661904],\n",
       "         [-1.4134926 ,  0.20027951,  0.49875053, ..., -0.12130676,\n",
       "           0.25761412,  0.42421249],\n",
       "         ...,\n",
       "         [-0.37547815,  0.94878348, -0.88327903, ..., -0.22727846,\n",
       "          -1.43697097,  1.46445585],\n",
       "         [ 0.83479623, -2.09870349, -0.54193508, ..., -2.27956534,\n",
       "           0.49980843,  0.54307642],\n",
       "         [-1.79404108, -0.12581658,  1.07103751, ...,  0.58776949,\n",
       "           0.58262294, -0.87580718]]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input.grad.detach().numpy(), in_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14817ea7-b07b-489d-a1c8-edf3b54de9b7",
   "metadata": {},
   "source": [
    "# conv2d check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea2d0d69-3c0d-447b-a63f-21418bcdcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(BaseLayer):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels  # number of input channels\n",
    "        self.out_channels = out_channels  # number of output channels\n",
    "        self.kernel_size = kernel_size  # kernel size, int\n",
    "        self.stride = stride  # stride, int\n",
    "        self.padding = padding  # padding, int, only zeros padding\n",
    "\n",
    "        limit = 1 / np.sqrt(self.kernel_size ** 2)\n",
    "        self.kernel = np.random.uniform(\n",
    "            -limit,\n",
    "            limit,\n",
    "            size=(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        )\n",
    "\n",
    "        self.bias = np.zeros((self.out_channels, 1))\n",
    "        self.kernel_optimizer = None\n",
    "        self.bias_optimizer = None\n",
    "\n",
    "        self.input_tensor = None\n",
    "        self.batch_size = None\n",
    "        self.reformatted_input = None\n",
    "        self.reformatted_kernel = None\n",
    "\n",
    "        self.channel_steps = []\n",
    "\n",
    "    def set_kernel(self, kernel: np.array) -> None:\n",
    "        \"\"\"\n",
    "        For setting the kernel manually\n",
    "        \"\"\"\n",
    "        assert len(kernel.shape) == 4\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.kernel_optimizer = copy.copy(optimizer)\n",
    "        self.bias_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.kernel_optimizer.set_weight(self.kernel)\n",
    "        self.bias_optimizer.set_weight(self.bias)\n",
    "\n",
    "    def _reformat_kernel(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformat the kernel to perform convolution as matrix multiplication\n",
    "        \"\"\"\n",
    "        assert len(self.kernel.shape) == 4\n",
    "        return self.kernel.reshape((self.out_channels, -1))\n",
    "\n",
    "    def _reformat_input(self, input_tensor: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformat the batch of input images to perform convolution as matrix multiplication\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for image in input_tensor:\n",
    "            result.append(self._reformat_image(image))\n",
    "\n",
    "        return np.hstack(result)\n",
    "\n",
    "    def _reformat_image(self, image: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformatting each image in the batch\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for channel in image:\n",
    "            result.append(self._reformat_channel(channel))\n",
    "\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def _reformat_channel(self, channel: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformat each channel in the image with addinction of zeros as padding\n",
    "        \"\"\"\n",
    "        input_map = self._add_padding(channel)\n",
    "\n",
    "        self.padded_width_in, self.padded_height_in = input_map.shape\n",
    "\n",
    "        width_in, height_in = channel.shape\n",
    "\n",
    "        self.width_out = (width_in - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        self.height_out = (height_in - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        output_map = []\n",
    "\n",
    "        write_steps = True\n",
    "        if self.channel_steps:\n",
    "            write_steps = False\n",
    "\n",
    "        for i in range(self.width_out):\n",
    "            for j in range(self.height_out):\n",
    "                horizontal_slice_from = i * self.stride\n",
    "                horizontal_slice_to = self.kernel_size + i * self.stride\n",
    "\n",
    "                vertical_slice_from = j * self.stride\n",
    "                vertical_slice_to = self.kernel_size + j * self.stride\n",
    "\n",
    "                if write_steps:\n",
    "                    self.channel_steps.append(\n",
    "                        (horizontal_slice_from, horizontal_slice_to, vertical_slice_from, vertical_slice_to)\n",
    "                    )\n",
    "\n",
    "                subsample = input_map[horizontal_slice_from:horizontal_slice_to, vertical_slice_from:vertical_slice_to]\n",
    "                output_map.append(subsample.reshape(-1, 1))\n",
    "\n",
    "        return np.hstack(output_map)\n",
    "\n",
    "    def _add_padding(self, input_tensor: np.array) -> np.array:\n",
    "        new_width = input_tensor.shape[0] + self.padding * 2\n",
    "        new_height = input_tensor.shape[1] + self.padding * 2\n",
    "        padded_map = np.zeros((new_width, new_height))\n",
    "\n",
    "        padded_map[self.padding:new_width - self.padding, self.padding:new_height - self.padding] = input_tensor\n",
    "        return padded_map\n",
    "\n",
    "    def _reformat_forward(self, forward_output: np.array) -> np.array:\n",
    "        result = []\n",
    "        for image in forward_output:\n",
    "            result.append(image.reshape(self.batch_size, self.height_out, self.width_out))\n",
    "        return np.hstack(result).reshape((self.batch_size, self.out_channels, self.height_out, self.width_out))\n",
    "\n",
    "    @staticmethod\n",
    "    def _reformat_output(output_error: np.array) -> np.array:\n",
    "        images_result = []\n",
    "        for image in output_error:\n",
    "            channel_result = []\n",
    "            for channel in image:\n",
    "                channel_result.append(channel.ravel())\n",
    "            images_result.append(np.vstack(channel_result))\n",
    "        return np.hstack(images_result)\n",
    "\n",
    "    def _format_image_back(self, image: np.array) -> np.array:\n",
    "        zeros = np.zeros((self.in_channels, self.padded_width_in, self.padded_height_in))\n",
    "        elements_in_kernel = self.kernel_size ** 2\n",
    "        for i in range(self.in_channels):\n",
    "            for j, line in enumerate(image.T):\n",
    "                reshaped_conv = line[i * elements_in_kernel:(i + 1) * elements_in_kernel].reshape(self.kernel_size,\n",
    "                                                                                                  self.kernel_size)\n",
    "\n",
    "                horizontal_slice_from, horizontal_slice_to, vertical_slice_from, vertical_slice_to = self.channel_steps[\n",
    "                    j]\n",
    "                zeros[i][horizontal_slice_from:horizontal_slice_to,\n",
    "                         vertical_slice_from:vertical_slice_to] += reshaped_conv\n",
    "        return zeros\n",
    "\n",
    "    def _cut_padding(self, image: np.array) -> np.array:\n",
    "        result = []\n",
    "        for channel in image:\n",
    "            channel_no_pad = channel[self.padding:self.padded_width_in - self.padding,\n",
    "                                     self.padding:self.padded_height_in - self.padding]\n",
    "            result.append(channel_no_pad[np.newaxis, :])\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def _reformat_input_error(self, input_error: np.array) -> np.array:\n",
    "        assert len(input_error.T) % self.batch_size == 0\n",
    "        size_for_one_image = int(len(input_error.T) / self.batch_size)\n",
    "        result = []\n",
    "        for i in range(self.batch_size):\n",
    "            image = input_error[:, i * size_for_one_image:(i + 1) * size_for_one_image]\n",
    "            image = self._format_image_back(image)\n",
    "\n",
    "            if self.padding != 0:\n",
    "                image = self._cut_padding(image)\n",
    "\n",
    "            result.append(image[np.newaxis, :])\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def forward(self, input_tensor: np.array, grad: bool = False) -> np.array:\n",
    "        assert len(input_tensor.shape) == 4\n",
    "        self.input_tensor = input_tensor\n",
    "        self.batch_size = self.input_tensor.shape[0]\n",
    "\n",
    "        self.reformatted_input = self._reformat_input(input_tensor)\n",
    "        self.reformatted_kernel = self._reformat_kernel()\n",
    "\n",
    "        result = self.reformatted_kernel.dot(self.reformatted_input) + self.bias\n",
    "        return self._reformat_forward(result)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        # assert self.kernel_optimizer is not None and self.bias_optimizer is not None, 'You should set an optimizer'\n",
    "        output_error = self._reformat_output(output_error)\n",
    "        w_grad = output_error.dot(self.reformatted_input.T).reshape(self.kernel.shape)\n",
    "        b_grad = output_error.dot(np.ones((output_error.shape[1], 1)))\n",
    "\n",
    "        # self.kernel = self.kernel_optimizer.step(w_grad)\n",
    "        # self.bias = self.bias_optimizer.step(b_grad)\n",
    "\n",
    "        input_error = self.reformatted_kernel.T.dot(output_error)#.reshape(self.input_tensor.shape)\n",
    "        input_error = self._reformat_input_error(input_error)\n",
    "        return w_grad, b_grad, input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0e9bbc5-ac95-4a60-827b-5873d2576556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ставим параметры stride и padding: {'stride': 1, 'padding': 0}\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 8, 29, 29)\n",
      "Градиент weight совпадает: True\n",
      "Градиент bias совпадает: True\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n",
      "Ставим параметры stride и padding: {'stride': 3, 'padding': 4}\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 8, 13, 13)\n",
      "Градиент weight совпадает: True\n",
      "Градиент bias совпадает: True\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n",
      "Ставим параметры stride и padding: {'stride': 2, 'padding': 2}\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 8, 17, 17)\n",
      "Градиент weight совпадает: True\n",
      "Градиент bias совпадает: True\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что conv2d работает также как и в torch\n",
    "batch_size, channels_in, height, width = 100, 10, 32, 32\n",
    "\n",
    "params = [{'stride': 1, 'padding': 0}, {'stride': 3, 'padding': 4}, {'stride': 2, 'padding': 2}]\n",
    "\n",
    "for param in params:\n",
    "    print('Ставим параметры stride и padding:', param)\n",
    "    sample = np.random.randn(batch_size, channels_in, height, width)\n",
    "\n",
    "    my_conv = Conv2D(in_channels=channels_in, out_channels=8, kernel_size=4, **param)\n",
    "    torch_conv = nn.Conv2d(in_channels=channels_in, out_channels=8, kernel_size=4, **param)\n",
    "    \n",
    "    # ставим одинаковые иходные веса для слоев, layernorm инциализируются одинаково\n",
    "    torch_conv.weight.data = torch.tensor(my_conv.kernel, requires_grad=True)\n",
    "    torch_conv.bias.data = torch.tensor(my_conv.bias.ravel(), requires_grad=True)\n",
    "    \n",
    "    out = my_conv(sample)\n",
    "    \n",
    "    torch_input = torch.tensor(sample, requires_grad=True)\n",
    "    torch_out = torch_conv(torch_input)\n",
    "    \n",
    "    # проверка что forward работает также как у torch, \n",
    "    print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), out, atol=1e-6))\n",
    "    \n",
    "    print(\"Shape выхода слоя:\", out.shape)\n",
    "    \n",
    "    # # случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "    check_error = np.random.randn(*out.shape)\n",
    "    check_error_torch = torch.tensor(check_error)\n",
    "    \n",
    "    torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "    w_grad, b_grad, input_error = my_conv.backward(check_error)  # тоже самое, только в ручном слое\n",
    "    \n",
    "    # проверка градиентов весов и входа\n",
    "    print(\"Градиент weight совпадает:\", np.allclose(w_grad, torch_conv.weight.grad.data.detach().numpy()))\n",
    "    print(\"Градиент bias совпадает:\", np.allclose(b_grad.ravel(), torch_conv.bias.grad.data.detach().numpy()))\n",
    "    \n",
    "    print(\"Градиент по входу совпадает:\", np.allclose(input_error, torch_input.grad.data.detach().numpy()))\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e06c39-3f81-460d-bc01-390b882620a9",
   "metadata": {},
   "source": [
    "# maxpool2d check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d78892ab-59ca-42b8-b647-f67c5878827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D(BaseLayer):\n",
    "    def __init__(self, kernel_size: int, stride: int, padding: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.input_tensor = None\n",
    "        self.channel_steps = []\n",
    "        self.grad = None\n",
    "        self.padded_width_in = None\n",
    "        self.padded_height_in = None\n",
    "\n",
    "    def _reformat_input(self, input_tensor: np.array) -> np.array:\n",
    "        result = []\n",
    "        grad_result = []\n",
    "        for image in input_tensor:\n",
    "            forward_result, grad = self._reformat_image(image)\n",
    "            result.append(forward_result)\n",
    "            grad_result.append(grad)\n",
    "\n",
    "        self.grad = grad_result\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def _reformat_image(self, image: np.array) -> np.array:\n",
    "        result = []\n",
    "        grad_result = []\n",
    "        for channel in image:\n",
    "            forward_result, grad = self._reformat_channel(channel)\n",
    "            result.append(forward_result)\n",
    "            grad_result.append(grad)\n",
    "\n",
    "        return np.vstack(result), grad_result\n",
    "\n",
    "    def _reformat_channel(self, channel: np.array) -> Tuple[np.array, np.array]:\n",
    "        input_map = self._add_padding(channel)\n",
    "\n",
    "        width_in, height_in = channel.shape\n",
    "\n",
    "        self.width_out = (width_in - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        self.height_out = (height_in - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        output_map = np.zeros((self.height_out, self.width_out))\n",
    "\n",
    "        grad = []\n",
    "\n",
    "        write_steps = True\n",
    "        if self.channel_steps:\n",
    "            write_steps = False\n",
    "\n",
    "        for i in range(self.width_out):\n",
    "            for j in range(self.height_out):\n",
    "                horizontal_slice_from = i * self.stride\n",
    "                horizontal_slice_to = self.kernel_size + i * self.stride\n",
    "\n",
    "                vertical_slice_from = j * self.stride\n",
    "                vertical_slice_to = self.kernel_size + j * self.stride\n",
    "\n",
    "                if write_steps:\n",
    "                    self.channel_steps.append(\n",
    "                        (horizontal_slice_from, horizontal_slice_to, vertical_slice_from, vertical_slice_to)\n",
    "                    )\n",
    "\n",
    "                subsample = input_map[horizontal_slice_from:horizontal_slice_to, vertical_slice_from:vertical_slice_to]\n",
    "                output_map[i][j] = np.max(subsample)\n",
    "\n",
    "                grad.append(np.argmax(subsample))\n",
    "\n",
    "        return output_map, grad\n",
    "\n",
    "    def _add_padding(self, input_tensor: np.array) -> np.array:\n",
    "        self.padded_width_in = input_tensor.shape[0] + self.padding * 2\n",
    "        self.padded_height_in = input_tensor.shape[1] + self.padding * 2\n",
    "        padded_map = np.full((self.padded_width_in, self.padded_height_in), float('-inf'))\n",
    "\n",
    "        padded_map[self.padding:self.padded_width_in - self.padding, self.padding:self.padded_height_in - self.padding] = input_tensor\n",
    "        return padded_map\n",
    "\n",
    "    def _rebuild_output_error(self, output_error: np.array) -> np.array:\n",
    "        batch_size, channels_num, = self.input_tensor.shape[0], self.input_tensor.shape[1]\n",
    "        zeros = np.zeros((batch_size, channels_num, self.padded_height_in, self.padded_width_in))\n",
    "\n",
    "        input_error = []\n",
    "        for image_num, image in enumerate(output_error):\n",
    "            for channel_num, channel in enumerate(image):\n",
    "                for i in range(self.width_out * self.height_out):\n",
    "                    horizontal_slice_from, horizontal_slice_to, vertical_slice_from, vertical_slice_to \\\n",
    "                        = self.channel_steps[i]\n",
    "\n",
    "                    slice_ = zeros[image_num][channel_num][horizontal_slice_from:horizontal_slice_to,\n",
    "                                                           vertical_slice_from:vertical_slice_to]\n",
    "                    cur_max = self.grad[image_num][channel_num][i]\n",
    "\n",
    "                    flatten_channel = channel.flatten()\n",
    "                    flatten_zeros = slice_.flatten()\n",
    "\n",
    "                    flatten_zeros[cur_max] += flatten_channel[i] # !!!\n",
    "\n",
    "                    slice_ = flatten_zeros.reshape(self.kernel_size, self.kernel_size)\n",
    "                    zeros[image_num][channel_num][horizontal_slice_from:horizontal_slice_to,\n",
    "                                                  vertical_slice_from:vertical_slice_to] = slice_\n",
    "\n",
    "            if self.padding != 0:\n",
    "                input_error_image = self._cut_padding(zeros[image_num])\n",
    "            else:\n",
    "                input_error_image = zeros[image_num]\n",
    "            input_error.append(input_error_image)\n",
    "\n",
    "        input_error = np.vstack(input_error).reshape(self.input_tensor.shape)\n",
    "        return input_error\n",
    "\n",
    "    def _cut_padding(self, image: np.array) -> np.array:\n",
    "        result = []\n",
    "        for channel in image:\n",
    "            channel_no_pad = channel[self.padding:self.padded_width_in - self.padding,\n",
    "                                     self.padding:self.padded_height_in - self.padding]\n",
    "            result.append(channel_no_pad[np.newaxis, :])\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def forward(self, input_tensor: np.array, grad: bool = False) -> np.array:\n",
    "        assert len(input_tensor.shape) == 4\n",
    "        self.input_tensor = input_tensor\n",
    "\n",
    "        result = self._reformat_input(input_tensor)\n",
    "        return result.reshape((input_tensor.shape[0], input_tensor.shape[1], self.height_out, self.width_out))\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return self._rebuild_output_error(output_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cf6f101-9948-4695-8eed-3928104e4e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ставим параметры: {'kernel_size': 2, 'stride': 1, 'padding': 0}\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 10, 31, 31)\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n",
      "Ставим параметры: {'kernel_size': 4, 'stride': 3, 'padding': 2}\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 10, 11, 11)\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n",
      "Ставим параметры: {'kernel_size': 4, 'stride': 2, 'padding': 2}\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 10, 17, 17)\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n",
      "Ставим параметры: {'kernel_size': 2, 'stride': 2, 'padding': 0}\n",
      "Vanilla Forward такой же как и torch forward: True\n",
      "Shape выхода слоя: (100, 10, 16, 16)\n",
      "Градиент по входу совпадает: True\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# проверка того, что maxpool2d работает также как и в torch\n",
    "batch_size, channels_in, height, width = 100, 10, 32, 32\n",
    "\n",
    "params = [\n",
    "    {'kernel_size': 2, 'stride': 1, 'padding': 0},\n",
    "    {'kernel_size': 4, 'stride': 3, 'padding': 2},\n",
    "    {'kernel_size': 4, 'stride': 2, 'padding': 2},\n",
    "    {'kernel_size': 2, 'stride': 2, 'padding': 0}\n",
    "]\n",
    "\n",
    "for param in params:\n",
    "    print('Ставим параметры:', param)\n",
    "    sample = np.random.randn(batch_size, channels_in, height, width)\n",
    "\n",
    "    my_pool = MaxPool2D(**param)\n",
    "    torch_pool = nn.MaxPool2d(**param)\n",
    "    \n",
    "    out = my_pool(sample)\n",
    "    \n",
    "    torch_input = torch.tensor(sample, requires_grad=True)\n",
    "    torch_out = torch_pool(torch_input)\n",
    "    \n",
    "    # проверка что forward работает также как у torch, \n",
    "    print(\"Vanilla Forward такой же как и torch forward:\", np.allclose(torch_out.detach().numpy(), out, atol=1e-6))\n",
    "    \n",
    "    print(\"Shape выхода слоя:\", out.shape)\n",
    "    \n",
    "    # # случайная ошибка, которая приходит \"сверху\" от вышестоящих слоев, по размеру она совпадает с выходом слоя\n",
    "    check_error = np.random.randn(*out.shape)\n",
    "    check_error_torch = torch.tensor(check_error)\n",
    "    \n",
    "    torch_out.backward(check_error_torch)  # считаем градиенты для всех тензоров, которые участвуют в forward проходе\n",
    "    input_error = my_pool.backward(check_error)  # тоже самое, только в ручном слое\n",
    "    \n",
    "    # проверка градиентов входа\n",
    "    print(\"Градиент по входу совпадает:\", np.allclose(input_error, torch_input.grad.data.detach().numpy()))\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06395571-677c-4a18-9cee-fd310d9b2461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yandex_nlp",
   "language": "python",
   "name": "yandex_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
