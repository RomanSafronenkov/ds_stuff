{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a851b1a-5955-4df8-bd8a-31c899c42605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\MyProjects\\venvs\\uplift\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from catboost.datasets import msrank_10k\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils import compute_ideal_dcg, ndcg\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, n_estimators: int = 100, lr: float = 0.5, ndcg_top_k: int = 10,\n",
    "                 subsample: float = 0.6, colsample_bytree: float = 0.9,\n",
    "                 max_depth: int = 5, min_samples_leaf: int = 8):\n",
    "        self._prepare_data()\n",
    "\n",
    "        self.ndcg_top_k = ndcg_top_k\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "\n",
    "        self._estimators = []\n",
    "        self._col_indexes = []\n",
    "\n",
    "    def _get_data(self) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Загрузка данных MSRANK\n",
    "        \"\"\"\n",
    "        train_df, test_df = msrank_10k()\n",
    "\n",
    "        X_train = train_df.drop([0, 1], axis=1).values\n",
    "        y_train = train_df[0].values\n",
    "        query_ids_train = train_df[1].values.astype(int)\n",
    "\n",
    "        X_test = test_df.drop([0, 1], axis=1).values\n",
    "        y_test = test_df[0].values\n",
    "        query_ids_test = test_df[1].values.astype(int)\n",
    "\n",
    "        return [X_train, y_train, query_ids_train, X_test, y_test, query_ids_test]\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Подготовка данных, сохранение в качестве тензоров\n",
    "        \"\"\"\n",
    "        (X_train, y_train, self.query_ids_train,\n",
    "            X_test, y_test, self.query_ids_test) = self._get_data()\n",
    "        \n",
    "        self.X_train = self._scale_features_in_query_groups(X_train, self.query_ids_train)\n",
    "        self.X_test = self._scale_features_in_query_groups(X_test, self.query_ids_test)\n",
    "\n",
    "        self.ys_train = torch.as_tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        self.ys_test = torch.as_tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def _scale_features_in_query_groups(self, inp_feat_array: np.ndarray,\n",
    "                                        inp_query_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Масштабирование признаков, происходит в каждом запросе независимо\n",
    "        \"\"\"\n",
    "        scaled_features = np.zeros_like(inp_feat_array)\n",
    "        \n",
    "        for query in np.unique(inp_query_ids):\n",
    "            idxs = np.where(inp_query_ids == query)[0]\n",
    "            scaled_features[idxs] = StandardScaler().fit_transform(inp_feat_array[idxs])\n",
    "        return torch.as_tensor(scaled_features, dtype=torch.float32)\n",
    "\n",
    "    def _train_one_tree(self, cur_tree_idx: int,\n",
    "                        train_preds: torch.FloatTensor\n",
    "                        ) -> Tuple[DecisionTreeRegressor, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Обучить одно дерево: считаем lambda, обучаем дерево их предсказывать\n",
    "        \"\"\"\n",
    "        np.random.seed(cur_tree_idx)\n",
    "        \n",
    "        tree_ = DecisionTreeRegressor(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            random_state=cur_tree_idx\n",
    "        )\n",
    "\n",
    "        lambdas = self._compute_lambdas(self.ys_train, train_preds)\n",
    "\n",
    "        n_rows, n_cols = self.X_train.shape\n",
    "        cols_to_train = np.random.choice(np.arange(n_cols), size=np.ceil(n_cols*self.colsample_bytree).astype(int), replace=False)\n",
    "        rows_to_train = np.random.choice(np.arange(n_rows), size=np.ceil(n_rows*self.subsample).astype(int))\n",
    "\n",
    "        x_train = self.X_train[:, cols_to_train]\n",
    "        x_train = x_train[rows_to_train]\n",
    "        y_train = lambdas[rows_to_train]\n",
    "        tree_.fit(x_train, y_train)\n",
    "        \n",
    "        return tree_, cols_to_train\n",
    "\n",
    "    def _calc_data_ndcg(self, queries_list: np.ndarray,\n",
    "                        true_labels: torch.FloatTensor, preds: torch.FloatTensor) -> float:\n",
    "        \"\"\"\n",
    "        Подсчет NDCG в группах, для каждого запроса с последующим усреднением по всем\n",
    "        \"\"\"\n",
    "        ndcgs = []\n",
    "        preds = preds.flatten()\n",
    "        true_labels = true_labels.flatten()\n",
    "        \n",
    "        for query in np.unique(queries_list):\n",
    "            query_idxs = np.where(queries_list == query)[0]\n",
    "            \n",
    "            ndcg = self._ndcg_k(true_labels[query_idxs], preds[query_idxs], ndcg_top_k=self.ndcg_top_k)\n",
    "\n",
    "            ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Обучение модели на данных MSRANK\n",
    "        Также отслеживается метрика NDCG для тренировочных и тестовых данных, с последующим обрезанием итоговой модели\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        preds = torch.zeros((self.X_train.shape[0], 1), dtype=torch.float32)\n",
    "        test_preds = torch.zeros((self.X_test.shape[0], 1), dtype=torch.float32)\n",
    "\n",
    "        train_ndcgs = []\n",
    "        test_ndcgs = []\n",
    "\n",
    "        best_i = 0\n",
    "        best_score = 0\n",
    "\n",
    "        for cur_tree_idx in tqdm(range(self.n_estimators)):\n",
    "            tree_, col_idx = self._train_one_tree(\n",
    "                cur_tree_idx=cur_tree_idx,\n",
    "                train_preds=preds\n",
    "            )\n",
    "            self._estimators.append(tree_)\n",
    "            self._col_indexes.append(col_idx)\n",
    "\n",
    "            tree_predict = tree_.predict(self.X_train[:, col_idx])\n",
    "            tree_predict = torch.as_tensor(tree_predict, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "            preds -= self.lr * tree_predict\n",
    "\n",
    "            train_ndcg = self._calc_data_ndcg(self.query_ids_train, self.ys_train, preds)\n",
    "            train_ndcgs.append(train_ndcg)\n",
    "\n",
    "            test_tree_predict = tree_.predict(self.X_test[:, col_idx])\n",
    "            test_tree_predict = torch.as_tensor(test_tree_predict, dtype=torch.float32).view(-1, 1)\n",
    "            test_preds -= self.lr * test_tree_predict\n",
    "            \n",
    "            test_ndcg = self._calc_data_ndcg(self.query_ids_test, self.ys_test, test_preds)\n",
    "            test_ndcgs.append(test_ndcg)\n",
    "\n",
    "            if test_ndcg > best_score:\n",
    "                best_score = test_ndcg\n",
    "                best_i = cur_tree_idx\n",
    "\n",
    "        print(f'Best iteration: {best_i}, shrink model to the first {best_i} iterations')\n",
    "        self._estimators = self._estimators[:best_i+1]\n",
    "\n",
    "        return train_ndcgs, test_ndcgs, best_i\n",
    "\n",
    "    def predict(self, data: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Предсказание модели\n",
    "        \"\"\"\n",
    "        preds = torch.zeros((data.shape[0], 1), dtype=torch.float32)\n",
    "        \n",
    "        for i, tree_ in enumerate(self._estimators):\n",
    "            data_to_predict = data[:, self._col_indexes[i]]\n",
    "\n",
    "            tree_pred = tree_.predict(data_to_predict)\n",
    "            tree_pred = torch.as_tensor(tree_pred, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "            preds -= self.lr * tree_pred\n",
    "\n",
    "        return preds\n",
    "            \n",
    "\n",
    "    def _compute_lambdas(self, y_true: torch.FloatTensor, y_pred: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Код для подсчета lambda значений, подробнее: https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf\n",
    "        \"\"\"\n",
    "        \n",
    "        def compute_lambdas(y_true, y_pred, ndcg_scheme='exp2'):\n",
    "            # рассчитаем нормировку, IdealDCG\n",
    "            ideal_dcg = compute_ideal_dcg(y_true, gain_scheme=ndcg_scheme)\n",
    "            N = 1 / ideal_dcg\n",
    "            \n",
    "            # рассчитаем порядок документов согласно оценкам релевантности\n",
    "            _, rank_order = torch.sort(y_true, descending=True, axis=0)\n",
    "            rank_order += 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # получаем все попарные разницы скоров в батче\n",
    "                pos_pairs_score_diff = 1.0 + torch.exp((y_pred - y_pred.t()))\n",
    "                \n",
    "                # поставим разметку для пар, 1 если первый документ релевантнее\n",
    "                # -1 если второй документ релевантнее\n",
    "                Sij = compute_labels_in_batch(y_true)\n",
    "                # посчитаем изменение gain из-за перестановок\n",
    "                gain_diff = compute_gain_diff(y_true, ndcg_scheme)\n",
    "                \n",
    "                # посчитаем изменение знаменателей-дискаунтеров\n",
    "                decay_diff = (1.0 / torch.log2(rank_order + 1.0)) - (1.0 / torch.log2(rank_order.t() + 1.0))\n",
    "                # посчитаем непосредственное изменение nDCG\n",
    "                delta_ndcg = torch.abs(N * gain_diff * decay_diff)\n",
    "                # посчитаем лямбды\n",
    "                lambda_update =  (0.5 * (1 - Sij) - 1 / pos_pairs_score_diff) * delta_ndcg\n",
    "                lambda_update = torch.sum(lambda_update, dim=1, keepdim=True)\n",
    "                \n",
    "                return lambda_update\n",
    "    \n",
    "    \n",
    "        def compute_labels_in_batch(y_true):\n",
    "            \n",
    "            # разница релевантностей каждого с каждым объектом\n",
    "            rel_diff = y_true - y_true.t()\n",
    "            \n",
    "            # 1 в этой матрице - объект более релевантен\n",
    "            pos_pairs = (rel_diff > 0).type(torch.float32)\n",
    "            \n",
    "            # 1 тут - объект менее релевантен\n",
    "            neg_pairs = (rel_diff < 0).type(torch.float32)\n",
    "            Sij = pos_pairs - neg_pairs\n",
    "            return Sij\n",
    "        \n",
    "        def compute_gain_diff(y_true, gain_scheme):\n",
    "            if gain_scheme == \"exp2\":\n",
    "                gain_diff = torch.pow(2.0, y_true) - torch.pow(2.0, y_true.t())\n",
    "            elif gain_scheme == \"diff\":\n",
    "                gain_diff = y_true - y_true.t()\n",
    "            else:\n",
    "                raise ValueError(f\"{gain_scheme} method not supported\")\n",
    "            return gain_diff\n",
    "\n",
    "        lambdas = compute_lambdas(y_true, y_pred)\n",
    "        return lambdas\n",
    "\n",
    "    def _ndcg_k(self, ys_true, ys_pred, ndcg_top_k) -> float:\n",
    "        \"\"\"\n",
    "        Подсчет метрики NDCG в топ k значениях\n",
    "        \"\"\"\n",
    "        \n",
    "        def compute_gain(y_value: float, gain_scheme: str) -> float:\n",
    "            assert gain_scheme in ['const', 'exp2']\n",
    "            if gain_scheme == 'const':\n",
    "                return y_value\n",
    "            elif gain_scheme == 'exp2':\n",
    "                return 2 ** y_value - 1\n",
    "        \n",
    "        \n",
    "        def dcg_k(ys_true: torch.Tensor, ys_pred: torch.Tensor, gain_scheme: str, k: int) -> float:\n",
    "            ys_true, ys_pred = ys_true.flatten(), ys_pred.flatten()\n",
    "            \n",
    "            dcg_value = 0\n",
    "            _, sorted_ys_pred_idx = torch.sort(ys_pred, descending=True)\n",
    "            for i, rel in enumerate(ys_true[sorted_ys_pred_idx][:k]):\n",
    "                dcg_value += compute_gain(rel, gain_scheme=gain_scheme) / math.log2(i+2)\n",
    "            return dcg_value.item()\n",
    "\n",
    "        # расчет по экспоненциальной формуле\n",
    "        try:\n",
    "            dcg_value = dcg_k(ys_true, ys_pred, gain_scheme='exp2', k=ndcg_top_k)\n",
    "            perfect_dcg = dcg_k(ys_true, ys_true, gain_scheme='exp2', k=ndcg_top_k)\n",
    "\n",
    "            ndcg_value = dcg_value / perfect_dcg\n",
    "        except ZeroDivisionError:\n",
    "            ndcg_value = 0\n",
    "        \n",
    "        return ndcg_value\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"\n",
    "        Сохранение модели\n",
    "        \"\"\"\n",
    "        params_to_save = {\n",
    "            '_estimators': self._estimators,\n",
    "            '_col_indexes': self._col_indexes,\n",
    "            'lr': self.lr\n",
    "        }\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(params_to_save, f)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"\n",
    "        Загрузка модели inplace\n",
    "        \"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        self._estimators = params['_estimators']\n",
    "        self._col_indexes = params['_col_indexes']\n",
    "        self.lr = params['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173d22bd-b85a-4d0c-ba44-640a29d9cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = Solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44080456-8780-46e3-8e8e-c961f6081efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:26<00:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 94, shrink model to the first 94 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ndcgs, test_ndcgs, best_i = solution.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab1eda1-90cc-467d-a0ef-74ff67b5e5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_ndcgs)\n",
    "plt.plot(test_ndcgs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ccc946-63cb-432d-8125-4d202bdef2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.save_model('./lambdamart.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1363b6-4930-4fb0-a963-2d8636bb4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = Solution()\n",
    "solution.load_model('./lambdamart.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71587e7e-5522-402b-b02f-ec47ed6c56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = solution.predict(solution.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f2a956-1a77-4130-8557-5d37692f86f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3898748383608827, 0.3898748383608827)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution._calc_data_ndcg(solution.query_ids_test, solution.ys_test, test_preds), test_ndcgs[best_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828e49a9-72b6-4d77-9ffa-6e27bfa37b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 16),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'lr': trial.suggest_float('lr', 0.01, 1),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'ndcg_top_k': 10\n",
    "    }\n",
    "    \n",
    "    model = Solution(**params)\n",
    "    train_ndcgs, test_ndcgs, best_i = model.fit()\n",
    "    \n",
    "    return test_ndcgs[best_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b09d8c20-da5c-47bc-92d1-db8b3eac56d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-03 20:34:20,881] A new study created in memory with name: no-name-8181d28a-0d2d-434e-b8af-ea48a5b510e8\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [02:50<00:00,  1.66s/it]\n",
      "[I 2025-02-03 20:37:12,097] Trial 0 finished with value: 0.42122678958899334 and parameters: {'max_depth': 11, 'n_estimators': 103, 'lr': 0.3580436334128195, 'min_samples_leaf': 58, 'subsample': 0.8286663974408439, 'colsample_bytree': 0.7235410203054733}. Best is trial 0 with value: 0.42122678958899334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 98, shrink model to the first 98 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 263/263 [05:58<00:00,  1.37s/it]\n",
      "[I 2025-02-03 20:43:11,619] Trial 1 finished with value: 0.4226809547624109 and parameters: {'max_depth': 14, 'n_estimators': 263, 'lr': 0.5042636024691471, 'min_samples_leaf': 79, 'subsample': 0.7839947309968889, 'colsample_bytree': 0.22700735628976806}. Best is trial 1 with value: 0.4226809547624109.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 255, shrink model to the first 255 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [06:58<00:00,  1.57s/it]\n",
      "[I 2025-02-03 20:50:10,335] Trial 2 finished with value: 0.4433211372526488 and parameters: {'max_depth': 13, 'n_estimators': 267, 'lr': 0.4913654636391831, 'min_samples_leaf': 63, 'subsample': 0.8781103393066291, 'colsample_bytree': 0.48727511307559246}. Best is trial 2 with value: 0.4433211372526488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 263, shrink model to the first 263 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 177/177 [04:12<00:00,  1.43s/it]\n",
      "[I 2025-02-03 20:54:23,525] Trial 3 finished with value: 0.4045871463301062 and parameters: {'max_depth': 4, 'n_estimators': 177, 'lr': 0.20322525071695244, 'min_samples_leaf': 41, 'subsample': 0.9948744709328557, 'colsample_bytree': 0.6661004159464016}. Best is trial 2 with value: 0.4433211372526488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 171, shrink model to the first 171 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 291/291 [06:15<00:00,  1.29s/it]\n",
      "[I 2025-02-03 21:00:39,417] Trial 4 finished with value: 0.4072509060898387 and parameters: {'max_depth': 3, 'n_estimators': 291, 'lr': 0.633302553641935, 'min_samples_leaf': 27, 'subsample': 0.7305747083945751, 'colsample_bytree': 0.326568178857373}. Best is trial 2 with value: 0.4433211372526488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 129, shrink model to the first 129 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [02:26<00:00,  1.42s/it]\n",
      "[I 2025-02-03 21:03:06,087] Trial 5 finished with value: 0.3668116583681518 and parameters: {'max_depth': 3, 'n_estimators': 103, 'lr': 0.03360530190413289, 'min_samples_leaf': 9, 'subsample': 0.844651547876835, 'colsample_bytree': 0.9503536877474335}. Best is trial 2 with value: 0.4433211372526488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 88, shrink model to the first 88 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [05:23<00:00,  1.44s/it]\n",
      "[I 2025-02-03 21:08:30,060] Trial 6 finished with value: 0.40937331232949015 and parameters: {'max_depth': 5, 'n_estimators': 225, 'lr': 0.9589883020042064, 'min_samples_leaf': 18, 'subsample': 0.6140400146734708, 'colsample_bytree': 0.8090252322796037}. Best is trial 2 with value: 0.4433211372526488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 216, shrink model to the first 216 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 254/254 [06:07<00:00,  1.45s/it]\n",
      "[I 2025-02-03 21:14:38,627] Trial 7 finished with value: 0.40714088053364195 and parameters: {'max_depth': 11, 'n_estimators': 254, 'lr': 0.08588775766452696, 'min_samples_leaf': 99, 'subsample': 0.4066582289766284, 'colsample_bytree': 0.8080148834839878}. Best is trial 2 with value: 0.4433211372526488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 207, shrink model to the first 207 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 184/184 [04:19<00:00,  1.41s/it]\n",
      "[I 2025-02-03 21:18:58,921] Trial 8 finished with value: 0.42391134144415593 and parameters: {'max_depth': 5, 'n_estimators': 184, 'lr': 0.3774373011923074, 'min_samples_leaf': 70, 'subsample': 0.8687401863544195, 'colsample_bytree': 0.560044381481824}. Best is trial 2 with value: 0.4433211372526488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 175, shrink model to the first 175 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 282/282 [08:12<00:00,  1.75s/it]\n",
      "[I 2025-02-03 21:27:11,895] Trial 9 finished with value: 0.427803385035281 and parameters: {'max_depth': 10, 'n_estimators': 282, 'lr': 0.41353854826275377, 'min_samples_leaf': 64, 'subsample': 0.4960698188507283, 'colsample_bytree': 0.8859482011310462}. Best is trial 2 with value: 0.4433211372526488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 275, shrink model to the first 275 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████▌                                           | 102/223 [02:51<03:23,  1.68s/it]\n",
      "[W 2025-02-03 21:30:03,535] Trial 10 failed with parameters: {'max_depth': 16, 'n_estimators': 223, 'lr': 0.7358913440323582, 'min_samples_leaf': 41, 'subsample': 0.9699733338767, 'colsample_bytree': 0.5074941836333766} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\MyProjects\\venvs\\uplift\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\roynys_pc\\AppData\\Local\\Temp\\ipykernel_9540\\1899621134.py\", line 15, in objective\n",
      "    train_ndcgs, test_ndcgs, best_i = model.fit()\n",
      "                                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\roynys_pc\\AppData\\Local\\Temp\\ipykernel_9540\\1332862113.py\", line 136, in fit\n",
      "    tree_, col_idx = self._train_one_tree(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\roynys_pc\\AppData\\Local\\Temp\\ipykernel_9540\\1332862113.py\", line 90, in _train_one_tree\n",
      "    lambdas = self._compute_lambdas(self.ys_train, train_preds)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\roynys_pc\\AppData\\Local\\Temp\\ipykernel_9540\\1332862113.py\", line 241, in _compute_lambdas\n",
      "    lambdas = compute_lambdas(y_true, y_pred)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\roynys_pc\\AppData\\Local\\Temp\\ipykernel_9540\\1332862113.py\", line 214, in compute_lambdas\n",
      "    lambda_update = torch.sum(lambda_update, dim=1, keepdim=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-02-03 21:30:03,548] Trial 10 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\MyProjects\\venvs\\uplift\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\MyProjects\\venvs\\uplift\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mE:\\MyProjects\\venvs\\uplift\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mE:\\MyProjects\\venvs\\uplift\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mE:\\MyProjects\\venvs\\uplift\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      4\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m300\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndcg_top_k\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m Solution(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 15\u001b[0m train_ndcgs, test_ndcgs, best_i \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_ndcgs[best_i]\n",
      "Cell \u001b[1;32mIn[1], line 136\u001b[0m, in \u001b[0;36mSolution.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_tree_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators)):\n\u001b[1;32m--> 136\u001b[0m     tree_, col_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_one_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcur_tree_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_tree_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_preds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimators\u001b[38;5;241m.\u001b[39mappend(tree_)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_indexes\u001b[38;5;241m.\u001b[39mappend(col_idx)\n",
      "Cell \u001b[1;32mIn[1], line 90\u001b[0m, in \u001b[0;36mSolution._train_one_tree\u001b[1;34m(self, cur_tree_idx, train_preds)\u001b[0m\n\u001b[0;32m     82\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(cur_tree_idx)\n\u001b[0;32m     84\u001b[0m tree_ \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(\n\u001b[0;32m     85\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth,\n\u001b[0;32m     86\u001b[0m     min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf,\n\u001b[0;32m     87\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mcur_tree_idx\n\u001b[0;32m     88\u001b[0m )\n\u001b[1;32m---> 90\u001b[0m lambdas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_lambdas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mys_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m n_rows, n_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     93\u001b[0m cols_to_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(n_cols), size\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mceil(n_cols\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolsample_bytree)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 241\u001b[0m, in \u001b[0;36mSolution._compute_lambdas\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgain_scheme\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m method not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gain_diff\n\u001b[1;32m--> 241\u001b[0m lambdas \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_lambdas\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lambdas\n",
      "Cell \u001b[1;32mIn[1], line 214\u001b[0m, in \u001b[0;36mSolution._compute_lambdas.<locals>.compute_lambdas\u001b[1;34m(y_true, y_pred, ndcg_scheme)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# посчитаем лямбды\u001b[39;00m\n\u001b[0;32m    213\u001b[0m lambda_update \u001b[38;5;241m=\u001b[39m  (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Sij) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m pos_pairs_score_diff) \u001b[38;5;241m*\u001b[39m delta_ndcg\n\u001b[1;32m--> 214\u001b[0m lambda_update \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lambda_update\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4f5967-7e93-430d-94fe-19cc63d24985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 13,\n",
       " 'n_estimators': 267,\n",
       " 'lr': 0.4913654636391831,\n",
       " 'min_samples_leaf': 63,\n",
       " 'subsample': 0.8781103393066291,\n",
       " 'colsample_bytree': 0.48727511307559246}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6afdf5ad-aa74-49bf-a3d5-92f60546efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [07:00<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 263, shrink model to the first 263 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.41302315221985325,\n",
       "  0.43826737652223674,\n",
       "  0.4654168880019816,\n",
       "  0.49372923043813927,\n",
       "  0.5115660741823991,\n",
       "  0.5132232243464975,\n",
       "  0.5236813918418727,\n",
       "  0.5291039372221847,\n",
       "  0.5343118118038889,\n",
       "  0.5484809938057554,\n",
       "  0.5549373898634037,\n",
       "  0.5574305140220993,\n",
       "  0.5642721822429536,\n",
       "  0.5678680630112495,\n",
       "  0.5746093523709634,\n",
       "  0.5742314433841441,\n",
       "  0.5764175247730311,\n",
       "  0.5797449394049087,\n",
       "  0.5805026322682675,\n",
       "  0.5866149334821666,\n",
       "  0.5839480612400046,\n",
       "  0.5832221876644746,\n",
       "  0.5879999638958289,\n",
       "  0.5938864019237323,\n",
       "  0.6023891809283001,\n",
       "  0.5988126183653776,\n",
       "  0.6099782115559642,\n",
       "  0.6087370254523825,\n",
       "  0.6083761896591977,\n",
       "  0.606795512794974,\n",
       "  0.6112291565303711,\n",
       "  0.611384579449673,\n",
       "  0.6125247853181042,\n",
       "  0.613094174711406,\n",
       "  0.6173027765445119,\n",
       "  0.617939211140932,\n",
       "  0.6206134912937475,\n",
       "  0.6202896802736969,\n",
       "  0.6228127404026205,\n",
       "  0.6211640808901749,\n",
       "  0.6251408577898202,\n",
       "  0.6256881643880576,\n",
       "  0.6296389842435431,\n",
       "  0.6281472758585976,\n",
       "  0.6305426019926962,\n",
       "  0.6345578657322674,\n",
       "  0.6381653136299461,\n",
       "  0.6415057089300334,\n",
       "  0.6422253980168826,\n",
       "  0.6455817417797185,\n",
       "  0.6452579254450577,\n",
       "  0.648060280045928,\n",
       "  0.6483966617553997,\n",
       "  0.6507572288862696,\n",
       "  0.6462873347189537,\n",
       "  0.6486176349210757,\n",
       "  0.6532909749422519,\n",
       "  0.6498507427934846,\n",
       "  0.6526233534367251,\n",
       "  0.654805259979026,\n",
       "  0.6581239397528958,\n",
       "  0.6598350622106489,\n",
       "  0.6615359046235899,\n",
       "  0.664388281251145,\n",
       "  0.6643579651507763,\n",
       "  0.6665940262789964,\n",
       "  0.6676993873888033,\n",
       "  0.667764357908715,\n",
       "  0.6673782668683131,\n",
       "  0.6687067043614728,\n",
       "  0.6684942395797476,\n",
       "  0.6672591480285425,\n",
       "  0.6682744960240308,\n",
       "  0.6723786050992508,\n",
       "  0.6702864282989781,\n",
       "  0.6714721014208664,\n",
       "  0.6739666510729065,\n",
       "  0.6726522005149396,\n",
       "  0.6731226954673786,\n",
       "  0.6722175344243951,\n",
       "  0.6748862027440671,\n",
       "  0.6774138031933287,\n",
       "  0.6758603985353381,\n",
       "  0.6779900330603728,\n",
       "  0.6767737889403838,\n",
       "  0.6802217481626037,\n",
       "  0.6781843531583209,\n",
       "  0.6787121536632433,\n",
       "  0.6812979145989077,\n",
       "  0.6833738745805832,\n",
       "  0.6831352997645672,\n",
       "  0.6824613657035031,\n",
       "  0.6836270571233392,\n",
       "  0.6846230194033305,\n",
       "  0.6843496792208803,\n",
       "  0.6870235232929189,\n",
       "  0.6883915364295693,\n",
       "  0.6886696493522863,\n",
       "  0.69220065283158,\n",
       "  0.6907035402294437,\n",
       "  0.692348356641533,\n",
       "  0.6918628915938865,\n",
       "  0.6920734580070085,\n",
       "  0.6944030079359703,\n",
       "  0.6939744736353457,\n",
       "  0.6946662535593648,\n",
       "  0.6974599547551144,\n",
       "  0.698058333366985,\n",
       "  0.6986112496651988,\n",
       "  0.6998445111120458,\n",
       "  0.6986814025953204,\n",
       "  0.6988925866711029,\n",
       "  0.7004330861942948,\n",
       "  0.7022455197628503,\n",
       "  0.7046160917371916,\n",
       "  0.703667543575719,\n",
       "  0.7027282100568033,\n",
       "  0.7034641756673286,\n",
       "  0.7049077592615904,\n",
       "  0.7053304022911941,\n",
       "  0.7046400402715588,\n",
       "  0.7063607883814164,\n",
       "  0.7081636857683004,\n",
       "  0.7086383551459751,\n",
       "  0.7085729786270448,\n",
       "  0.7105284117338798,\n",
       "  0.7096707266882456,\n",
       "  0.7087538485141808,\n",
       "  0.7137118913182637,\n",
       "  0.7116374876430567,\n",
       "  0.7114403236619765,\n",
       "  0.7136721613082231,\n",
       "  0.7135009035529225,\n",
       "  0.7119736133141753,\n",
       "  0.7143503535705366,\n",
       "  0.7154029183325479,\n",
       "  0.7165520235212551,\n",
       "  0.7163152431703641,\n",
       "  0.7179217432048283,\n",
       "  0.7177927799486691,\n",
       "  0.7197522663327777,\n",
       "  0.7201493967891167,\n",
       "  0.7225251168437986,\n",
       "  0.7224441876800894,\n",
       "  0.7213179392184771,\n",
       "  0.7216383938861567,\n",
       "  0.7218551422483614,\n",
       "  0.7221385541620289,\n",
       "  0.7249484833900007,\n",
       "  0.724259204297099,\n",
       "  0.7243837155904997,\n",
       "  0.7251905594583594,\n",
       "  0.7255336466059904,\n",
       "  0.7240428080315531,\n",
       "  0.7266886787419,\n",
       "  0.7271791633828498,\n",
       "  0.7263507378702379,\n",
       "  0.7279615152762179,\n",
       "  0.7277194449796329,\n",
       "  0.728225009812048,\n",
       "  0.7281323134662915,\n",
       "  0.7290350930649648,\n",
       "  0.7295798485766883,\n",
       "  0.7303701809990011,\n",
       "  0.7315452189467823,\n",
       "  0.7318646303209873,\n",
       "  0.7338273219309386,\n",
       "  0.7340876716556454,\n",
       "  0.7330321024360145,\n",
       "  0.734837968642308,\n",
       "  0.7330878533527353,\n",
       "  0.7351018164813665,\n",
       "  0.7359779554333612,\n",
       "  0.7362916023921171,\n",
       "  0.7337429884808678,\n",
       "  0.7358690469895628,\n",
       "  0.7371160842742182,\n",
       "  0.7342428527314963,\n",
       "  0.7375000312479142,\n",
       "  0.7361499744424376,\n",
       "  0.7381544525929115,\n",
       "  0.738232303162741,\n",
       "  0.7402094976609481,\n",
       "  0.7399230628385077,\n",
       "  0.7408278995929627,\n",
       "  0.7418227024951329,\n",
       "  0.7449765579571421,\n",
       "  0.7433711421731324,\n",
       "  0.7439628016418433,\n",
       "  0.7441576692379925,\n",
       "  0.7478617571665837,\n",
       "  0.7488071975322538,\n",
       "  0.7469069463161482,\n",
       "  0.7480137110912891,\n",
       "  0.7487548225396076,\n",
       "  0.7484032391747109,\n",
       "  0.7496554734764409,\n",
       "  0.74970656583694,\n",
       "  0.7513697174967703,\n",
       "  0.7527306745621664,\n",
       "  0.7532059457168198,\n",
       "  0.7537248393115296,\n",
       "  0.7535631917666092,\n",
       "  0.7546735086421906,\n",
       "  0.754629888638147,\n",
       "  0.754589934172299,\n",
       "  0.7555329285029844,\n",
       "  0.7550021969020565,\n",
       "  0.757818054830702,\n",
       "  0.7577439624127157,\n",
       "  0.7577205371956399,\n",
       "  0.7590436160963917,\n",
       "  0.7592537455793923,\n",
       "  0.7588632066943537,\n",
       "  0.7601821295408482,\n",
       "  0.7599674659345987,\n",
       "  0.7597223526644628,\n",
       "  0.7607997420271383,\n",
       "  0.7618274334802233,\n",
       "  0.7616966753666607,\n",
       "  0.7624916093505619,\n",
       "  0.7617113550341855,\n",
       "  0.761803446727763,\n",
       "  0.7623321813115967,\n",
       "  0.7629910675062785,\n",
       "  0.7634306534796059,\n",
       "  0.7636152446230494,\n",
       "  0.7638440973767228,\n",
       "  0.7637626937711143,\n",
       "  0.7642020477465782,\n",
       "  0.7664350074155152,\n",
       "  0.7667960970388759,\n",
       "  0.7657347155395781,\n",
       "  0.7659172070626452,\n",
       "  0.7676494244005521,\n",
       "  0.7678569161576697,\n",
       "  0.7675227845497882,\n",
       "  0.7686847333567208,\n",
       "  0.7694656168617181,\n",
       "  0.770160011812288,\n",
       "  0.7723506123741756,\n",
       "  0.7723199336727516,\n",
       "  0.7730799661324156,\n",
       "  0.7719630671811424,\n",
       "  0.7749145102361524,\n",
       "  0.7768814305307276,\n",
       "  0.7777712123090901,\n",
       "  0.7777409783100433,\n",
       "  0.7771905636821221,\n",
       "  0.7782752373293409,\n",
       "  0.7768538397243276,\n",
       "  0.7791672783588657,\n",
       "  0.7802724905328597,\n",
       "  0.7808932645590119,\n",
       "  0.7786653354990632,\n",
       "  0.7807636060691607,\n",
       "  0.7797528306674447,\n",
       "  0.7797462223701142,\n",
       "  0.7787205809400819,\n",
       "  0.7798991608593184,\n",
       "  0.7789092314310063,\n",
       "  0.7789127788657695,\n",
       "  0.7807042675274656,\n",
       "  0.7789542803863143,\n",
       "  0.7805529556536903,\n",
       "  0.7801699804321283,\n",
       "  0.782052718313342],\n",
       " [0.32060533825763193,\n",
       "  0.3515377138314268,\n",
       "  0.3586579158076085,\n",
       "  0.36193495177506624,\n",
       "  0.35923897980981284,\n",
       "  0.3867549412275601,\n",
       "  0.3965300776760193,\n",
       "  0.39364362902857475,\n",
       "  0.40229599076597994,\n",
       "  0.3873340579839667,\n",
       "  0.3885700753200895,\n",
       "  0.38608350719468876,\n",
       "  0.39101640470156496,\n",
       "  0.38839251898494137,\n",
       "  0.3939993423664488,\n",
       "  0.3947083908102684,\n",
       "  0.3911892230778447,\n",
       "  0.3870584162279451,\n",
       "  0.3968948186078018,\n",
       "  0.3927140931825994,\n",
       "  0.3925126350219379,\n",
       "  0.3913660226055143,\n",
       "  0.39733933597719423,\n",
       "  0.3973986745907494,\n",
       "  0.3955885937806782,\n",
       "  0.39320098585778135,\n",
       "  0.39817081416603517,\n",
       "  0.3949186852522441,\n",
       "  0.39265106899821556,\n",
       "  0.3907348100306293,\n",
       "  0.3902267438956017,\n",
       "  0.3949693915200697,\n",
       "  0.39635700525748874,\n",
       "  0.4018306225798265,\n",
       "  0.40000707613537223,\n",
       "  0.4008267757150959,\n",
       "  0.3957535736707266,\n",
       "  0.4008550451883554,\n",
       "  0.3987837226538408,\n",
       "  0.4034662709590021,\n",
       "  0.4019100288550824,\n",
       "  0.3990101702924086,\n",
       "  0.3995422301535721,\n",
       "  0.39843697916189247,\n",
       "  0.40155102049359503,\n",
       "  0.40148146492752884,\n",
       "  0.40334537100288903,\n",
       "  0.40135167921858506,\n",
       "  0.4041894608419659,\n",
       "  0.4032797683035953,\n",
       "  0.40407101057890266,\n",
       "  0.40209746565162613,\n",
       "  0.40366277481144514,\n",
       "  0.40861849453007526,\n",
       "  0.41116113814529504,\n",
       "  0.4123421945171762,\n",
       "  0.41222649583365545,\n",
       "  0.4102904439247326,\n",
       "  0.41554591655937045,\n",
       "  0.41442128315876237,\n",
       "  0.41507882886012,\n",
       "  0.4135468070666757,\n",
       "  0.4129850129590638,\n",
       "  0.4157840810676248,\n",
       "  0.41790792124580867,\n",
       "  0.41616837357312747,\n",
       "  0.41915703749401617,\n",
       "  0.4187862422600476,\n",
       "  0.4193107739133664,\n",
       "  0.41914270010679044,\n",
       "  0.42058412545226154,\n",
       "  0.4220566406126692,\n",
       "  0.42342418742865484,\n",
       "  0.42322136035501534,\n",
       "  0.42421745512364945,\n",
       "  0.4213483688600178,\n",
       "  0.42312278606907744,\n",
       "  0.4249230954890015,\n",
       "  0.42548741266249945,\n",
       "  0.42375156760330585,\n",
       "  0.4226401489400013,\n",
       "  0.4233275952382185,\n",
       "  0.42425419628112926,\n",
       "  0.42326642047660273,\n",
       "  0.42476582356550024,\n",
       "  0.42695996054244945,\n",
       "  0.42593362393718776,\n",
       "  0.42597527726931017,\n",
       "  0.42788170029984796,\n",
       "  0.42600975579806144,\n",
       "  0.4254869777892863,\n",
       "  0.4250650070036833,\n",
       "  0.42593062741670645,\n",
       "  0.4274996293025511,\n",
       "  0.42559791898594135,\n",
       "  0.4248354063992572,\n",
       "  0.42774429868444713,\n",
       "  0.4300904633588855,\n",
       "  0.4310115689083533,\n",
       "  0.432541353200897,\n",
       "  0.43224651086596305,\n",
       "  0.42992625955712777,\n",
       "  0.4265697387068302,\n",
       "  0.4318067574627886,\n",
       "  0.43131526899903927,\n",
       "  0.4307141701077303,\n",
       "  0.43070433880738074,\n",
       "  0.4335372419082262,\n",
       "  0.43348029694334406,\n",
       "  0.43295788670148916,\n",
       "  0.4323708267149046,\n",
       "  0.4330121980653562,\n",
       "  0.43091551392836414,\n",
       "  0.4325144385204347,\n",
       "  0.42996657086679835,\n",
       "  0.42863131795322923,\n",
       "  0.42934549321903165,\n",
       "  0.4290215860821343,\n",
       "  0.4287640482694668,\n",
       "  0.4299657196104258,\n",
       "  0.43058144408620885,\n",
       "  0.43203886972423644,\n",
       "  0.4326444948049529,\n",
       "  0.43166938679614636,\n",
       "  0.43539911374452783,\n",
       "  0.4369225573189691,\n",
       "  0.43750296490323387,\n",
       "  0.43654352660212664,\n",
       "  0.4360039022616069,\n",
       "  0.43626702040506404,\n",
       "  0.4367726062707533,\n",
       "  0.4347758057572928,\n",
       "  0.4320701653924564,\n",
       "  0.4341465515432752,\n",
       "  0.4365493380790076,\n",
       "  0.43682392835285794,\n",
       "  0.4382871822771175,\n",
       "  0.43437993562698546,\n",
       "  0.43621162346478326,\n",
       "  0.4325376588266277,\n",
       "  0.4334052137881614,\n",
       "  0.4334922459532725,\n",
       "  0.4349116040779278,\n",
       "  0.4348132249330655,\n",
       "  0.43306945571692057,\n",
       "  0.43341993625504355,\n",
       "  0.43762616408900085,\n",
       "  0.43412911717435504,\n",
       "  0.43686736151690897,\n",
       "  0.43476493765366725,\n",
       "  0.4345178540035897,\n",
       "  0.4358313913119559,\n",
       "  0.43686368686239907,\n",
       "  0.4368832887447787,\n",
       "  0.4370187424483063,\n",
       "  0.4352360099331994,\n",
       "  0.4370476063794891,\n",
       "  0.43694093824594754,\n",
       "  0.43553870571107256,\n",
       "  0.43738047134947916,\n",
       "  0.43699007499790365,\n",
       "  0.4358538280371237,\n",
       "  0.4357237678194732,\n",
       "  0.43617989215846503,\n",
       "  0.4386840603690745,\n",
       "  0.43844188434653925,\n",
       "  0.4389296552369593,\n",
       "  0.4388840673857493,\n",
       "  0.43850520181031355,\n",
       "  0.43723861049836116,\n",
       "  0.4372815413240023,\n",
       "  0.4382453924504944,\n",
       "  0.4382417579753437,\n",
       "  0.43776233050629026,\n",
       "  0.4379961076655228,\n",
       "  0.4383342317062647,\n",
       "  0.43774269605001154,\n",
       "  0.43791520573790504,\n",
       "  0.4385843195285947,\n",
       "  0.43844628117335205,\n",
       "  0.43876499862818685,\n",
       "  0.4395713294886631,\n",
       "  0.43922778459217754,\n",
       "  0.4387919024932511,\n",
       "  0.4386928045208528,\n",
       "  0.43975777684996337,\n",
       "  0.4396195113863517,\n",
       "  0.439680318124478,\n",
       "  0.43862452236037475,\n",
       "  0.4382885878518501,\n",
       "  0.4410168206131466,\n",
       "  0.4391434226892251,\n",
       "  0.4375606252744458,\n",
       "  0.4380928037400968,\n",
       "  0.4371460500438136,\n",
       "  0.43764420073704574,\n",
       "  0.4395467842236652,\n",
       "  0.4394706728516348,\n",
       "  0.4397567127286821,\n",
       "  0.44066329552636047,\n",
       "  0.43849721886262094,\n",
       "  0.4384921940688434,\n",
       "  0.4383942130637767,\n",
       "  0.4383307491801631,\n",
       "  0.4384322937765928,\n",
       "  0.4381914363164425,\n",
       "  0.4376478487615177,\n",
       "  0.4366929608122383,\n",
       "  0.4368348488703349,\n",
       "  0.43662917525546413,\n",
       "  0.43581302343838524,\n",
       "  0.4369963631101859,\n",
       "  0.43751756683334847,\n",
       "  0.4373476476858056,\n",
       "  0.4373380781235994,\n",
       "  0.4372712528212455,\n",
       "  0.43654704006947703,\n",
       "  0.4357246915157789,\n",
       "  0.43807762666553207,\n",
       "  0.43626861339976347,\n",
       "  0.43764554340791745,\n",
       "  0.43572419738713675,\n",
       "  0.4357712184989549,\n",
       "  0.4387680487855107,\n",
       "  0.4366813384742558,\n",
       "  0.4382867782201597,\n",
       "  0.43902584033370196,\n",
       "  0.43807749800636503,\n",
       "  0.43779321550548106,\n",
       "  0.4379269236337384,\n",
       "  0.43847889101690657,\n",
       "  0.4362967970057159,\n",
       "  0.43676689089655907,\n",
       "  0.43812972677066514,\n",
       "  0.43949614809926446,\n",
       "  0.43840646626080626,\n",
       "  0.43718129891643726,\n",
       "  0.43623162865808207,\n",
       "  0.4384553877662764,\n",
       "  0.43877396369853155,\n",
       "  0.43890214889159634,\n",
       "  0.43730482673885906,\n",
       "  0.4365060002389091,\n",
       "  0.43720255070367353,\n",
       "  0.43690612287169517,\n",
       "  0.43760901353992754,\n",
       "  0.4375554115028533,\n",
       "  0.4373661606580436,\n",
       "  0.43793146148582823,\n",
       "  0.43942703776412356,\n",
       "  0.44089256983587427,\n",
       "  0.4393424902803582,\n",
       "  0.43947105595327507,\n",
       "  0.43954702233772824,\n",
       "  0.44154355696817943,\n",
       "  0.4411713208365967,\n",
       "  0.442135479662916,\n",
       "  0.4401090949428818,\n",
       "  0.440446680918955,\n",
       "  0.4408249052133083,\n",
       "  0.4420364161784317,\n",
       "  0.44233072634127324,\n",
       "  0.4424916599409918,\n",
       "  0.4433211372526488,\n",
       "  0.4426577822005058,\n",
       "  0.4421717733955932,\n",
       "  0.44241338821757376],\n",
       " 263)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = Solution(**study.best_params)\n",
    "solution.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54ddcd7e-9ce0-4dd9-a297-6ae0d4123a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4433211372526488"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = solution.predict(solution.X_test)\n",
    "solution._calc_data_ndcg(solution.query_ids_test, solution.ys_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e67329-77ae-47da-a93d-56a83fdf3c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uplift",
   "language": "python",
   "name": "uplift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
